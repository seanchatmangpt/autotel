{
    "sourceFile": "bpmn_orchestrator.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 7,
            "patches": [
                {
                    "date": 1752191746285,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1752191771238,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -0,0 +1,663 @@\n+#!/usr/bin/env python3\n+\"\"\"\n+AutoTel BPMN Orchestrator - Enterprise BPMN 2.0 Execution Engine\n+Powered by SpiffWorkflow with zero-touch telemetry integration\n+\"\"\"\n+\n+import json\n+import uuid\n+import logging\n+from datetime import datetime, timedelta\n+from typing import Dict, List, Any, Optional, Callable, Union\n+from dataclasses import dataclass, field\n+from enum import Enum\n+from pathlib import Path\n+import xml.etree.ElementTree as ET\n+\n+# SpiffWorkflow imports\n+from SpiffWorkflow.bpmn import BpmnWorkflow\n+from SpiffWorkflow.bpmn.parser.BpmnParser import BpmnParser\n+from SpiffWorkflow.bpmn.specs.bpmn_task_spec import BpmnTaskSpec\n+from SpiffWorkflow.bpmn.specs.bpmn_process_spec import BpmnProcessSpec\n+from SpiffWorkflow.bpmn.util.task import BpmnTaskFilter\n+from SpiffWorkflow.bpmn.util.subworkflow import BpmnSubWorkflow\n+from SpiffWorkflow.bpmn.serializer import BpmnWorkflowSerializer\n+from SpiffWorkflow.bpmn.serializer.config import DEFAULT_CONFIG\n+\n+# OpenTelemetry for observability\n+from opentelemetry import trace, metrics\n+from opentelemetry.trace import Status, StatusCode\n+from opentelemetry.metrics import Counter, Histogram\n+from opentelemetry.sdk.trace import TracerProvider\n+from opentelemetry.sdk.trace.export import ConsoleSpanExporter, BatchSpanProcessor\n+from opentelemetry.sdk.metrics import MeterProvider\n+from opentelemetry.sdk.metrics.export import ConsoleMetricExporter, PeriodicExportingMetricReader\n+\n+# Configure logging\n+logging.basicConfig(level=logging.INFO)\n+logger = logging.getLogger(__name__)\n+\n+# Configure OpenTelemetry\n+trace.set_tracer_provider(TracerProvider())\n+trace.get_tracer_provider().add_span_processor(\n+    BatchSpanProcessor(ConsoleSpanExporter())\n+)\n+\n+metrics.set_meter_provider(MeterProvider())\n+meter = metrics.get_meter(__name__)\n+\n+# Telemetry instruments\n+process_counter = meter.create_counter(\n+    name=\"bpmn_processes_total\",\n+    description=\"Total number of BPMN processes executed\"\n+)\n+\n+task_duration_histogram = meter.create_histogram(\n+    name=\"bpmn_task_duration_seconds\",\n+    description=\"Duration of BPMN tasks in seconds\"\n+)\n+\n+error_counter = meter.create_counter(\n+    name=\"bpmn_errors_total\",\n+    description=\"Total number of BPMN execution errors\"\n+)\n+\n+class ProcessStatus(Enum):\n+    \"\"\"BPMN Process execution status\"\"\"\n+    PENDING = \"pending\"\n+    RUNNING = \"running\"\n+    COMPLETED = \"completed\"\n+    FAILED = \"failed\"\n+    SUSPENDED = \"suspended\"\n+    CANCELLED = \"cancelled\"\n+\n+class TaskStatus(Enum):\n+    \"\"\"BPMN Task execution status\"\"\"\n+    PENDING = \"pending\"\n+    READY = \"ready\"\n+    RUNNING = \"running\"\n+    COMPLETED = \"completed\"\n+    FAILED = \"failed\"\n+    SKIPPED = \"skipped\"\n+    CANCELLED = \"cancelled\"\n+\n+@dataclass\n+class ProcessInstance:\n+    \"\"\"BPMN Process instance with execution state and telemetry\"\"\"\n+    instance_id: str\n+    process_definition_id: str\n+    workflow: BpmnWorkflow\n+    start_time: datetime\n+    end_time: Optional[datetime] = None\n+    status: ProcessStatus = ProcessStatus.PENDING\n+    variables: Dict[str, Any] = field(default_factory=dict)\n+    execution_path: List[str] = field(default_factory=list)\n+    error_message: Optional[str] = None\n+    telemetry_data: Dict[str, Any] = field(default_factory=dict)\n+    \n+    def to_dict(self) -> Dict[str, Any]:\n+        \"\"\"Convert process instance to dictionary for serialization\"\"\"\n+        return {\n+            \"instance_id\": self.instance_id,\n+            \"process_definition_id\": self.process_definition_id,\n+            \"start_time\": self.start_time.isoformat(),\n+            \"end_time\": self.end_time.isoformat() if self.end_time else None,\n+            \"status\": self.status.value,\n+            \"variables\": self.variables,\n+            \"execution_path\": self.execution_path,\n+            \"error_message\": self.error_message,\n+            \"telemetry_data\": self.telemetry_data\n+        }\n+\n+@dataclass\n+class TaskExecution:\n+    \"\"\"BPMN Task execution context with telemetry\"\"\"\n+    task_id: str\n+    task_name: str\n+    task_type: str\n+    start_time: datetime\n+    end_time: Optional[datetime] = None\n+    status: TaskStatus = TaskStatus.PENDING\n+    duration: Optional[float] = None\n+    error_message: Optional[str] = None\n+    input_data: Dict[str, Any] = field(default_factory=dict)\n+    output_data: Dict[str, Any] = field(default_factory=dict)\n+    telemetry_data: Dict[str, Any] = field(default_factory=dict)\n+\n+class BPMNOrchestrator:\n+    \"\"\"\n+    Enterprise-grade BPMN 2.0 Orchestrator powered by SpiffWorkflow\n+    \n+    Features:\n+    - Full BPMN 2.0 specification compliance\n+    - Zero-touch telemetry integration\n+    - Process persistence and recovery\n+    - Advanced task execution monitoring\n+    - Multi-instance process support\n+    - Event-driven execution\n+    - Error handling and recovery\n+    \"\"\"\n+    \n+    def __init__(self, \n+                 bpmn_files_path: Optional[str] = None,\n+                 enable_telemetry: bool = True,\n+                 enable_persistence: bool = True):\n+        \"\"\"\n+        Initialize the BPMN Orchestrator\n+        \n+        Args:\n+            bpmn_files_path: Path to BPMN XML files\n+            enable_telemetry: Enable OpenTelemetry integration\n+            enable_persistence: Enable process state persistence\n+        \"\"\"\n+        self.tracer = trace.get_tracer(__name__)\n+        self.bpmn_files_path = Path(bpmn_files_path) if bpmn_files_path else Path(\"bpmn\")\n+        self.enable_telemetry = enable_telemetry\n+        self.enable_persistence = enable_persistence\n+        \n+        # Initialize SpiffWorkflow parser\n+        self.parser = BpmnParser()\n+        self.parser.add_bpmn_files_by_glob(str(self.bpmn_files_path / \"*.bpmn\"))\n+        \n+        # Process definitions cache\n+        self.process_definitions: Dict[str, BpmnProcessSpec] = {}\n+        self.active_instances: Dict[str, ProcessInstance] = {}\n+        \n+        # Load process definitions\n+        self._load_process_definitions()\n+        \n+        logger.info(f\"BPMN Orchestrator initialized with {len(self.process_definitions)} process definitions\")\n+    \n+    def _load_process_definitions(self) -> None:\n+        \"\"\"Load all BPMN process definitions from the configured path\"\"\"\n+        with self.tracer.start_as_current_span(\"load_process_definitions\") as span:\n+            try:\n+                # Get all process specs from the parser\n+                for process_id, process_spec in self.parser.get_process_specs().items():\n+                    self.process_definitions[process_id] = process_spec\n+                    span.add_event(f\"Loaded process definition: {process_id}\")\n+                \n+                logger.info(f\"Loaded {len(self.process_definitions)} process definitions\")\n+                \n+            except Exception as e:\n+                span.set_status(Status(StatusCode.ERROR, str(e)))\n+                logger.error(f\"Failed to load process definitions: {e}\")\n+                raise\n+    \n+    def start_process(self, \n+                     process_id: str, \n+                     variables: Optional[Dict[str, Any]] = None,\n+                     instance_id: Optional[str] = None) -> ProcessInstance:\n+        \"\"\"\n+        Start a new BPMN process instance\n+        \n+        Args:\n+            process_id: ID of the process to start\n+            variables: Initial process variables\n+            instance_id: Optional custom instance ID\n+            \n+        Returns:\n+            ProcessInstance: The started process instance\n+        \"\"\"\n+        with self.tracer.start_as_current_span(\"start_process\") as span:\n+            span.set_attributes({\n+                \"process.id\": process_id,\n+                \"process.variables\": json.dumps(variables or {})\n+            })\n+            \n+            try:\n+                # Validate process exists\n+                if process_id not in self.process_definitions:\n+                    raise ValueError(f\"Process definition '{process_id}' not found\")\n+                \n+                # Generate instance ID if not provided\n+                if not instance_id:\n+                    instance_id = f\"{process_id}_{uuid.uuid4().hex[:8]}\"\n+                \n+                # Create workflow instance\n+                workflow = BpmnWorkflow(self.process_definitions[process_id])\n+                \n+                # Set initial variables\n+                if variables:\n+                    workflow.data.update(variables)\n+                \n+                # Create process instance\n+                process_instance = ProcessInstance(\n+                    instance_id=instance_id,\n+                    process_definition_id=process_id,\n+                    workflow=workflow,\n+                    start_time=datetime.utcnow(),\n+                    variables=variables or {},\n+                    status=ProcessStatus.RUNNING\n+                )\n+                \n+                # Store instance\n+                self.active_instances[instance_id] = process_instance\n+                \n+                # Update telemetry\n+                process_counter.add(1, {\"process.id\": process_id, \"action\": \"start\"})\n+                \n+                span.set_attributes({\n+                    \"instance.id\": instance_id,\n+                    \"instance.status\": ProcessStatus.RUNNING.value\n+                })\n+                \n+                logger.info(f\"Started process instance {instance_id} for process {process_id}\")\n+                return process_instance\n+                \n+            except Exception as e:\n+                span.set_status(Status(StatusCode.ERROR, str(e)))\n+                error_counter.add(1, {\"process.id\": process_id, \"error.type\": \"start_failed\"})\n+                logger.error(f\"Failed to start process {process_id}: {e}\")\n+                raise\n+    \n+    def execute_process(self, instance_id: str, max_steps: int = 100) -> ProcessInstance:\n+        \"\"\"\n+        Execute a BPMN process instance\n+        \n+        Args:\n+            instance_id: ID of the process instance to execute\n+            max_steps: Maximum number of execution steps\n+            \n+        Returns:\n+            ProcessInstance: Updated process instance\n+        \"\"\"\n+        with self.tracer.start_as_current_span(\"execute_process\") as span:\n+            span.set_attributes({\n+                \"instance.id\": instance_id,\n+                \"execution.max_steps\": max_steps\n+            })\n+            \n+            try:\n+                # Get process instance\n+                if instance_id not in self.active_instances:\n+                    raise ValueError(f\"Process instance '{instance_id}' not found\")\n+                \n+                process_instance = self.active_instances[instance_id]\n+                workflow = process_instance.workflow\n+                \n+                # Execute workflow\n+                step_count = 0\n+                while not workflow.is_completed() and step_count < max_steps:\n+                    # Get ready tasks\n+                    ready_tasks = workflow.get_tasks(state=TaskStatus.READY.value)\n+                    \n+                    if not ready_tasks:\n+                        # No ready tasks, workflow might be waiting for external events\n+                        break\n+                    \n+                    # Execute each ready task\n+                    for task in ready_tasks:\n+                        self._execute_task(task, process_instance)\n+                        step_count += 1\n+                        \n+                        if workflow.is_completed():\n+                            break\n+                \n+                # Update process status\n+                if workflow.is_completed():\n+                    process_instance.status = ProcessStatus.COMPLETED\n+                    process_instance.end_time = datetime.utcnow()\n+                elif step_count >= max_steps:\n+                    process_instance.status = ProcessStatus.SUSPENDED\n+                \n+                # Update execution path\n+                process_instance.execution_path = self._get_execution_path(workflow)\n+                \n+                span.set_attributes({\n+                    \"execution.steps\": step_count,\n+                    \"instance.status\": process_instance.status.value\n+                })\n+                \n+                logger.info(f\"Executed process instance {instance_id} ({step_count} steps)\")\n+                return process_instance\n+                \n+            except Exception as e:\n+                span.set_status(Status(StatusCode.ERROR, str(e)))\n+                error_counter.add(1, {\"instance.id\": instance_id, \"error.type\": \"execution_failed\"})\n+                logger.error(f\"Failed to execute process instance {instance_id}: {e}\")\n+                raise\n+    \n+    def _execute_task(self, task: BpmnTaskSpec, process_instance: ProcessInstance) -> None:\n+        \"\"\"Execute a single BPMN task with telemetry\"\"\"\n+        with self.tracer.start_as_current_span(\"execute_task\") as span:\n+            task_start_time = datetime.utcnow()\n+            \n+            span.set_attributes({\n+                \"task.id\": task.id,\n+                \"task.name\": task.name,\n+                \"task.type\": task.__class__.__name__\n+            })\n+            \n+            try:\n+                # Create task execution context\n+                task_execution = TaskExecution(\n+                    task_id=task.id,\n+                    task_name=task.name,\n+                    task_type=task.__class__.__name__,\n+                    start_time=task_start_time,\n+                    status=TaskStatus.RUNNING,\n+                    input_data=dict(task.data) if hasattr(task, 'data') else {}\n+                )\n+                \n+                # Execute the task\n+                task.complete()\n+                \n+                # Update task execution\n+                task_execution.end_time = datetime.utcnow()\n+                task_execution.duration = (task_execution.end_time - task_execution.start_time).total_seconds()\n+                task_execution.status = TaskStatus.COMPLETED\n+                task_execution.output_data = dict(task.data) if hasattr(task, 'data') else {}\n+                \n+                # Record telemetry\n+                if self.enable_telemetry:\n+                    task_duration_histogram.record(\n+                        task_execution.duration,\n+                        {\n+                            \"task.id\": task.id,\n+                            \"task.type\": task_execution.task_type,\n+                            \"process.id\": process_instance.process_definition_id\n+                        }\n+                    )\n+                \n+                span.set_attributes({\n+                    \"task.duration\": task_execution.duration,\n+                    \"task.status\": TaskStatus.COMPLETED.value\n+                })\n+                \n+                logger.debug(f\"Executed task {task.id} ({task_execution.duration:.3f}s)\")\n+                \n+            except Exception as e:\n+                task_execution.end_time = datetime.utcnow()\n+                task_execution.duration = (task_execution.end_time - task_execution.start_time).total_seconds()\n+                task_execution.status = TaskStatus.FAILED\n+                task_execution.error_message = str(e)\n+                \n+                span.set_status(Status(StatusCode.ERROR, str(e)))\n+                error_counter.add(1, {\n+                    \"task.id\": task.id,\n+                    \"task.type\": task.__class__.__name__,\n+                    \"error.type\": \"task_execution_failed\"\n+                })\n+                \n+                logger.error(f\"Failed to execute task {task.id}: {e}\")\n+                raise\n+    \n+    def _get_execution_path(self, workflow: BpmnWorkflow) -> List[str]:\n+        \"\"\"Get the execution path of completed tasks\"\"\"\n+        try:\n+            completed_tasks = workflow.get_tasks(state=TaskStatus.COMPLETED.value)\n+            return [task.id for task in completed_tasks]\n+        except Exception as e:\n+            logger.warning(f\"Failed to get execution path: {e}\")\n+            return []\n+    \n+    def get_process_instance(self, instance_id: str) -> Optional[ProcessInstance]:\n+        \"\"\"Get a process instance by ID\"\"\"\n+        return self.active_instances.get(instance_id)\n+    \n+    def list_process_instances(self, \n+                             status: Optional[ProcessStatus] = None,\n+                             process_id: Optional[str] = None) -> List[ProcessInstance]:\n+        \"\"\"List process instances with optional filtering\"\"\"\n+        instances = list(self.active_instances.values())\n+        \n+        if status:\n+            instances = [i for i in instances if i.status == status]\n+        \n+        if process_id:\n+            instances = [i for i in instances if i.process_definition_id == process_id]\n+        \n+        return instances\n+    \n+    def cancel_process(self, instance_id: str) -> ProcessInstance:\n+        \"\"\"Cancel a running process instance\"\"\"\n+        with self.tracer.start_as_current_span(\"cancel_process\") as span:\n+            span.set_attributes({\"instance.id\": instance_id})\n+            \n+            try:\n+                if instance_id not in self.active_instances:\n+                    raise ValueError(f\"Process instance '{instance_id}' not found\")\n+                \n+                process_instance = self.active_instances[instance_id]\n+                \n+                if process_instance.status not in [ProcessStatus.RUNNING, ProcessStatus.SUSPENDED]:\n+                    raise ValueError(f\"Cannot cancel process in status: {process_instance.status}\")\n+                \n+                # Cancel the workflow\n+                process_instance.workflow.cancel()\n+                process_instance.status = ProcessStatus.CANCELLED\n+                process_instance.end_time = datetime.utcnow()\n+                \n+                span.set_attributes({\"instance.status\": ProcessStatus.CANCELLED.value})\n+                \n+                logger.info(f\"Cancelled process instance {instance_id}\")\n+                return process_instance\n+                \n+            except Exception as e:\n+                span.set_status(Status(StatusCode.ERROR, str(e)))\n+                error_counter.add(1, {\"instance.id\": instance_id, \"error.type\": \"cancel_failed\"})\n+                logger.error(f\"Failed to cancel process instance {instance_id}: {e}\")\n+                raise\n+    \n+    def get_process_variables(self, instance_id: str) -> Dict[str, Any]:\n+        \"\"\"Get process variables for an instance\"\"\"\n+        if instance_id not in self.active_instances:\n+            raise ValueError(f\"Process instance '{instance_id}' not found\")\n+        \n+        process_instance = self.active_instances[instance_id]\n+        return dict(process_instance.workflow.data)\n+    \n+    def set_process_variables(self, instance_id: str, variables: Dict[str, Any]) -> None:\n+        \"\"\"Set process variables for an instance\"\"\"\n+        if instance_id not in self.active_instances:\n+            raise ValueError(f\"Process instance '{instance_id}' not found\")\n+        \n+        process_instance = self.active_instances[instance_id]\n+        process_instance.workflow.data.update(variables)\n+        process_instance.variables.update(variables)\n+    \n+    def get_ready_tasks(self, instance_id: str) -> List[Dict[str, Any]]:\n+        \"\"\"Get ready tasks for a process instance\"\"\"\n+        if instance_id not in self.active_instances:\n+            raise ValueError(f\"Process instance '{instance_id}' not found\")\n+        \n+        process_instance = self.active_instances[instance_id]\n+        ready_tasks = process_instance.workflow.get_tasks(state=TaskStatus.READY.value)\n+        \n+        return [\n+            {\n+                \"task_id\": task.id,\n+                \"task_name\": task.name,\n+                \"task_type\": task.__class__.__name__,\n+                \"data\": dict(task.data) if hasattr(task, 'data') else {}\n+            }\n+            for task in ready_tasks\n+        ]\n+    \n+    def complete_task(self, instance_id: str, task_id: str, data: Optional[Dict[str, Any]] = None) -> None:\n+        \"\"\"Complete a specific task in a process instance\"\"\"\n+        with self.tracer.start_as_current_span(\"complete_task\") as span:\n+            span.set_attributes({\n+                \"instance.id\": instance_id,\n+                \"task.id\": task_id\n+            })\n+            \n+            try:\n+                if instance_id not in self.active_instances:\n+                    raise ValueError(f\"Process instance '{instance_id}' not found\")\n+                \n+                process_instance = self.active_instances[instance_id]\n+                workflow = process_instance.workflow\n+                \n+                # Find the task\n+                tasks = workflow.get_tasks()\n+                target_task = None\n+                \n+                for task in tasks:\n+                    if task.id == task_id:\n+                        target_task = task\n+                        break\n+                \n+                if not target_task:\n+                    raise ValueError(f\"Task '{task_id}' not found in process instance '{instance_id}'\")\n+                \n+                # Set task data if provided\n+                if data and hasattr(target_task, 'data'):\n+                    target_task.data.update(data)\n+                \n+                # Complete the task\n+                target_task.complete()\n+                \n+                span.set_attributes({\"task.status\": \"completed\"})\n+                \n+                logger.info(f\"Completed task {task_id} in process instance {instance_id}\")\n+                \n+            except Exception as e:\n+                span.set_status(Status(StatusCode.ERROR, str(e)))\n+                error_counter.add(1, {\n+                    \"instance.id\": instance_id,\n+                    \"task.id\": task_id,\n+                    \"error.type\": \"task_completion_failed\"\n+                })\n+                logger.error(f\"Failed to complete task {task_id} in process instance {instance_id}: {e}\")\n+                raise\n+    \n+    def serialize_workflow(self, instance_id: str) -> str:\n+        \"\"\"Serialize a workflow instance to JSON\"\"\"\n+        if instance_id not in self.active_instances:\n+            raise ValueError(f\"Process instance '{instance_id}' not found\")\n+        \n+        process_instance = self.active_instances[instance_id]\n+        serializer = BpmnWorkflowSerializer(BPMN_CONFIG)\n+        return serializer.serialize_workflow(process_instance.workflow)\n+    \n+    def deserialize_workflow(self, instance_id: str, serialized_data: str) -> ProcessInstance:\n+        \"\"\"Deserialize a workflow instance from JSON\"\"\"\n+        serializer = BpmnWorkflowSerializer(BPMN_CONFIG)\n+        workflow = serializer.deserialize_workflow(serialized_data)\n+        \n+        process_instance = ProcessInstance(\n+            instance_id=instance_id,\n+            process_definition_id=workflow.spec.name,\n+            workflow=workflow,\n+            start_time=datetime.utcnow(),\n+            status=ProcessStatus.RUNNING\n+        )\n+        \n+        self.active_instances[instance_id] = process_instance\n+        return process_instance\n+    \n+    def get_process_statistics(self) -> Dict[str, Any]:\n+        \"\"\"Get orchestrator statistics\"\"\"\n+        total_instances = len(self.active_instances)\n+        status_counts = {}\n+        \n+        for status in ProcessStatus:\n+            status_counts[status.value] = len([\n+                i for i in self.active_instances.values() \n+                if i.status == status\n+            ])\n+        \n+        return {\n+            \"total_process_definitions\": len(self.process_definitions),\n+            \"total_instances\": total_instances,\n+            \"status_distribution\": status_counts,\n+            \"process_definitions\": list(self.process_definitions.keys())\n+        }\n+    \n+    def cleanup_completed_processes(self, max_age_hours: int = 24) -> int:\n+        \"\"\"Clean up completed process instances older than specified age\"\"\"\n+        cutoff_time = datetime.utcnow() - timedelta(hours=max_age_hours)\n+        removed_count = 0\n+        \n+        instances_to_remove = []\n+        \n+        for instance_id, process_instance in self.active_instances.items():\n+            if (process_instance.status in [ProcessStatus.COMPLETED, ProcessStatus.FAILED, ProcessStatus.CANCELLED] and\n+                process_instance.end_time and process_instance.end_time < cutoff_time):\n+                instances_to_remove.append(instance_id)\n+        \n+        for instance_id in instances_to_remove:\n+            del self.active_instances[instance_id]\n+            removed_count += 1\n+        \n+        logger.info(f\"Cleaned up {removed_count} completed process instances\")\n+        return removed_count\n+\n+# Example usage and utility functions\n+def create_sample_bpmn_file() -> str:\n+    \"\"\"Create a sample BPMN file for testing\"\"\"\n+    sample_bpmn = '''<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n+<bpmn:definitions xmlns:bpmn=\"http://www.omg.org/spec/BPMN/20100524/MODEL\"\n+                  xmlns:bpmndi=\"http://www.omg.org/spec/BPMN/20100524/DI\"\n+                  xmlns:dc=\"http://www.omg.org/spec/DD/20100524/DC\"\n+                  xmlns:di=\"http://www.omg.org/spec/DD/20100524/DI\"\n+                  id=\"Definitions_1\"\n+                  targetNamespace=\"http://bpmn.io/schema/bpmn\">\n+  <bpmn:process id=\"Process_1\" isExecutable=\"true\">\n+    <bpmn:startEvent id=\"StartEvent_1\" name=\"Start\">\n+      <bpmn:outgoing>Flow_1</bpmn:outgoing>\n+    </bpmn:startEvent>\n+    <bpmn:task id=\"Task_1\" name=\"Sample Task\">\n+      <bpmn:incoming>Flow_1</bpmn:incoming>\n+      <bpmn:outgoing>Flow_2</bpmn:outgoing>\n+    </bpmn:task>\n+    <bpmn:endEvent id=\"EndEvent_1\" name=\"End\">\n+      <bpmn:incoming>Flow_2</bpmn:incoming>\n+    </bpmn:endEvent>\n+    <bpmn:sequenceFlow id=\"Flow_1\" sourceRef=\"StartEvent_1\" targetRef=\"Task_1\" />\n+    <bpmn:sequenceFlow id=\"Flow_2\" sourceRef=\"Task_1\" targetRef=\"EndEvent_1\" />\n+  </bpmn:process>\n+  <bpmndi:BPMNDiagram id=\"BPMNDiagram_1\">\n+    <bpmndi:BPMNPlane id=\"BPMNPlane_1\" bpmnElement=\"Process_1\">\n+      <bpmndi:BPMNShape id=\"StartEvent_1_di\" bpmnElement=\"StartEvent_1\">\n+        <dc:Bounds x=\"152\" y=\"102\" width=\"36\" height=\"36\" />\n+      </bpmndi:BPMNShape>\n+      <bpmndi:BPMNShape id=\"Task_1_di\" bpmnElement=\"Task_1\">\n+        <dc:Bounds x=\"240\" y=\"80\" width=\"100\" height=\"80\" />\n+      </bpmndi:BPMNShape>\n+      <bpmndi:BPMNShape id=\"EndEvent_1_di\" bpmnElement=\"EndEvent_1\">\n+        <dc:Bounds x=\"392\" y=\"102\" width=\"36\" height=\"36\" />\n+      </bpmndi:BPMNShape>\n+      <bpmndi:BPMNEdge id=\"Flow_1_di\" bpmnElement=\"Flow_1\">\n+        <di:waypoint x=\"188\" y=\"120\" />\n+        <di:waypoint x=\"240\" y=\"120\" />\n+      </bpmndi:BPMNEdge>\n+      <bpmndi:BPMNEdge id=\"Flow_2_di\" bpmnElement=\"Flow_2\">\n+        <di:waypoint x=\"340\" y=\"120\" />\n+        <di:waypoint x=\"392\" y=\"120\" />\n+      </bpmndi:BPMNEdge>\n+    </bpmndi:BPMNPlane>\n+  </bpmndi:BPMNDiagram>\n+</bpmn:definitions>'''\n+    \n+    return sample_bpmn\n+\n+if __name__ == \"__main__\":\n+    # Example usage\n+    print(\"AutoTel BPMN Orchestrator - SpiffWorkflow Integration\")\n+    print(\"=\" * 60)\n+    \n+    # Create sample BPMN file\n+    sample_bpmn = create_sample_bpmn_file()\n+    bpmn_path = Path(\"bpmn\")\n+    bpmn_path.mkdir(exist_ok=True)\n+    \n+    with open(bpmn_path / \"sample_process.bpmn\", \"w\") as f:\n+        f.write(sample_bpmn)\n+    \n+    # Initialize orchestrator\n+    orchestrator = BPMNOrchestrator(bpmn_files_path=\"bpmn\")\n+    \n+    # Start a process\n+    instance = orchestrator.start_process(\"Process_1\", {\"input\": \"test\"})\n+    print(f\"Started process instance: {instance.instance_id}\")\n+    \n+    # Execute the process\n+    result = orchestrator.execute_process(instance.instance_id)\n+    print(f\"Process execution completed. Status: {result.status.value}\")\n+    \n+    # Get statistics\n+    stats = orchestrator.get_process_statistics()\n+    print(f\"Orchestrator statistics: {stats}\")\n\\ No newline at end of file\n"
                },
                {
                    "date": 1752191780394,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -528,9 +528,9 @@\n         if instance_id not in self.active_instances:\n             raise ValueError(f\"Process instance '{instance_id}' not found\")\n         \n         process_instance = self.active_instances[instance_id]\n-        serializer = BpmnWorkflowSerializer(BPMN_CONFIG)\n+        serializer = BpmnWorkflowSerializer(DEFAULT_CONFIG)\n         return serializer.serialize_workflow(process_instance.workflow)\n     \n     def deserialize_workflow(self, instance_id: str, serialized_data: str) -> ProcessInstance:\n         \"\"\"Deserialize a workflow instance from JSON\"\"\"\n@@ -659,668 +659,5 @@\n     print(f\"Process execution completed. Status: {result.status.value}\")\n     \n     # Get statistics\n     stats = orchestrator.get_process_statistics()\n-    print(f\"Orchestrator statistics: {stats}\")\n-#!/usr/bin/env python3\n-\"\"\"\n-AutoTel BPMN Orchestrator - Enterprise BPMN 2.0 Execution Engine\n-Powered by SpiffWorkflow with zero-touch telemetry integration\n-\"\"\"\n-\n-import json\n-import uuid\n-import logging\n-from datetime import datetime, timedelta\n-from typing import Dict, List, Any, Optional, Callable, Union\n-from dataclasses import dataclass, field\n-from enum import Enum\n-from pathlib import Path\n-import xml.etree.ElementTree as ET\n-\n-# SpiffWorkflow imports\n-from SpiffWorkflow.bpmn import BpmnWorkflow\n-from SpiffWorkflow.bpmn.parser.BpmnParser import BpmnParser\n-from SpiffWorkflow.bpmn.specs.bpmn_task_spec import BpmnTaskSpec\n-from SpiffWorkflow.bpmn.specs.bpmn_process_spec import BpmnProcessSpec\n-from SpiffWorkflow.bpmn.util.task import BpmnTaskFilter\n-from SpiffWorkflow.bpmn.util.subworkflow import BpmnSubWorkflow\n-from SpiffWorkflow.bpmn.serializer import BpmnWorkflowSerializer\n-from SpiffWorkflow.bpmn.serializer.config import BPMN_CONFIG\n-\n-# OpenTelemetry for observability\n-from opentelemetry import trace, metrics\n-from opentelemetry.trace import Status, StatusCode\n-from opentelemetry.metrics import Counter, Histogram\n-from opentelemetry.sdk.trace import TracerProvider\n-from opentelemetry.sdk.trace.export import ConsoleSpanExporter, BatchSpanProcessor\n-from opentelemetry.sdk.metrics import MeterProvider\n-from opentelemetry.sdk.metrics.export import ConsoleMetricExporter, PeriodicExportingMetricReader\n-\n-# Configure logging\n-logging.basicConfig(level=logging.INFO)\n-logger = logging.getLogger(__name__)\n-\n-# Configure OpenTelemetry\n-trace.set_tracer_provider(TracerProvider())\n-trace.get_tracer_provider().add_span_processor(\n-    BatchSpanProcessor(ConsoleSpanExporter())\n-)\n-\n-metrics.set_meter_provider(MeterProvider())\n-meter = metrics.get_meter(__name__)\n-\n-# Telemetry instruments\n-process_counter = meter.create_counter(\n-    name=\"bpmn_processes_total\",\n-    description=\"Total number of BPMN processes executed\"\n-)\n-\n-task_duration_histogram = meter.create_histogram(\n-    name=\"bpmn_task_duration_seconds\",\n-    description=\"Duration of BPMN tasks in seconds\"\n-)\n-\n-error_counter = meter.create_counter(\n-    name=\"bpmn_errors_total\",\n-    description=\"Total number of BPMN execution errors\"\n-)\n-\n-class ProcessStatus(Enum):\n-    \"\"\"BPMN Process execution status\"\"\"\n-    PENDING = \"pending\"\n-    RUNNING = \"running\"\n-    COMPLETED = \"completed\"\n-    FAILED = \"failed\"\n-    SUSPENDED = \"suspended\"\n-    CANCELLED = \"cancelled\"\n-\n-class TaskStatus(Enum):\n-    \"\"\"BPMN Task execution status\"\"\"\n-    PENDING = \"pending\"\n-    READY = \"ready\"\n-    RUNNING = \"running\"\n-    COMPLETED = \"completed\"\n-    FAILED = \"failed\"\n-    SKIPPED = \"skipped\"\n-    CANCELLED = \"cancelled\"\n-\n-@dataclass\n-class ProcessInstance:\n-    \"\"\"BPMN Process instance with execution state and telemetry\"\"\"\n-    instance_id: str\n-    process_definition_id: str\n-    workflow: BpmnWorkflow\n-    start_time: datetime\n-    end_time: Optional[datetime] = None\n-    status: ProcessStatus = ProcessStatus.PENDING\n-    variables: Dict[str, Any] = field(default_factory=dict)\n-    execution_path: List[str] = field(default_factory=list)\n-    error_message: Optional[str] = None\n-    telemetry_data: Dict[str, Any] = field(default_factory=dict)\n-    \n-    def to_dict(self) -> Dict[str, Any]:\n-        \"\"\"Convert process instance to dictionary for serialization\"\"\"\n-        return {\n-            \"instance_id\": self.instance_id,\n-            \"process_definition_id\": self.process_definition_id,\n-            \"start_time\": self.start_time.isoformat(),\n-            \"end_time\": self.end_time.isoformat() if self.end_time else None,\n-            \"status\": self.status.value,\n-            \"variables\": self.variables,\n-            \"execution_path\": self.execution_path,\n-            \"error_message\": self.error_message,\n-            \"telemetry_data\": self.telemetry_data\n-        }\n-\n-@dataclass\n-class TaskExecution:\n-    \"\"\"BPMN Task execution context with telemetry\"\"\"\n-    task_id: str\n-    task_name: str\n-    task_type: str\n-    start_time: datetime\n-    end_time: Optional[datetime] = None\n-    status: TaskStatus = TaskStatus.PENDING\n-    duration: Optional[float] = None\n-    error_message: Optional[str] = None\n-    input_data: Dict[str, Any] = field(default_factory=dict)\n-    output_data: Dict[str, Any] = field(default_factory=dict)\n-    telemetry_data: Dict[str, Any] = field(default_factory=dict)\n-\n-class BPMNOrchestrator:\n-    \"\"\"\n-    Enterprise-grade BPMN 2.0 Orchestrator powered by SpiffWorkflow\n-    \n-    Features:\n-    - Full BPMN 2.0 specification compliance\n-    - Zero-touch telemetry integration\n-    - Process persistence and recovery\n-    - Advanced task execution monitoring\n-    - Multi-instance process support\n-    - Event-driven execution\n-    - Error handling and recovery\n-    \"\"\"\n-    \n-    def __init__(self, \n-                 bpmn_files_path: Optional[str] = None,\n-                 enable_telemetry: bool = True,\n-                 enable_persistence: bool = True):\n-        \"\"\"\n-        Initialize the BPMN Orchestrator\n-        \n-        Args:\n-            bpmn_files_path: Path to BPMN XML files\n-            enable_telemetry: Enable OpenTelemetry integration\n-            enable_persistence: Enable process state persistence\n-        \"\"\"\n-        self.tracer = trace.get_tracer(__name__)\n-        self.bpmn_files_path = Path(bpmn_files_path) if bpmn_files_path else Path(\"bpmn\")\n-        self.enable_telemetry = enable_telemetry\n-        self.enable_persistence = enable_persistence\n-        \n-        # Initialize SpiffWorkflow parser\n-        self.parser = BpmnParser()\n-        self.parser.add_bpmn_files_by_glob(str(self.bpmn_files_path / \"*.bpmn\"))\n-        \n-        # Process definitions cache\n-        self.process_definitions: Dict[str, BpmnProcessSpec] = {}\n-        self.active_instances: Dict[str, ProcessInstance] = {}\n-        \n-        # Load process definitions\n-        self._load_process_definitions()\n-        \n-        logger.info(f\"BPMN Orchestrator initialized with {len(self.process_definitions)} process definitions\")\n-    \n-    def _load_process_definitions(self) -> None:\n-        \"\"\"Load all BPMN process definitions from the configured path\"\"\"\n-        with self.tracer.start_as_current_span(\"load_process_definitions\") as span:\n-            try:\n-                # Get all process specs from the parser\n-                for process_id, process_spec in self.parser.get_process_specs().items():\n-                    self.process_definitions[process_id] = process_spec\n-                    span.add_event(f\"Loaded process definition: {process_id}\")\n-                \n-                logger.info(f\"Loaded {len(self.process_definitions)} process definitions\")\n-                \n-            except Exception as e:\n-                span.set_status(Status(StatusCode.ERROR, str(e)))\n-                logger.error(f\"Failed to load process definitions: {e}\")\n-                raise\n-    \n-    def start_process(self, \n-                     process_id: str, \n-                     variables: Optional[Dict[str, Any]] = None,\n-                     instance_id: Optional[str] = None) -> ProcessInstance:\n-        \"\"\"\n-        Start a new BPMN process instance\n-        \n-        Args:\n-            process_id: ID of the process to start\n-            variables: Initial process variables\n-            instance_id: Optional custom instance ID\n-            \n-        Returns:\n-            ProcessInstance: The started process instance\n-        \"\"\"\n-        with self.tracer.start_as_current_span(\"start_process\") as span:\n-            span.set_attributes({\n-                \"process.id\": process_id,\n-                \"process.variables\": json.dumps(variables or {})\n-            })\n-            \n-            try:\n-                # Validate process exists\n-                if process_id not in self.process_definitions:\n-                    raise ValueError(f\"Process definition '{process_id}' not found\")\n-                \n-                # Generate instance ID if not provided\n-                if not instance_id:\n-                    instance_id = f\"{process_id}_{uuid.uuid4().hex[:8]}\"\n-                \n-                # Create workflow instance\n-                workflow = BpmnWorkflow(self.process_definitions[process_id])\n-                \n-                # Set initial variables\n-                if variables:\n-                    workflow.data.update(variables)\n-                \n-                # Create process instance\n-                process_instance = ProcessInstance(\n-                    instance_id=instance_id,\n-                    process_definition_id=process_id,\n-                    workflow=workflow,\n-                    start_time=datetime.utcnow(),\n-                    variables=variables or {},\n-                    status=ProcessStatus.RUNNING\n-                )\n-                \n-                # Store instance\n-                self.active_instances[instance_id] = process_instance\n-                \n-                # Update telemetry\n-                process_counter.add(1, {\"process.id\": process_id, \"action\": \"start\"})\n-                \n-                span.set_attributes({\n-                    \"instance.id\": instance_id,\n-                    \"instance.status\": ProcessStatus.RUNNING.value\n-                })\n-                \n-                logger.info(f\"Started process instance {instance_id} for process {process_id}\")\n-                return process_instance\n-                \n-            except Exception as e:\n-                span.set_status(Status(StatusCode.ERROR, str(e)))\n-                error_counter.add(1, {\"process.id\": process_id, \"error.type\": \"start_failed\"})\n-                logger.error(f\"Failed to start process {process_id}: {e}\")\n-                raise\n-    \n-    def execute_process(self, instance_id: str, max_steps: int = 100) -> ProcessInstance:\n-        \"\"\"\n-        Execute a BPMN process instance\n-        \n-        Args:\n-            instance_id: ID of the process instance to execute\n-            max_steps: Maximum number of execution steps\n-            \n-        Returns:\n-            ProcessInstance: Updated process instance\n-        \"\"\"\n-        with self.tracer.start_as_current_span(\"execute_process\") as span:\n-            span.set_attributes({\n-                \"instance.id\": instance_id,\n-                \"execution.max_steps\": max_steps\n-            })\n-            \n-            try:\n-                # Get process instance\n-                if instance_id not in self.active_instances:\n-                    raise ValueError(f\"Process instance '{instance_id}' not found\")\n-                \n-                process_instance = self.active_instances[instance_id]\n-                workflow = process_instance.workflow\n-                \n-                # Execute workflow\n-                step_count = 0\n-                while not workflow.is_completed() and step_count < max_steps:\n-                    # Get ready tasks\n-                    ready_tasks = workflow.get_tasks(state=TaskStatus.READY.value)\n-                    \n-                    if not ready_tasks:\n-                        # No ready tasks, workflow might be waiting for external events\n-                        break\n-                    \n-                    # Execute each ready task\n-                    for task in ready_tasks:\n-                        self._execute_task(task, process_instance)\n-                        step_count += 1\n-                        \n-                        if workflow.is_completed():\n-                            break\n-                \n-                # Update process status\n-                if workflow.is_completed():\n-                    process_instance.status = ProcessStatus.COMPLETED\n-                    process_instance.end_time = datetime.utcnow()\n-                elif step_count >= max_steps:\n-                    process_instance.status = ProcessStatus.SUSPENDED\n-                \n-                # Update execution path\n-                process_instance.execution_path = self._get_execution_path(workflow)\n-                \n-                span.set_attributes({\n-                    \"execution.steps\": step_count,\n-                    \"instance.status\": process_instance.status.value\n-                })\n-                \n-                logger.info(f\"Executed process instance {instance_id} ({step_count} steps)\")\n-                return process_instance\n-                \n-            except Exception as e:\n-                span.set_status(Status(StatusCode.ERROR, str(e)))\n-                error_counter.add(1, {\"instance.id\": instance_id, \"error.type\": \"execution_failed\"})\n-                logger.error(f\"Failed to execute process instance {instance_id}: {e}\")\n-                raise\n-    \n-    def _execute_task(self, task: BpmnTaskSpec, process_instance: ProcessInstance) -> None:\n-        \"\"\"Execute a single BPMN task with telemetry\"\"\"\n-        with self.tracer.start_as_current_span(\"execute_task\") as span:\n-            task_start_time = datetime.utcnow()\n-            \n-            span.set_attributes({\n-                \"task.id\": task.id,\n-                \"task.name\": task.name,\n-                \"task.type\": task.__class__.__name__\n-            })\n-            \n-            try:\n-                # Create task execution context\n-                task_execution = TaskExecution(\n-                    task_id=task.id,\n-                    task_name=task.name,\n-                    task_type=task.__class__.__name__,\n-                    start_time=task_start_time,\n-                    status=TaskStatus.RUNNING,\n-                    input_data=dict(task.data) if hasattr(task, 'data') else {}\n-                )\n-                \n-                # Execute the task\n-                task.complete()\n-                \n-                # Update task execution\n-                task_execution.end_time = datetime.utcnow()\n-                task_execution.duration = (task_execution.end_time - task_execution.start_time).total_seconds()\n-                task_execution.status = TaskStatus.COMPLETED\n-                task_execution.output_data = dict(task.data) if hasattr(task, 'data') else {}\n-                \n-                # Record telemetry\n-                if self.enable_telemetry:\n-                    task_duration_histogram.record(\n-                        task_execution.duration,\n-                        {\n-                            \"task.id\": task.id,\n-                            \"task.type\": task_execution.task_type,\n-                            \"process.id\": process_instance.process_definition_id\n-                        }\n-                    )\n-                \n-                span.set_attributes({\n-                    \"task.duration\": task_execution.duration,\n-                    \"task.status\": TaskStatus.COMPLETED.value\n-                })\n-                \n-                logger.debug(f\"Executed task {task.id} ({task_execution.duration:.3f}s)\")\n-                \n-            except Exception as e:\n-                task_execution.end_time = datetime.utcnow()\n-                task_execution.duration = (task_execution.end_time - task_execution.start_time).total_seconds()\n-                task_execution.status = TaskStatus.FAILED\n-                task_execution.error_message = str(e)\n-                \n-                span.set_status(Status(StatusCode.ERROR, str(e)))\n-                error_counter.add(1, {\n-                    \"task.id\": task.id,\n-                    \"task.type\": task.__class__.__name__,\n-                    \"error.type\": \"task_execution_failed\"\n-                })\n-                \n-                logger.error(f\"Failed to execute task {task.id}: {e}\")\n-                raise\n-    \n-    def _get_execution_path(self, workflow: BpmnWorkflow) -> List[str]:\n-        \"\"\"Get the execution path of completed tasks\"\"\"\n-        try:\n-            completed_tasks = workflow.get_tasks(state=TaskStatus.COMPLETED.value)\n-            return [task.id for task in completed_tasks]\n-        except Exception as e:\n-            logger.warning(f\"Failed to get execution path: {e}\")\n-            return []\n-    \n-    def get_process_instance(self, instance_id: str) -> Optional[ProcessInstance]:\n-        \"\"\"Get a process instance by ID\"\"\"\n-        return self.active_instances.get(instance_id)\n-    \n-    def list_process_instances(self, \n-                             status: Optional[ProcessStatus] = None,\n-                             process_id: Optional[str] = None) -> List[ProcessInstance]:\n-        \"\"\"List process instances with optional filtering\"\"\"\n-        instances = list(self.active_instances.values())\n-        \n-        if status:\n-            instances = [i for i in instances if i.status == status]\n-        \n-        if process_id:\n-            instances = [i for i in instances if i.process_definition_id == process_id]\n-        \n-        return instances\n-    \n-    def cancel_process(self, instance_id: str) -> ProcessInstance:\n-        \"\"\"Cancel a running process instance\"\"\"\n-        with self.tracer.start_as_current_span(\"cancel_process\") as span:\n-            span.set_attributes({\"instance.id\": instance_id})\n-            \n-            try:\n-                if instance_id not in self.active_instances:\n-                    raise ValueError(f\"Process instance '{instance_id}' not found\")\n-                \n-                process_instance = self.active_instances[instance_id]\n-                \n-                if process_instance.status not in [ProcessStatus.RUNNING, ProcessStatus.SUSPENDED]:\n-                    raise ValueError(f\"Cannot cancel process in status: {process_instance.status}\")\n-                \n-                # Cancel the workflow\n-                process_instance.workflow.cancel()\n-                process_instance.status = ProcessStatus.CANCELLED\n-                process_instance.end_time = datetime.utcnow()\n-                \n-                span.set_attributes({\"instance.status\": ProcessStatus.CANCELLED.value})\n-                \n-                logger.info(f\"Cancelled process instance {instance_id}\")\n-                return process_instance\n-                \n-            except Exception as e:\n-                span.set_status(Status(StatusCode.ERROR, str(e)))\n-                error_counter.add(1, {\"instance.id\": instance_id, \"error.type\": \"cancel_failed\"})\n-                logger.error(f\"Failed to cancel process instance {instance_id}: {e}\")\n-                raise\n-    \n-    def get_process_variables(self, instance_id: str) -> Dict[str, Any]:\n-        \"\"\"Get process variables for an instance\"\"\"\n-        if instance_id not in self.active_instances:\n-            raise ValueError(f\"Process instance '{instance_id}' not found\")\n-        \n-        process_instance = self.active_instances[instance_id]\n-        return dict(process_instance.workflow.data)\n-    \n-    def set_process_variables(self, instance_id: str, variables: Dict[str, Any]) -> None:\n-        \"\"\"Set process variables for an instance\"\"\"\n-        if instance_id not in self.active_instances:\n-            raise ValueError(f\"Process instance '{instance_id}' not found\")\n-        \n-        process_instance = self.active_instances[instance_id]\n-        process_instance.workflow.data.update(variables)\n-        process_instance.variables.update(variables)\n-    \n-    def get_ready_tasks(self, instance_id: str) -> List[Dict[str, Any]]:\n-        \"\"\"Get ready tasks for a process instance\"\"\"\n-        if instance_id not in self.active_instances:\n-            raise ValueError(f\"Process instance '{instance_id}' not found\")\n-        \n-        process_instance = self.active_instances[instance_id]\n-        ready_tasks = process_instance.workflow.get_tasks(state=TaskStatus.READY.value)\n-        \n-        return [\n-            {\n-                \"task_id\": task.id,\n-                \"task_name\": task.name,\n-                \"task_type\": task.__class__.__name__,\n-                \"data\": dict(task.data) if hasattr(task, 'data') else {}\n-            }\n-            for task in ready_tasks\n-        ]\n-    \n-    def complete_task(self, instance_id: str, task_id: str, data: Optional[Dict[str, Any]] = None) -> None:\n-        \"\"\"Complete a specific task in a process instance\"\"\"\n-        with self.tracer.start_as_current_span(\"complete_task\") as span:\n-            span.set_attributes({\n-                \"instance.id\": instance_id,\n-                \"task.id\": task_id\n-            })\n-            \n-            try:\n-                if instance_id not in self.active_instances:\n-                    raise ValueError(f\"Process instance '{instance_id}' not found\")\n-                \n-                process_instance = self.active_instances[instance_id]\n-                workflow = process_instance.workflow\n-                \n-                # Find the task\n-                tasks = workflow.get_tasks()\n-                target_task = None\n-                \n-                for task in tasks:\n-                    if task.id == task_id:\n-                        target_task = task\n-                        break\n-                \n-                if not target_task:\n-                    raise ValueError(f\"Task '{task_id}' not found in process instance '{instance_id}'\")\n-                \n-                # Set task data if provided\n-                if data and hasattr(target_task, 'data'):\n-                    target_task.data.update(data)\n-                \n-                # Complete the task\n-                target_task.complete()\n-                \n-                span.set_attributes({\"task.status\": \"completed\"})\n-                \n-                logger.info(f\"Completed task {task_id} in process instance {instance_id}\")\n-                \n-            except Exception as e:\n-                span.set_status(Status(StatusCode.ERROR, str(e)))\n-                error_counter.add(1, {\n-                    \"instance.id\": instance_id,\n-                    \"task.id\": task_id,\n-                    \"error.type\": \"task_completion_failed\"\n-                })\n-                logger.error(f\"Failed to complete task {task_id} in process instance {instance_id}: {e}\")\n-                raise\n-    \n-    def serialize_workflow(self, instance_id: str) -> str:\n-        \"\"\"Serialize a workflow instance to JSON\"\"\"\n-        if instance_id not in self.active_instances:\n-            raise ValueError(f\"Process instance '{instance_id}' not found\")\n-        \n-        process_instance = self.active_instances[instance_id]\n-        serializer = BpmnWorkflowSerializer(BPMN_CONFIG)\n-        return serializer.serialize_workflow(process_instance.workflow)\n-    \n-    def deserialize_workflow(self, instance_id: str, serialized_data: str) -> ProcessInstance:\n-        \"\"\"Deserialize a workflow instance from JSON\"\"\"\n-        serializer = BpmnWorkflowSerializer(BPMN_CONFIG)\n-        workflow = serializer.deserialize_workflow(serialized_data)\n-        \n-        process_instance = ProcessInstance(\n-            instance_id=instance_id,\n-            process_definition_id=workflow.spec.name,\n-            workflow=workflow,\n-            start_time=datetime.utcnow(),\n-            status=ProcessStatus.RUNNING\n-        )\n-        \n-        self.active_instances[instance_id] = process_instance\n-        return process_instance\n-    \n-    def get_process_statistics(self) -> Dict[str, Any]:\n-        \"\"\"Get orchestrator statistics\"\"\"\n-        total_instances = len(self.active_instances)\n-        status_counts = {}\n-        \n-        for status in ProcessStatus:\n-            status_counts[status.value] = len([\n-                i for i in self.active_instances.values() \n-                if i.status == status\n-            ])\n-        \n-        return {\n-            \"total_process_definitions\": len(self.process_definitions),\n-            \"total_instances\": total_instances,\n-            \"status_distribution\": status_counts,\n-            \"process_definitions\": list(self.process_definitions.keys())\n-        }\n-    \n-    def cleanup_completed_processes(self, max_age_hours: int = 24) -> int:\n-        \"\"\"Clean up completed process instances older than specified age\"\"\"\n-        cutoff_time = datetime.utcnow() - timedelta(hours=max_age_hours)\n-        removed_count = 0\n-        \n-        instances_to_remove = []\n-        \n-        for instance_id, process_instance in self.active_instances.items():\n-            if (process_instance.status in [ProcessStatus.COMPLETED, ProcessStatus.FAILED, ProcessStatus.CANCELLED] and\n-                process_instance.end_time and process_instance.end_time < cutoff_time):\n-                instances_to_remove.append(instance_id)\n-        \n-        for instance_id in instances_to_remove:\n-            del self.active_instances[instance_id]\n-            removed_count += 1\n-        \n-        logger.info(f\"Cleaned up {removed_count} completed process instances\")\n-        return removed_count\n-\n-# Example usage and utility functions\n-def create_sample_bpmn_file() -> str:\n-    \"\"\"Create a sample BPMN file for testing\"\"\"\n-    sample_bpmn = '''<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n-<bpmn:definitions xmlns:bpmn=\"http://www.omg.org/spec/BPMN/20100524/MODEL\"\n-                  xmlns:bpmndi=\"http://www.omg.org/spec/BPMN/20100524/DI\"\n-                  xmlns:dc=\"http://www.omg.org/spec/DD/20100524/DC\"\n-                  xmlns:di=\"http://www.omg.org/spec/DD/20100524/DI\"\n-                  id=\"Definitions_1\"\n-                  targetNamespace=\"http://bpmn.io/schema/bpmn\">\n-  <bpmn:process id=\"Process_1\" isExecutable=\"true\">\n-    <bpmn:startEvent id=\"StartEvent_1\" name=\"Start\">\n-      <bpmn:outgoing>Flow_1</bpmn:outgoing>\n-    </bpmn:startEvent>\n-    <bpmn:task id=\"Task_1\" name=\"Sample Task\">\n-      <bpmn:incoming>Flow_1</bpmn:incoming>\n-      <bpmn:outgoing>Flow_2</bpmn:outgoing>\n-    </bpmn:task>\n-    <bpmn:endEvent id=\"EndEvent_1\" name=\"End\">\n-      <bpmn:incoming>Flow_2</bpmn:incoming>\n-    </bpmn:endEvent>\n-    <bpmn:sequenceFlow id=\"Flow_1\" sourceRef=\"StartEvent_1\" targetRef=\"Task_1\" />\n-    <bpmn:sequenceFlow id=\"Flow_2\" sourceRef=\"Task_1\" targetRef=\"EndEvent_1\" />\n-  </bpmn:process>\n-  <bpmndi:BPMNDiagram id=\"BPMNDiagram_1\">\n-    <bpmndi:BPMNPlane id=\"BPMNPlane_1\" bpmnElement=\"Process_1\">\n-      <bpmndi:BPMNShape id=\"StartEvent_1_di\" bpmnElement=\"StartEvent_1\">\n-        <dc:Bounds x=\"152\" y=\"102\" width=\"36\" height=\"36\" />\n-      </bpmndi:BPMNShape>\n-      <bpmndi:BPMNShape id=\"Task_1_di\" bpmnElement=\"Task_1\">\n-        <dc:Bounds x=\"240\" y=\"80\" width=\"100\" height=\"80\" />\n-      </bpmndi:BPMNShape>\n-      <bpmndi:BPMNShape id=\"EndEvent_1_di\" bpmnElement=\"EndEvent_1\">\n-        <dc:Bounds x=\"392\" y=\"102\" width=\"36\" height=\"36\" />\n-      </bpmndi:BPMNShape>\n-      <bpmndi:BPMNEdge id=\"Flow_1_di\" bpmnElement=\"Flow_1\">\n-        <di:waypoint x=\"188\" y=\"120\" />\n-        <di:waypoint x=\"240\" y=\"120\" />\n-      </bpmndi:BPMNEdge>\n-      <bpmndi:BPMNEdge id=\"Flow_2_di\" bpmnElement=\"Flow_2\">\n-        <di:waypoint x=\"340\" y=\"120\" />\n-        <di:waypoint x=\"392\" y=\"120\" />\n-      </bpmndi:BPMNEdge>\n-    </bpmndi:BPMNPlane>\n-  </bpmndi:BPMNDiagram>\n-</bpmn:definitions>'''\n-    \n-    return sample_bpmn\n-\n-if __name__ == \"__main__\":\n-    # Example usage\n-    print(\"AutoTel BPMN Orchestrator - SpiffWorkflow Integration\")\n-    print(\"=\" * 60)\n-    \n-    # Create sample BPMN file\n-    sample_bpmn = create_sample_bpmn_file()\n-    bpmn_path = Path(\"bpmn\")\n-    bpmn_path.mkdir(exist_ok=True)\n-    \n-    with open(bpmn_path / \"sample_process.bpmn\", \"w\") as f:\n-        f.write(sample_bpmn)\n-    \n-    # Initialize orchestrator\n-    orchestrator = BPMNOrchestrator(bpmn_files_path=\"bpmn\")\n-    \n-    # Start a process\n-    instance = orchestrator.start_process(\"Process_1\", {\"input\": \"test\"})\n-    print(f\"Started process instance: {instance.instance_id}\")\n-    \n-    # Execute the process\n-    result = orchestrator.execute_process(instance.instance_id)\n-    print(f\"Process execution completed. Status: {result.status.value}\")\n-    \n-    # Get statistics\n-    stats = orchestrator.get_process_statistics()\n     print(f\"Orchestrator statistics: {stats}\")\n\\ No newline at end of file\n"
                },
                {
                    "date": 1752191787524,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -0,0 +1,663 @@\n+#!/usr/bin/env python3\n+\"\"\"\n+AutoTel BPMN Orchestrator - Enterprise BPMN 2.0 Execution Engine\n+Powered by SpiffWorkflow with zero-touch telemetry integration\n+\"\"\"\n+\n+import json\n+import uuid\n+import logging\n+from datetime import datetime, timedelta\n+from typing import Dict, List, Any, Optional, Callable, Union\n+from dataclasses import dataclass, field\n+from enum import Enum\n+from pathlib import Path\n+import xml.etree.ElementTree as ET\n+\n+# SpiffWorkflow imports\n+from SpiffWorkflow.bpmn import BpmnWorkflow\n+from SpiffWorkflow.bpmn.parser.BpmnParser import BpmnParser\n+from SpiffWorkflow.bpmn.specs.bpmn_task_spec import BpmnTaskSpec\n+from SpiffWorkflow.bpmn.specs.bpmn_process_spec import BpmnProcessSpec\n+from SpiffWorkflow.bpmn.util.task import BpmnTaskFilter\n+from SpiffWorkflow.bpmn.util.subworkflow import BpmnSubWorkflow\n+from SpiffWorkflow.bpmn.serializer import BpmnWorkflowSerializer\n+from SpiffWorkflow.bpmn.serializer.config import DEFAULT_CONFIG\n+\n+# OpenTelemetry for observability\n+from opentelemetry import trace, metrics\n+from opentelemetry.trace import Status, StatusCode\n+from opentelemetry.metrics import Counter, Histogram\n+from opentelemetry.sdk.trace import TracerProvider\n+from opentelemetry.sdk.trace.export import ConsoleSpanExporter, BatchSpanProcessor\n+from opentelemetry.sdk.metrics import MeterProvider\n+from opentelemetry.sdk.metrics.export import ConsoleMetricExporter, PeriodicExportingMetricReader\n+\n+# Configure logging\n+logging.basicConfig(level=logging.INFO)\n+logger = logging.getLogger(__name__)\n+\n+# Configure OpenTelemetry\n+trace.set_tracer_provider(TracerProvider())\n+trace.get_tracer_provider().add_span_processor(\n+    BatchSpanProcessor(ConsoleSpanExporter())\n+)\n+\n+metrics.set_meter_provider(MeterProvider())\n+meter = metrics.get_meter(__name__)\n+\n+# Telemetry instruments\n+process_counter = meter.create_counter(\n+    name=\"bpmn_processes_total\",\n+    description=\"Total number of BPMN processes executed\"\n+)\n+\n+task_duration_histogram = meter.create_histogram(\n+    name=\"bpmn_task_duration_seconds\",\n+    description=\"Duration of BPMN tasks in seconds\"\n+)\n+\n+error_counter = meter.create_counter(\n+    name=\"bpmn_errors_total\",\n+    description=\"Total number of BPMN execution errors\"\n+)\n+\n+class ProcessStatus(Enum):\n+    \"\"\"BPMN Process execution status\"\"\"\n+    PENDING = \"pending\"\n+    RUNNING = \"running\"\n+    COMPLETED = \"completed\"\n+    FAILED = \"failed\"\n+    SUSPENDED = \"suspended\"\n+    CANCELLED = \"cancelled\"\n+\n+class TaskStatus(Enum):\n+    \"\"\"BPMN Task execution status\"\"\"\n+    PENDING = \"pending\"\n+    READY = \"ready\"\n+    RUNNING = \"running\"\n+    COMPLETED = \"completed\"\n+    FAILED = \"failed\"\n+    SKIPPED = \"skipped\"\n+    CANCELLED = \"cancelled\"\n+\n+@dataclass\n+class ProcessInstance:\n+    \"\"\"BPMN Process instance with execution state and telemetry\"\"\"\n+    instance_id: str\n+    process_definition_id: str\n+    workflow: BpmnWorkflow\n+    start_time: datetime\n+    end_time: Optional[datetime] = None\n+    status: ProcessStatus = ProcessStatus.PENDING\n+    variables: Dict[str, Any] = field(default_factory=dict)\n+    execution_path: List[str] = field(default_factory=list)\n+    error_message: Optional[str] = None\n+    telemetry_data: Dict[str, Any] = field(default_factory=dict)\n+    \n+    def to_dict(self) -> Dict[str, Any]:\n+        \"\"\"Convert process instance to dictionary for serialization\"\"\"\n+        return {\n+            \"instance_id\": self.instance_id,\n+            \"process_definition_id\": self.process_definition_id,\n+            \"start_time\": self.start_time.isoformat(),\n+            \"end_time\": self.end_time.isoformat() if self.end_time else None,\n+            \"status\": self.status.value,\n+            \"variables\": self.variables,\n+            \"execution_path\": self.execution_path,\n+            \"error_message\": self.error_message,\n+            \"telemetry_data\": self.telemetry_data\n+        }\n+\n+@dataclass\n+class TaskExecution:\n+    \"\"\"BPMN Task execution context with telemetry\"\"\"\n+    task_id: str\n+    task_name: str\n+    task_type: str\n+    start_time: datetime\n+    end_time: Optional[datetime] = None\n+    status: TaskStatus = TaskStatus.PENDING\n+    duration: Optional[float] = None\n+    error_message: Optional[str] = None\n+    input_data: Dict[str, Any] = field(default_factory=dict)\n+    output_data: Dict[str, Any] = field(default_factory=dict)\n+    telemetry_data: Dict[str, Any] = field(default_factory=dict)\n+\n+class BPMNOrchestrator:\n+    \"\"\"\n+    Enterprise-grade BPMN 2.0 Orchestrator powered by SpiffWorkflow\n+    \n+    Features:\n+    - Full BPMN 2.0 specification compliance\n+    - Zero-touch telemetry integration\n+    - Process persistence and recovery\n+    - Advanced task execution monitoring\n+    - Multi-instance process support\n+    - Event-driven execution\n+    - Error handling and recovery\n+    \"\"\"\n+    \n+    def __init__(self, \n+                 bpmn_files_path: Optional[str] = None,\n+                 enable_telemetry: bool = True,\n+                 enable_persistence: bool = True):\n+        \"\"\"\n+        Initialize the BPMN Orchestrator\n+        \n+        Args:\n+            bpmn_files_path: Path to BPMN XML files\n+            enable_telemetry: Enable OpenTelemetry integration\n+            enable_persistence: Enable process state persistence\n+        \"\"\"\n+        self.tracer = trace.get_tracer(__name__)\n+        self.bpmn_files_path = Path(bpmn_files_path) if bpmn_files_path else Path(\"bpmn\")\n+        self.enable_telemetry = enable_telemetry\n+        self.enable_persistence = enable_persistence\n+        \n+        # Initialize SpiffWorkflow parser\n+        self.parser = BpmnParser()\n+        self.parser.add_bpmn_files_by_glob(str(self.bpmn_files_path / \"*.bpmn\"))\n+        \n+        # Process definitions cache\n+        self.process_definitions: Dict[str, BpmnProcessSpec] = {}\n+        self.active_instances: Dict[str, ProcessInstance] = {}\n+        \n+        # Load process definitions\n+        self._load_process_definitions()\n+        \n+        logger.info(f\"BPMN Orchestrator initialized with {len(self.process_definitions)} process definitions\")\n+    \n+    def _load_process_definitions(self) -> None:\n+        \"\"\"Load all BPMN process definitions from the configured path\"\"\"\n+        with self.tracer.start_as_current_span(\"load_process_definitions\") as span:\n+            try:\n+                # Get all process specs from the parser\n+                for process_id, process_spec in self.parser.get_process_specs().items():\n+                    self.process_definitions[process_id] = process_spec\n+                    span.add_event(f\"Loaded process definition: {process_id}\")\n+                \n+                logger.info(f\"Loaded {len(self.process_definitions)} process definitions\")\n+                \n+            except Exception as e:\n+                span.set_status(Status(StatusCode.ERROR, str(e)))\n+                logger.error(f\"Failed to load process definitions: {e}\")\n+                raise\n+    \n+    def start_process(self, \n+                     process_id: str, \n+                     variables: Optional[Dict[str, Any]] = None,\n+                     instance_id: Optional[str] = None) -> ProcessInstance:\n+        \"\"\"\n+        Start a new BPMN process instance\n+        \n+        Args:\n+            process_id: ID of the process to start\n+            variables: Initial process variables\n+            instance_id: Optional custom instance ID\n+            \n+        Returns:\n+            ProcessInstance: The started process instance\n+        \"\"\"\n+        with self.tracer.start_as_current_span(\"start_process\") as span:\n+            span.set_attributes({\n+                \"process.id\": process_id,\n+                \"process.variables\": json.dumps(variables or {})\n+            })\n+            \n+            try:\n+                # Validate process exists\n+                if process_id not in self.process_definitions:\n+                    raise ValueError(f\"Process definition '{process_id}' not found\")\n+                \n+                # Generate instance ID if not provided\n+                if not instance_id:\n+                    instance_id = f\"{process_id}_{uuid.uuid4().hex[:8]}\"\n+                \n+                # Create workflow instance\n+                workflow = BpmnWorkflow(self.process_definitions[process_id])\n+                \n+                # Set initial variables\n+                if variables:\n+                    workflow.data.update(variables)\n+                \n+                # Create process instance\n+                process_instance = ProcessInstance(\n+                    instance_id=instance_id,\n+                    process_definition_id=process_id,\n+                    workflow=workflow,\n+                    start_time=datetime.utcnow(),\n+                    variables=variables or {},\n+                    status=ProcessStatus.RUNNING\n+                )\n+                \n+                # Store instance\n+                self.active_instances[instance_id] = process_instance\n+                \n+                # Update telemetry\n+                process_counter.add(1, {\"process.id\": process_id, \"action\": \"start\"})\n+                \n+                span.set_attributes({\n+                    \"instance.id\": instance_id,\n+                    \"instance.status\": ProcessStatus.RUNNING.value\n+                })\n+                \n+                logger.info(f\"Started process instance {instance_id} for process {process_id}\")\n+                return process_instance\n+                \n+            except Exception as e:\n+                span.set_status(Status(StatusCode.ERROR, str(e)))\n+                error_counter.add(1, {\"process.id\": process_id, \"error.type\": \"start_failed\"})\n+                logger.error(f\"Failed to start process {process_id}: {e}\")\n+                raise\n+    \n+    def execute_process(self, instance_id: str, max_steps: int = 100) -> ProcessInstance:\n+        \"\"\"\n+        Execute a BPMN process instance\n+        \n+        Args:\n+            instance_id: ID of the process instance to execute\n+            max_steps: Maximum number of execution steps\n+            \n+        Returns:\n+            ProcessInstance: Updated process instance\n+        \"\"\"\n+        with self.tracer.start_as_current_span(\"execute_process\") as span:\n+            span.set_attributes({\n+                \"instance.id\": instance_id,\n+                \"execution.max_steps\": max_steps\n+            })\n+            \n+            try:\n+                # Get process instance\n+                if instance_id not in self.active_instances:\n+                    raise ValueError(f\"Process instance '{instance_id}' not found\")\n+                \n+                process_instance = self.active_instances[instance_id]\n+                workflow = process_instance.workflow\n+                \n+                # Execute workflow\n+                step_count = 0\n+                while not workflow.is_completed() and step_count < max_steps:\n+                    # Get ready tasks\n+                    ready_tasks = workflow.get_tasks(state=TaskStatus.READY.value)\n+                    \n+                    if not ready_tasks:\n+                        # No ready tasks, workflow might be waiting for external events\n+                        break\n+                    \n+                    # Execute each ready task\n+                    for task in ready_tasks:\n+                        self._execute_task(task, process_instance)\n+                        step_count += 1\n+                        \n+                        if workflow.is_completed():\n+                            break\n+                \n+                # Update process status\n+                if workflow.is_completed():\n+                    process_instance.status = ProcessStatus.COMPLETED\n+                    process_instance.end_time = datetime.utcnow()\n+                elif step_count >= max_steps:\n+                    process_instance.status = ProcessStatus.SUSPENDED\n+                \n+                # Update execution path\n+                process_instance.execution_path = self._get_execution_path(workflow)\n+                \n+                span.set_attributes({\n+                    \"execution.steps\": step_count,\n+                    \"instance.status\": process_instance.status.value\n+                })\n+                \n+                logger.info(f\"Executed process instance {instance_id} ({step_count} steps)\")\n+                return process_instance\n+                \n+            except Exception as e:\n+                span.set_status(Status(StatusCode.ERROR, str(e)))\n+                error_counter.add(1, {\"instance.id\": instance_id, \"error.type\": \"execution_failed\"})\n+                logger.error(f\"Failed to execute process instance {instance_id}: {e}\")\n+                raise\n+    \n+    def _execute_task(self, task: BpmnTaskSpec, process_instance: ProcessInstance) -> None:\n+        \"\"\"Execute a single BPMN task with telemetry\"\"\"\n+        with self.tracer.start_as_current_span(\"execute_task\") as span:\n+            task_start_time = datetime.utcnow()\n+            \n+            span.set_attributes({\n+                \"task.id\": task.id,\n+                \"task.name\": task.name,\n+                \"task.type\": task.__class__.__name__\n+            })\n+            \n+            try:\n+                # Create task execution context\n+                task_execution = TaskExecution(\n+                    task_id=task.id,\n+                    task_name=task.name,\n+                    task_type=task.__class__.__name__,\n+                    start_time=task_start_time,\n+                    status=TaskStatus.RUNNING,\n+                    input_data=dict(task.data) if hasattr(task, 'data') else {}\n+                )\n+                \n+                # Execute the task\n+                task.complete()\n+                \n+                # Update task execution\n+                task_execution.end_time = datetime.utcnow()\n+                task_execution.duration = (task_execution.end_time - task_execution.start_time).total_seconds()\n+                task_execution.status = TaskStatus.COMPLETED\n+                task_execution.output_data = dict(task.data) if hasattr(task, 'data') else {}\n+                \n+                # Record telemetry\n+                if self.enable_telemetry:\n+                    task_duration_histogram.record(\n+                        task_execution.duration,\n+                        {\n+                            \"task.id\": task.id,\n+                            \"task.type\": task_execution.task_type,\n+                            \"process.id\": process_instance.process_definition_id\n+                        }\n+                    )\n+                \n+                span.set_attributes({\n+                    \"task.duration\": task_execution.duration,\n+                    \"task.status\": TaskStatus.COMPLETED.value\n+                })\n+                \n+                logger.debug(f\"Executed task {task.id} ({task_execution.duration:.3f}s)\")\n+                \n+            except Exception as e:\n+                task_execution.end_time = datetime.utcnow()\n+                task_execution.duration = (task_execution.end_time - task_execution.start_time).total_seconds()\n+                task_execution.status = TaskStatus.FAILED\n+                task_execution.error_message = str(e)\n+                \n+                span.set_status(Status(StatusCode.ERROR, str(e)))\n+                error_counter.add(1, {\n+                    \"task.id\": task.id,\n+                    \"task.type\": task.__class__.__name__,\n+                    \"error.type\": \"task_execution_failed\"\n+                })\n+                \n+                logger.error(f\"Failed to execute task {task.id}: {e}\")\n+                raise\n+    \n+    def _get_execution_path(self, workflow: BpmnWorkflow) -> List[str]:\n+        \"\"\"Get the execution path of completed tasks\"\"\"\n+        try:\n+            completed_tasks = workflow.get_tasks(state=TaskStatus.COMPLETED.value)\n+            return [task.id for task in completed_tasks]\n+        except Exception as e:\n+            logger.warning(f\"Failed to get execution path: {e}\")\n+            return []\n+    \n+    def get_process_instance(self, instance_id: str) -> Optional[ProcessInstance]:\n+        \"\"\"Get a process instance by ID\"\"\"\n+        return self.active_instances.get(instance_id)\n+    \n+    def list_process_instances(self, \n+                             status: Optional[ProcessStatus] = None,\n+                             process_id: Optional[str] = None) -> List[ProcessInstance]:\n+        \"\"\"List process instances with optional filtering\"\"\"\n+        instances = list(self.active_instances.values())\n+        \n+        if status:\n+            instances = [i for i in instances if i.status == status]\n+        \n+        if process_id:\n+            instances = [i for i in instances if i.process_definition_id == process_id]\n+        \n+        return instances\n+    \n+    def cancel_process(self, instance_id: str) -> ProcessInstance:\n+        \"\"\"Cancel a running process instance\"\"\"\n+        with self.tracer.start_as_current_span(\"cancel_process\") as span:\n+            span.set_attributes({\"instance.id\": instance_id})\n+            \n+            try:\n+                if instance_id not in self.active_instances:\n+                    raise ValueError(f\"Process instance '{instance_id}' not found\")\n+                \n+                process_instance = self.active_instances[instance_id]\n+                \n+                if process_instance.status not in [ProcessStatus.RUNNING, ProcessStatus.SUSPENDED]:\n+                    raise ValueError(f\"Cannot cancel process in status: {process_instance.status}\")\n+                \n+                # Cancel the workflow\n+                process_instance.workflow.cancel()\n+                process_instance.status = ProcessStatus.CANCELLED\n+                process_instance.end_time = datetime.utcnow()\n+                \n+                span.set_attributes({\"instance.status\": ProcessStatus.CANCELLED.value})\n+                \n+                logger.info(f\"Cancelled process instance {instance_id}\")\n+                return process_instance\n+                \n+            except Exception as e:\n+                span.set_status(Status(StatusCode.ERROR, str(e)))\n+                error_counter.add(1, {\"instance.id\": instance_id, \"error.type\": \"cancel_failed\"})\n+                logger.error(f\"Failed to cancel process instance {instance_id}: {e}\")\n+                raise\n+    \n+    def get_process_variables(self, instance_id: str) -> Dict[str, Any]:\n+        \"\"\"Get process variables for an instance\"\"\"\n+        if instance_id not in self.active_instances:\n+            raise ValueError(f\"Process instance '{instance_id}' not found\")\n+        \n+        process_instance = self.active_instances[instance_id]\n+        return dict(process_instance.workflow.data)\n+    \n+    def set_process_variables(self, instance_id: str, variables: Dict[str, Any]) -> None:\n+        \"\"\"Set process variables for an instance\"\"\"\n+        if instance_id not in self.active_instances:\n+            raise ValueError(f\"Process instance '{instance_id}' not found\")\n+        \n+        process_instance = self.active_instances[instance_id]\n+        process_instance.workflow.data.update(variables)\n+        process_instance.variables.update(variables)\n+    \n+    def get_ready_tasks(self, instance_id: str) -> List[Dict[str, Any]]:\n+        \"\"\"Get ready tasks for a process instance\"\"\"\n+        if instance_id not in self.active_instances:\n+            raise ValueError(f\"Process instance '{instance_id}' not found\")\n+        \n+        process_instance = self.active_instances[instance_id]\n+        ready_tasks = process_instance.workflow.get_tasks(state=TaskStatus.READY.value)\n+        \n+        return [\n+            {\n+                \"task_id\": task.id,\n+                \"task_name\": task.name,\n+                \"task_type\": task.__class__.__name__,\n+                \"data\": dict(task.data) if hasattr(task, 'data') else {}\n+            }\n+            for task in ready_tasks\n+        ]\n+    \n+    def complete_task(self, instance_id: str, task_id: str, data: Optional[Dict[str, Any]] = None) -> None:\n+        \"\"\"Complete a specific task in a process instance\"\"\"\n+        with self.tracer.start_as_current_span(\"complete_task\") as span:\n+            span.set_attributes({\n+                \"instance.id\": instance_id,\n+                \"task.id\": task_id\n+            })\n+            \n+            try:\n+                if instance_id not in self.active_instances:\n+                    raise ValueError(f\"Process instance '{instance_id}' not found\")\n+                \n+                process_instance = self.active_instances[instance_id]\n+                workflow = process_instance.workflow\n+                \n+                # Find the task\n+                tasks = workflow.get_tasks()\n+                target_task = None\n+                \n+                for task in tasks:\n+                    if task.id == task_id:\n+                        target_task = task\n+                        break\n+                \n+                if not target_task:\n+                    raise ValueError(f\"Task '{task_id}' not found in process instance '{instance_id}'\")\n+                \n+                # Set task data if provided\n+                if data and hasattr(target_task, 'data'):\n+                    target_task.data.update(data)\n+                \n+                # Complete the task\n+                target_task.complete()\n+                \n+                span.set_attributes({\"task.status\": \"completed\"})\n+                \n+                logger.info(f\"Completed task {task_id} in process instance {instance_id}\")\n+                \n+            except Exception as e:\n+                span.set_status(Status(StatusCode.ERROR, str(e)))\n+                error_counter.add(1, {\n+                    \"instance.id\": instance_id,\n+                    \"task.id\": task_id,\n+                    \"error.type\": \"task_completion_failed\"\n+                })\n+                logger.error(f\"Failed to complete task {task_id} in process instance {instance_id}: {e}\")\n+                raise\n+    \n+    def serialize_workflow(self, instance_id: str) -> str:\n+        \"\"\"Serialize a workflow instance to JSON\"\"\"\n+        if instance_id not in self.active_instances:\n+            raise ValueError(f\"Process instance '{instance_id}' not found\")\n+        \n+        process_instance = self.active_instances[instance_id]\n+        serializer = BpmnWorkflowSerializer(DEFAULT_CONFIG)\n+        return serializer.serialize_workflow(process_instance.workflow)\n+    \n+    def deserialize_workflow(self, instance_id: str, serialized_data: str) -> ProcessInstance:\n+        \"\"\"Deserialize a workflow instance from JSON\"\"\"\n+        serializer = BpmnWorkflowSerializer(DEFAULT_CONFIG)\n+        workflow = serializer.deserialize_workflow(serialized_data)\n+        \n+        process_instance = ProcessInstance(\n+            instance_id=instance_id,\n+            process_definition_id=workflow.spec.name,\n+            workflow=workflow,\n+            start_time=datetime.utcnow(),\n+            status=ProcessStatus.RUNNING\n+        )\n+        \n+        self.active_instances[instance_id] = process_instance\n+        return process_instance\n+    \n+    def get_process_statistics(self) -> Dict[str, Any]:\n+        \"\"\"Get orchestrator statistics\"\"\"\n+        total_instances = len(self.active_instances)\n+        status_counts = {}\n+        \n+        for status in ProcessStatus:\n+            status_counts[status.value] = len([\n+                i for i in self.active_instances.values() \n+                if i.status == status\n+            ])\n+        \n+        return {\n+            \"total_process_definitions\": len(self.process_definitions),\n+            \"total_instances\": total_instances,\n+            \"status_distribution\": status_counts,\n+            \"process_definitions\": list(self.process_definitions.keys())\n+        }\n+    \n+    def cleanup_completed_processes(self, max_age_hours: int = 24) -> int:\n+        \"\"\"Clean up completed process instances older than specified age\"\"\"\n+        cutoff_time = datetime.utcnow() - timedelta(hours=max_age_hours)\n+        removed_count = 0\n+        \n+        instances_to_remove = []\n+        \n+        for instance_id, process_instance in self.active_instances.items():\n+            if (process_instance.status in [ProcessStatus.COMPLETED, ProcessStatus.FAILED, ProcessStatus.CANCELLED] and\n+                process_instance.end_time and process_instance.end_time < cutoff_time):\n+                instances_to_remove.append(instance_id)\n+        \n+        for instance_id in instances_to_remove:\n+            del self.active_instances[instance_id]\n+            removed_count += 1\n+        \n+        logger.info(f\"Cleaned up {removed_count} completed process instances\")\n+        return removed_count\n+\n+# Example usage and utility functions\n+def create_sample_bpmn_file() -> str:\n+    \"\"\"Create a sample BPMN file for testing\"\"\"\n+    sample_bpmn = '''<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n+<bpmn:definitions xmlns:bpmn=\"http://www.omg.org/spec/BPMN/20100524/MODEL\"\n+                  xmlns:bpmndi=\"http://www.omg.org/spec/BPMN/20100524/DI\"\n+                  xmlns:dc=\"http://www.omg.org/spec/DD/20100524/DC\"\n+                  xmlns:di=\"http://www.omg.org/spec/DD/20100524/DI\"\n+                  id=\"Definitions_1\"\n+                  targetNamespace=\"http://bpmn.io/schema/bpmn\">\n+  <bpmn:process id=\"Process_1\" isExecutable=\"true\">\n+    <bpmn:startEvent id=\"StartEvent_1\" name=\"Start\">\n+      <bpmn:outgoing>Flow_1</bpmn:outgoing>\n+    </bpmn:startEvent>\n+    <bpmn:task id=\"Task_1\" name=\"Sample Task\">\n+      <bpmn:incoming>Flow_1</bpmn:incoming>\n+      <bpmn:outgoing>Flow_2</bpmn:outgoing>\n+    </bpmn:task>\n+    <bpmn:endEvent id=\"EndEvent_1\" name=\"End\">\n+      <bpmn:incoming>Flow_2</bpmn:incoming>\n+    </bpmn:endEvent>\n+    <bpmn:sequenceFlow id=\"Flow_1\" sourceRef=\"StartEvent_1\" targetRef=\"Task_1\" />\n+    <bpmn:sequenceFlow id=\"Flow_2\" sourceRef=\"Task_1\" targetRef=\"EndEvent_1\" />\n+  </bpmn:process>\n+  <bpmndi:BPMNDiagram id=\"BPMNDiagram_1\">\n+    <bpmndi:BPMNPlane id=\"BPMNPlane_1\" bpmnElement=\"Process_1\">\n+      <bpmndi:BPMNShape id=\"StartEvent_1_di\" bpmnElement=\"StartEvent_1\">\n+        <dc:Bounds x=\"152\" y=\"102\" width=\"36\" height=\"36\" />\n+      </bpmndi:BPMNShape>\n+      <bpmndi:BPMNShape id=\"Task_1_di\" bpmnElement=\"Task_1\">\n+        <dc:Bounds x=\"240\" y=\"80\" width=\"100\" height=\"80\" />\n+      </bpmndi:BPMNShape>\n+      <bpmndi:BPMNShape id=\"EndEvent_1_di\" bpmnElement=\"EndEvent_1\">\n+        <dc:Bounds x=\"392\" y=\"102\" width=\"36\" height=\"36\" />\n+      </bpmndi:BPMNShape>\n+      <bpmndi:BPMNEdge id=\"Flow_1_di\" bpmnElement=\"Flow_1\">\n+        <di:waypoint x=\"188\" y=\"120\" />\n+        <di:waypoint x=\"240\" y=\"120\" />\n+      </bpmndi:BPMNEdge>\n+      <bpmndi:BPMNEdge id=\"Flow_2_di\" bpmnElement=\"Flow_2\">\n+        <di:waypoint x=\"340\" y=\"120\" />\n+        <di:waypoint x=\"392\" y=\"120\" />\n+      </bpmndi:BPMNEdge>\n+    </bpmndi:BPMNPlane>\n+  </bpmndi:BPMNDiagram>\n+</bpmn:definitions>'''\n+    \n+    return sample_bpmn\n+\n+if __name__ == \"__main__\":\n+    # Example usage\n+    print(\"AutoTel BPMN Orchestrator - SpiffWorkflow Integration\")\n+    print(\"=\" * 60)\n+    \n+    # Create sample BPMN file\n+    sample_bpmn = create_sample_bpmn_file()\n+    bpmn_path = Path(\"bpmn\")\n+    bpmn_path.mkdir(exist_ok=True)\n+    \n+    with open(bpmn_path / \"sample_process.bpmn\", \"w\") as f:\n+        f.write(sample_bpmn)\n+    \n+    # Initialize orchestrator\n+    orchestrator = BPMNOrchestrator(bpmn_files_path=\"bpmn\")\n+    \n+    # Start a process\n+    instance = orchestrator.start_process(\"Process_1\", {\"input\": \"test\"})\n+    print(f\"Started process instance: {instance.instance_id}\")\n+    \n+    # Execute the process\n+    result = orchestrator.execute_process(instance.instance_id)\n+    print(f\"Process execution completed. Status: {result.status.value}\")\n+    \n+    # Get statistics\n+    stats = orchestrator.get_process_statistics()\n+    print(f\"Orchestrator statistics: {stats}\")\n\\ No newline at end of file\n"
                },
                {
                    "date": 1752191808218,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -172,9 +172,9 @@\n         \"\"\"Load all BPMN process definitions from the configured path\"\"\"\n         with self.tracer.start_as_current_span(\"load_process_definitions\") as span:\n             try:\n                 # Get all process specs from the parser\n-                for process_id, process_spec in self.parser.get_process_specs().items():\n+                for process_id, process_spec in self.parser.find_all_specs().items():\n                     self.process_definitions[process_id] = process_spec\n                     span.add_event(f\"Loaded process definition: {process_id}\")\n                 \n                 logger.info(f\"Loaded {len(self.process_definitions)} process definitions\")\n"
                },
                {
                    "date": 1752191833149,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -0,0 +1,664 @@\n+#!/usr/bin/env python3\n+\"\"\"\n+AutoTel BPMN Orchestrator - Enterprise BPMN 2.0 Execution Engine\n+Powered by SpiffWorkflow with zero-touch telemetry integration\n+\"\"\"\n+\n+import json\n+import uuid\n+import logging\n+from datetime import datetime, timedelta\n+from typing import Dict, List, Any, Optional, Callable, Union\n+from dataclasses import dataclass, field\n+from enum import Enum\n+from pathlib import Path\n+import xml.etree.ElementTree as ET\n+\n+# SpiffWorkflow imports\n+from SpiffWorkflow.bpmn import BpmnWorkflow\n+from SpiffWorkflow.bpmn.parser.BpmnParser import BpmnParser\n+from SpiffWorkflow.bpmn.specs.bpmn_task_spec import BpmnTaskSpec\n+from SpiffWorkflow.bpmn.specs.bpmn_process_spec import BpmnProcessSpec\n+from SpiffWorkflow.bpmn.util.task import BpmnTaskFilter\n+from SpiffWorkflow.bpmn.util.subworkflow import BpmnSubWorkflow\n+from SpiffWorkflow.bpmn.serializer import BpmnWorkflowSerializer\n+from SpiffWorkflow.bpmn.serializer.config import DEFAULT_CONFIG\n+from SpiffWorkflow.util.task import TaskState  # <-- Add this import\n+\n+# OpenTelemetry for observability\n+from opentelemetry import trace, metrics\n+from opentelemetry.trace import Status, StatusCode\n+from opentelemetry.metrics import Counter, Histogram\n+from opentelemetry.sdk.trace import TracerProvider\n+from opentelemetry.sdk.trace.export import ConsoleSpanExporter, BatchSpanProcessor\n+from opentelemetry.sdk.metrics import MeterProvider\n+from opentelemetry.sdk.metrics.export import ConsoleMetricExporter, PeriodicExportingMetricReader\n+\n+# Configure logging\n+logging.basicConfig(level=logging.INFO)\n+logger = logging.getLogger(__name__)\n+\n+# Configure OpenTelemetry\n+trace.set_tracer_provider(TracerProvider())\n+trace.get_tracer_provider().add_span_processor(\n+    BatchSpanProcessor(ConsoleSpanExporter())\n+)\n+\n+metrics.set_meter_provider(MeterProvider())\n+meter = metrics.get_meter(__name__)\n+\n+# Telemetry instruments\n+process_counter = meter.create_counter(\n+    name=\"bpmn_processes_total\",\n+    description=\"Total number of BPMN processes executed\"\n+)\n+\n+task_duration_histogram = meter.create_histogram(\n+    name=\"bpmn_task_duration_seconds\",\n+    description=\"Duration of BPMN tasks in seconds\"\n+)\n+\n+error_counter = meter.create_counter(\n+    name=\"bpmn_errors_total\",\n+    description=\"Total number of BPMN execution errors\"\n+)\n+\n+class ProcessStatus(Enum):\n+    \"\"\"BPMN Process execution status\"\"\"\n+    PENDING = \"pending\"\n+    RUNNING = \"running\"\n+    COMPLETED = \"completed\"\n+    FAILED = \"failed\"\n+    SUSPENDED = \"suspended\"\n+    CANCELLED = \"cancelled\"\n+\n+class TaskStatus(Enum):\n+    \"\"\"BPMN Task execution status\"\"\"\n+    PENDING = \"pending\"\n+    READY = \"ready\"\n+    RUNNING = \"running\"\n+    COMPLETED = \"completed\"\n+    FAILED = \"failed\"\n+    SKIPPED = \"skipped\"\n+    CANCELLED = \"cancelled\"\n+\n+@dataclass\n+class ProcessInstance:\n+    \"\"\"BPMN Process instance with execution state and telemetry\"\"\"\n+    instance_id: str\n+    process_definition_id: str\n+    workflow: BpmnWorkflow\n+    start_time: datetime\n+    end_time: Optional[datetime] = None\n+    status: ProcessStatus = ProcessStatus.PENDING\n+    variables: Dict[str, Any] = field(default_factory=dict)\n+    execution_path: List[str] = field(default_factory=list)\n+    error_message: Optional[str] = None\n+    telemetry_data: Dict[str, Any] = field(default_factory=dict)\n+    \n+    def to_dict(self) -> Dict[str, Any]:\n+        \"\"\"Convert process instance to dictionary for serialization\"\"\"\n+        return {\n+            \"instance_id\": self.instance_id,\n+            \"process_definition_id\": self.process_definition_id,\n+            \"start_time\": self.start_time.isoformat(),\n+            \"end_time\": self.end_time.isoformat() if self.end_time else None,\n+            \"status\": self.status.value,\n+            \"variables\": self.variables,\n+            \"execution_path\": self.execution_path,\n+            \"error_message\": self.error_message,\n+            \"telemetry_data\": self.telemetry_data\n+        }\n+\n+@dataclass\n+class TaskExecution:\n+    \"\"\"BPMN Task execution context with telemetry\"\"\"\n+    task_id: str\n+    task_name: str\n+    task_type: str\n+    start_time: datetime\n+    end_time: Optional[datetime] = None\n+    status: TaskStatus = TaskStatus.PENDING\n+    duration: Optional[float] = None\n+    error_message: Optional[str] = None\n+    input_data: Dict[str, Any] = field(default_factory=dict)\n+    output_data: Dict[str, Any] = field(default_factory=dict)\n+    telemetry_data: Dict[str, Any] = field(default_factory=dict)\n+\n+class BPMNOrchestrator:\n+    \"\"\"\n+    Enterprise-grade BPMN 2.0 Orchestrator powered by SpiffWorkflow\n+    \n+    Features:\n+    - Full BPMN 2.0 specification compliance\n+    - Zero-touch telemetry integration\n+    - Process persistence and recovery\n+    - Advanced task execution monitoring\n+    - Multi-instance process support\n+    - Event-driven execution\n+    - Error handling and recovery\n+    \"\"\"\n+    \n+    def __init__(self, \n+                 bpmn_files_path: Optional[str] = None,\n+                 enable_telemetry: bool = True,\n+                 enable_persistence: bool = True):\n+        \"\"\"\n+        Initialize the BPMN Orchestrator\n+        \n+        Args:\n+            bpmn_files_path: Path to BPMN XML files\n+            enable_telemetry: Enable OpenTelemetry integration\n+            enable_persistence: Enable process state persistence\n+        \"\"\"\n+        self.tracer = trace.get_tracer(__name__)\n+        self.bpmn_files_path = Path(bpmn_files_path) if bpmn_files_path else Path(\"bpmn\")\n+        self.enable_telemetry = enable_telemetry\n+        self.enable_persistence = enable_persistence\n+        \n+        # Initialize SpiffWorkflow parser\n+        self.parser = BpmnParser()\n+        self.parser.add_bpmn_files_by_glob(str(self.bpmn_files_path / \"*.bpmn\"))\n+        \n+        # Process definitions cache\n+        self.process_definitions: Dict[str, BpmnProcessSpec] = {}\n+        self.active_instances: Dict[str, ProcessInstance] = {}\n+        \n+        # Load process definitions\n+        self._load_process_definitions()\n+        \n+        logger.info(f\"BPMN Orchestrator initialized with {len(self.process_definitions)} process definitions\")\n+    \n+    def _load_process_definitions(self) -> None:\n+        \"\"\"Load all BPMN process definitions from the configured path\"\"\"\n+        with self.tracer.start_as_current_span(\"load_process_definitions\") as span:\n+            try:\n+                # Get all process specs from the parser\n+                for process_id, process_spec in self.parser.find_all_specs().items():\n+                    self.process_definitions[process_id] = process_spec\n+                    span.add_event(f\"Loaded process definition: {process_id}\")\n+                \n+                logger.info(f\"Loaded {len(self.process_definitions)} process definitions\")\n+                \n+            except Exception as e:\n+                span.set_status(Status(StatusCode.ERROR, str(e)))\n+                logger.error(f\"Failed to load process definitions: {e}\")\n+                raise\n+    \n+    def start_process(self, \n+                     process_id: str, \n+                     variables: Optional[Dict[str, Any]] = None,\n+                     instance_id: Optional[str] = None) -> ProcessInstance:\n+        \"\"\"\n+        Start a new BPMN process instance\n+        \n+        Args:\n+            process_id: ID of the process to start\n+            variables: Initial process variables\n+            instance_id: Optional custom instance ID\n+            \n+        Returns:\n+            ProcessInstance: The started process instance\n+        \"\"\"\n+        with self.tracer.start_as_current_span(\"start_process\") as span:\n+            span.set_attributes({\n+                \"process.id\": process_id,\n+                \"process.variables\": json.dumps(variables or {})\n+            })\n+            \n+            try:\n+                # Validate process exists\n+                if process_id not in self.process_definitions:\n+                    raise ValueError(f\"Process definition '{process_id}' not found\")\n+                \n+                # Generate instance ID if not provided\n+                if not instance_id:\n+                    instance_id = f\"{process_id}_{uuid.uuid4().hex[:8]}\"\n+                \n+                # Create workflow instance\n+                workflow = BpmnWorkflow(self.process_definitions[process_id])\n+                \n+                # Set initial variables\n+                if variables:\n+                    workflow.data.update(variables)\n+                \n+                # Create process instance\n+                process_instance = ProcessInstance(\n+                    instance_id=instance_id,\n+                    process_definition_id=process_id,\n+                    workflow=workflow,\n+                    start_time=datetime.utcnow(),\n+                    variables=variables or {},\n+                    status=ProcessStatus.RUNNING\n+                )\n+                \n+                # Store instance\n+                self.active_instances[instance_id] = process_instance\n+                \n+                # Update telemetry\n+                process_counter.add(1, {\"process.id\": process_id, \"action\": \"start\"})\n+                \n+                span.set_attributes({\n+                    \"instance.id\": instance_id,\n+                    \"instance.status\": ProcessStatus.RUNNING.value\n+                })\n+                \n+                logger.info(f\"Started process instance {instance_id} for process {process_id}\")\n+                return process_instance\n+                \n+            except Exception as e:\n+                span.set_status(Status(StatusCode.ERROR, str(e)))\n+                error_counter.add(1, {\"process.id\": process_id, \"error.type\": \"start_failed\"})\n+                logger.error(f\"Failed to start process {process_id}: {e}\")\n+                raise\n+    \n+    def execute_process(self, instance_id: str, max_steps: int = 100) -> ProcessInstance:\n+        \"\"\"\n+        Execute a BPMN process instance\n+        \n+        Args:\n+            instance_id: ID of the process instance to execute\n+            max_steps: Maximum number of execution steps\n+            \n+        Returns:\n+            ProcessInstance: Updated process instance\n+        \"\"\"\n+        with self.tracer.start_as_current_span(\"execute_process\") as span:\n+            span.set_attributes({\n+                \"instance.id\": instance_id,\n+                \"execution.max_steps\": max_steps\n+            })\n+            \n+            try:\n+                # Get process instance\n+                if instance_id not in self.active_instances:\n+                    raise ValueError(f\"Process instance '{instance_id}' not found\")\n+                \n+                process_instance = self.active_instances[instance_id]\n+                workflow = process_instance.workflow\n+                \n+                # Execute workflow\n+                step_count = 0\n+                while not workflow.is_completed() and step_count < max_steps:\n+                    # Get ready tasks\n+                    ready_tasks = workflow.get_tasks(state=TaskState.READY)\n+                    \n+                    if not ready_tasks:\n+                        # No ready tasks, workflow might be waiting for external events\n+                        break\n+                    \n+                    # Execute each ready task\n+                    for task in ready_tasks:\n+                        self._execute_task(task, process_instance)\n+                        step_count += 1\n+                        \n+                        if workflow.is_completed():\n+                            break\n+                \n+                # Update process status\n+                if workflow.is_completed():\n+                    process_instance.status = ProcessStatus.COMPLETED\n+                    process_instance.end_time = datetime.utcnow()\n+                elif step_count >= max_steps:\n+                    process_instance.status = ProcessStatus.SUSPENDED\n+                \n+                # Update execution path\n+                process_instance.execution_path = self._get_execution_path(workflow)\n+                \n+                span.set_attributes({\n+                    \"execution.steps\": step_count,\n+                    \"instance.status\": process_instance.status.value\n+                })\n+                \n+                logger.info(f\"Executed process instance {instance_id} ({step_count} steps)\")\n+                return process_instance\n+                \n+            except Exception as e:\n+                span.set_status(Status(StatusCode.ERROR, str(e)))\n+                error_counter.add(1, {\"instance.id\": instance_id, \"error.type\": \"execution_failed\"})\n+                logger.error(f\"Failed to execute process instance {instance_id}: {e}\")\n+                raise\n+    \n+    def _execute_task(self, task: BpmnTaskSpec, process_instance: ProcessInstance) -> None:\n+        \"\"\"Execute a single BPMN task with telemetry\"\"\"\n+        with self.tracer.start_as_current_span(\"execute_task\") as span:\n+            task_start_time = datetime.utcnow()\n+            \n+            span.set_attributes({\n+                \"task.id\": task.id,\n+                \"task.name\": task.name,\n+                \"task.type\": task.__class__.__name__\n+            })\n+            \n+            try:\n+                # Create task execution context\n+                task_execution = TaskExecution(\n+                    task_id=task.id,\n+                    task_name=task.name,\n+                    task_type=task.__class__.__name__,\n+                    start_time=task_start_time,\n+                    status=TaskStatus.RUNNING,\n+                    input_data=dict(task.data) if hasattr(task, 'data') else {}\n+                )\n+                \n+                # Execute the task\n+                task.complete()\n+                \n+                # Update task execution\n+                task_execution.end_time = datetime.utcnow()\n+                task_execution.duration = (task_execution.end_time - task_execution.start_time).total_seconds()\n+                task_execution.status = TaskStatus.COMPLETED\n+                task_execution.output_data = dict(task.data) if hasattr(task, 'data') else {}\n+                \n+                # Record telemetry\n+                if self.enable_telemetry:\n+                    task_duration_histogram.record(\n+                        task_execution.duration,\n+                        {\n+                            \"task.id\": task.id,\n+                            \"task.type\": task_execution.task_type,\n+                            \"process.id\": process_instance.process_definition_id\n+                        }\n+                    )\n+                \n+                span.set_attributes({\n+                    \"task.duration\": task_execution.duration,\n+                    \"task.status\": TaskStatus.COMPLETED.value\n+                })\n+                \n+                logger.debug(f\"Executed task {task.id} ({task_execution.duration:.3f}s)\")\n+                \n+            except Exception as e:\n+                task_execution.end_time = datetime.utcnow()\n+                task_execution.duration = (task_execution.end_time - task_execution.start_time).total_seconds()\n+                task_execution.status = TaskStatus.FAILED\n+                task_execution.error_message = str(e)\n+                \n+                span.set_status(Status(StatusCode.ERROR, str(e)))\n+                error_counter.add(1, {\n+                    \"task.id\": task.id,\n+                    \"task.type\": task.__class__.__name__,\n+                    \"error.type\": \"task_execution_failed\"\n+                })\n+                \n+                logger.error(f\"Failed to execute task {task.id}: {e}\")\n+                raise\n+    \n+    def _get_execution_path(self, workflow: BpmnWorkflow) -> List[str]:\n+        \"\"\"Get the execution path of completed tasks\"\"\"\n+        try:\n+            completed_tasks = workflow.get_tasks(state=TaskState.COMPLETED)\n+            return [task.id for task in completed_tasks]\n+        except Exception as e:\n+            logger.warning(f\"Failed to get execution path: {e}\")\n+            return []\n+    \n+    def get_process_instance(self, instance_id: str) -> Optional[ProcessInstance]:\n+        \"\"\"Get a process instance by ID\"\"\"\n+        return self.active_instances.get(instance_id)\n+    \n+    def list_process_instances(self, \n+                             status: Optional[ProcessStatus] = None,\n+                             process_id: Optional[str] = None) -> List[ProcessInstance]:\n+        \"\"\"List process instances with optional filtering\"\"\"\n+        instances = list(self.active_instances.values())\n+        \n+        if status:\n+            instances = [i for i in instances if i.status == status]\n+        \n+        if process_id:\n+            instances = [i for i in instances if i.process_definition_id == process_id]\n+        \n+        return instances\n+    \n+    def cancel_process(self, instance_id: str) -> ProcessInstance:\n+        \"\"\"Cancel a running process instance\"\"\"\n+        with self.tracer.start_as_current_span(\"cancel_process\") as span:\n+            span.set_attributes({\"instance.id\": instance_id})\n+            \n+            try:\n+                if instance_id not in self.active_instances:\n+                    raise ValueError(f\"Process instance '{instance_id}' not found\")\n+                \n+                process_instance = self.active_instances[instance_id]\n+                \n+                if process_instance.status not in [ProcessStatus.RUNNING, ProcessStatus.SUSPENDED]:\n+                    raise ValueError(f\"Cannot cancel process in status: {process_instance.status}\")\n+                \n+                # Cancel the workflow\n+                process_instance.workflow.cancel()\n+                process_instance.status = ProcessStatus.CANCELLED\n+                process_instance.end_time = datetime.utcnow()\n+                \n+                span.set_attributes({\"instance.status\": ProcessStatus.CANCELLED.value})\n+                \n+                logger.info(f\"Cancelled process instance {instance_id}\")\n+                return process_instance\n+                \n+            except Exception as e:\n+                span.set_status(Status(StatusCode.ERROR, str(e)))\n+                error_counter.add(1, {\"instance.id\": instance_id, \"error.type\": \"cancel_failed\"})\n+                logger.error(f\"Failed to cancel process instance {instance_id}: {e}\")\n+                raise\n+    \n+    def get_process_variables(self, instance_id: str) -> Dict[str, Any]:\n+        \"\"\"Get process variables for an instance\"\"\"\n+        if instance_id not in self.active_instances:\n+            raise ValueError(f\"Process instance '{instance_id}' not found\")\n+        \n+        process_instance = self.active_instances[instance_id]\n+        return dict(process_instance.workflow.data)\n+    \n+    def set_process_variables(self, instance_id: str, variables: Dict[str, Any]) -> None:\n+        \"\"\"Set process variables for an instance\"\"\"\n+        if instance_id not in self.active_instances:\n+            raise ValueError(f\"Process instance '{instance_id}' not found\")\n+        \n+        process_instance = self.active_instances[instance_id]\n+        process_instance.workflow.data.update(variables)\n+        process_instance.variables.update(variables)\n+    \n+    def get_ready_tasks(self, instance_id: str) -> List[Dict[str, Any]]:\n+        \"\"\"Get ready tasks for a process instance\"\"\"\n+        if instance_id not in self.active_instances:\n+            raise ValueError(f\"Process instance '{instance_id}' not found\")\n+        \n+        process_instance = self.active_instances[instance_id]\n+        ready_tasks = process_instance.workflow.get_tasks(state=TaskState.READY)\n+        \n+        return [\n+            {\n+                \"task_id\": task.id,\n+                \"task_name\": task.name,\n+                \"task_type\": task.__class__.__name__,\n+                \"data\": dict(task.data) if hasattr(task, 'data') else {}\n+            }\n+            for task in ready_tasks\n+        ]\n+    \n+    def complete_task(self, instance_id: str, task_id: str, data: Optional[Dict[str, Any]] = None) -> None:\n+        \"\"\"Complete a specific task in a process instance\"\"\"\n+        with self.tracer.start_as_current_span(\"complete_task\") as span:\n+            span.set_attributes({\n+                \"instance.id\": instance_id,\n+                \"task.id\": task_id\n+            })\n+            \n+            try:\n+                if instance_id not in self.active_instances:\n+                    raise ValueError(f\"Process instance '{instance_id}' not found\")\n+                \n+                process_instance = self.active_instances[instance_id]\n+                workflow = process_instance.workflow\n+                \n+                # Find the task\n+                tasks = workflow.get_tasks()\n+                target_task = None\n+                \n+                for task in tasks:\n+                    if task.id == task_id:\n+                        target_task = task\n+                        break\n+                \n+                if not target_task:\n+                    raise ValueError(f\"Task '{task_id}' not found in process instance '{instance_id}'\")\n+                \n+                # Set task data if provided\n+                if data and hasattr(target_task, 'data'):\n+                    target_task.data.update(data)\n+                \n+                # Complete the task\n+                target_task.complete()\n+                \n+                span.set_attributes({\"task.status\": \"completed\"})\n+                \n+                logger.info(f\"Completed task {task_id} in process instance {instance_id}\")\n+                \n+            except Exception as e:\n+                span.set_status(Status(StatusCode.ERROR, str(e)))\n+                error_counter.add(1, {\n+                    \"instance.id\": instance_id,\n+                    \"task.id\": task_id,\n+                    \"error.type\": \"task_completion_failed\"\n+                })\n+                logger.error(f\"Failed to complete task {task_id} in process instance {instance_id}: {e}\")\n+                raise\n+    \n+    def serialize_workflow(self, instance_id: str) -> str:\n+        \"\"\"Serialize a workflow instance to JSON\"\"\"\n+        if instance_id not in self.active_instances:\n+            raise ValueError(f\"Process instance '{instance_id}' not found\")\n+        \n+        process_instance = self.active_instances[instance_id]\n+        serializer = BpmnWorkflowSerializer(DEFAULT_CONFIG)\n+        return serializer.serialize_workflow(process_instance.workflow)\n+    \n+    def deserialize_workflow(self, instance_id: str, serialized_data: str) -> ProcessInstance:\n+        \"\"\"Deserialize a workflow instance from JSON\"\"\"\n+        serializer = BpmnWorkflowSerializer(DEFAULT_CONFIG)\n+        workflow = serializer.deserialize_workflow(serialized_data)\n+        \n+        process_instance = ProcessInstance(\n+            instance_id=instance_id,\n+            process_definition_id=workflow.spec.name,\n+            workflow=workflow,\n+            start_time=datetime.utcnow(),\n+            status=ProcessStatus.RUNNING\n+        )\n+        \n+        self.active_instances[instance_id] = process_instance\n+        return process_instance\n+    \n+    def get_process_statistics(self) -> Dict[str, Any]:\n+        \"\"\"Get orchestrator statistics\"\"\"\n+        total_instances = len(self.active_instances)\n+        status_counts = {}\n+        \n+        for status in ProcessStatus:\n+            status_counts[status.value] = len([\n+                i for i in self.active_instances.values() \n+                if i.status == status\n+            ])\n+        \n+        return {\n+            \"total_process_definitions\": len(self.process_definitions),\n+            \"total_instances\": total_instances,\n+            \"status_distribution\": status_counts,\n+            \"process_definitions\": list(self.process_definitions.keys())\n+        }\n+    \n+    def cleanup_completed_processes(self, max_age_hours: int = 24) -> int:\n+        \"\"\"Clean up completed process instances older than specified age\"\"\"\n+        cutoff_time = datetime.utcnow() - timedelta(hours=max_age_hours)\n+        removed_count = 0\n+        \n+        instances_to_remove = []\n+        \n+        for instance_id, process_instance in self.active_instances.items():\n+            if (process_instance.status in [ProcessStatus.COMPLETED, ProcessStatus.FAILED, ProcessStatus.CANCELLED] and\n+                process_instance.end_time and process_instance.end_time < cutoff_time):\n+                instances_to_remove.append(instance_id)\n+        \n+        for instance_id in instances_to_remove:\n+            del self.active_instances[instance_id]\n+            removed_count += 1\n+        \n+        logger.info(f\"Cleaned up {removed_count} completed process instances\")\n+        return removed_count\n+\n+# Example usage and utility functions\n+def create_sample_bpmn_file() -> str:\n+    \"\"\"Create a sample BPMN file for testing\"\"\"\n+    sample_bpmn = '''<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n+<bpmn:definitions xmlns:bpmn=\"http://www.omg.org/spec/BPMN/20100524/MODEL\"\n+                  xmlns:bpmndi=\"http://www.omg.org/spec/BPMN/20100524/DI\"\n+                  xmlns:dc=\"http://www.omg.org/spec/DD/20100524/DC\"\n+                  xmlns:di=\"http://www.omg.org/spec/DD/20100524/DI\"\n+                  id=\"Definitions_1\"\n+                  targetNamespace=\"http://bpmn.io/schema/bpmn\">\n+  <bpmn:process id=\"Process_1\" isExecutable=\"true\">\n+    <bpmn:startEvent id=\"StartEvent_1\" name=\"Start\">\n+      <bpmn:outgoing>Flow_1</bpmn:outgoing>\n+    </bpmn:startEvent>\n+    <bpmn:task id=\"Task_1\" name=\"Sample Task\">\n+      <bpmn:incoming>Flow_1</bpmn:incoming>\n+      <bpmn:outgoing>Flow_2</bpmn:outgoing>\n+    </bpmn:task>\n+    <bpmn:endEvent id=\"EndEvent_1\" name=\"End\">\n+      <bpmn:incoming>Flow_2</bpmn:incoming>\n+    </bpmn:endEvent>\n+    <bpmn:sequenceFlow id=\"Flow_1\" sourceRef=\"StartEvent_1\" targetRef=\"Task_1\" />\n+    <bpmn:sequenceFlow id=\"Flow_2\" sourceRef=\"Task_1\" targetRef=\"EndEvent_1\" />\n+  </bpmn:process>\n+  <bpmndi:BPMNDiagram id=\"BPMNDiagram_1\">\n+    <bpmndi:BPMNPlane id=\"BPMNPlane_1\" bpmnElement=\"Process_1\">\n+      <bpmndi:BPMNShape id=\"StartEvent_1_di\" bpmnElement=\"StartEvent_1\">\n+        <dc:Bounds x=\"152\" y=\"102\" width=\"36\" height=\"36\" />\n+      </bpmndi:BPMNShape>\n+      <bpmndi:BPMNShape id=\"Task_1_di\" bpmnElement=\"Task_1\">\n+        <dc:Bounds x=\"240\" y=\"80\" width=\"100\" height=\"80\" />\n+      </bpmndi:BPMNShape>\n+      <bpmndi:BPMNShape id=\"EndEvent_1_di\" bpmnElement=\"EndEvent_1\">\n+        <dc:Bounds x=\"392\" y=\"102\" width=\"36\" height=\"36\" />\n+      </bpmndi:BPMNShape>\n+      <bpmndi:BPMNEdge id=\"Flow_1_di\" bpmnElement=\"Flow_1\">\n+        <di:waypoint x=\"188\" y=\"120\" />\n+        <di:waypoint x=\"240\" y=\"120\" />\n+      </bpmndi:BPMNEdge>\n+      <bpmndi:BPMNEdge id=\"Flow_2_di\" bpmnElement=\"Flow_2\">\n+        <di:waypoint x=\"340\" y=\"120\" />\n+        <di:waypoint x=\"392\" y=\"120\" />\n+      </bpmndi:BPMNEdge>\n+    </bpmndi:BPMNPlane>\n+  </bpmndi:BPMNDiagram>\n+</bpmn:definitions>'''\n+    \n+    return sample_bpmn\n+\n+if __name__ == \"__main__\":\n+    # Example usage\n+    print(\"AutoTel BPMN Orchestrator - SpiffWorkflow Integration\")\n+    print(\"=\" * 60)\n+    \n+    # Create sample BPMN file\n+    sample_bpmn = create_sample_bpmn_file()\n+    bpmn_path = Path(\"bpmn\")\n+    bpmn_path.mkdir(exist_ok=True)\n+    \n+    with open(bpmn_path / \"sample_process.bpmn\", \"w\") as f:\n+        f.write(sample_bpmn)\n+    \n+    # Initialize orchestrator\n+    orchestrator = BPMNOrchestrator(bpmn_files_path=\"bpmn\")\n+    \n+    # Start a process\n+    instance = orchestrator.start_process(\"Process_1\", {\"input\": \"test\"})\n+    print(f\"Started process instance: {instance.instance_id}\")\n+    \n+    # Execute the process\n+    result = orchestrator.execute_process(instance.instance_id)\n+    print(f\"Process execution completed. Status: {result.status.value}\")\n+    \n+    # Get statistics\n+    stats = orchestrator.get_process_statistics()\n+    print(f\"Orchestrator statistics: {stats}\")\n\\ No newline at end of file\n"
                },
                {
                    "date": 1752191847786,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -325,18 +325,18 @@\n             task_start_time = datetime.utcnow()\n             \n             span.set_attributes({\n                 \"task.id\": task.id,\n-                \"task.name\": task.name,\n-                \"task.type\": task.__class__.__name__\n+                \"task.name\": task.task_spec.name,\n+                \"task.type\": task.task_spec.__class__.__name__\n             })\n             \n             try:\n                 # Create task execution context\n                 task_execution = TaskExecution(\n                     task_id=task.id,\n-                    task_name=task.name,\n-                    task_type=task.__class__.__name__,\n+                    task_name=task.task_spec.name,\n+                    task_type=task.task_spec.__class__.__name__,\n                     start_time=task_start_time,\n                     status=TaskStatus.RUNNING,\n                     input_data=dict(task.data) if hasattr(task, 'data') else {}\n                 )\n@@ -376,9 +376,9 @@\n                 \n                 span.set_status(Status(StatusCode.ERROR, str(e)))\n                 error_counter.add(1, {\n                     \"task.id\": task.id,\n-                    \"task.type\": task.__class__.__name__,\n+                    \"task.type\": task.task_spec.__class__.__name__,\n                     \"error.type\": \"task_execution_failed\"\n                 })\n                 \n                 logger.error(f\"Failed to execute task {task.id}: {e}\")\n@@ -468,10 +468,10 @@\n         \n         return [\n             {\n                 \"task_id\": task.id,\n-                \"task_name\": task.name,\n-                \"task_type\": task.__class__.__name__,\n+                \"task_name\": task.task_spec.name,\n+                \"task_type\": task.task_spec.__class__.__name__,\n                 \"data\": dict(task.data) if hasattr(task, 'data') else {}\n             }\n             for task in ready_tasks\n         ]\n"
                },
                {
                    "date": 1752192667267,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -75,9 +75,9 @@\n class TaskStatus(Enum):\n     \"\"\"BPMN Task execution status\"\"\"\n     PENDING = \"pending\"\n     READY = \"ready\"\n-    RUNNING = \"running\"\n+    RUNNING = \"running\" \n     COMPLETED = \"completed\"\n     FAILED = \"failed\"\n     SKIPPED = \"skipped\"\n     CANCELLED = \"cancelled\"\n@@ -660,1331 +660,5 @@\n     print(f\"Process execution completed. Status: {result.status.value}\")\n     \n     # Get statistics\n     stats = orchestrator.get_process_statistics()\n-    print(f\"Orchestrator statistics: {stats}\")\n-#!/usr/bin/env python3\n-\"\"\"\n-AutoTel BPMN Orchestrator - Enterprise BPMN 2.0 Execution Engine\n-Powered by SpiffWorkflow with zero-touch telemetry integration\n-\"\"\"\n-\n-import json\n-import uuid\n-import logging\n-from datetime import datetime, timedelta\n-from typing import Dict, List, Any, Optional, Callable, Union\n-from dataclasses import dataclass, field\n-from enum import Enum\n-from pathlib import Path\n-import xml.etree.ElementTree as ET\n-\n-# SpiffWorkflow imports\n-from SpiffWorkflow.bpmn import BpmnWorkflow\n-from SpiffWorkflow.bpmn.parser.BpmnParser import BpmnParser\n-from SpiffWorkflow.bpmn.specs.bpmn_task_spec import BpmnTaskSpec\n-from SpiffWorkflow.bpmn.specs.bpmn_process_spec import BpmnProcessSpec\n-from SpiffWorkflow.bpmn.util.task import BpmnTaskFilter\n-from SpiffWorkflow.bpmn.util.subworkflow import BpmnSubWorkflow\n-from SpiffWorkflow.bpmn.serializer import BpmnWorkflowSerializer\n-from SpiffWorkflow.bpmn.serializer.config import DEFAULT_CONFIG\n-\n-# OpenTelemetry for observability\n-from opentelemetry import trace, metrics\n-from opentelemetry.trace import Status, StatusCode\n-from opentelemetry.metrics import Counter, Histogram\n-from opentelemetry.sdk.trace import TracerProvider\n-from opentelemetry.sdk.trace.export import ConsoleSpanExporter, BatchSpanProcessor\n-from opentelemetry.sdk.metrics import MeterProvider\n-from opentelemetry.sdk.metrics.export import ConsoleMetricExporter, PeriodicExportingMetricReader\n-\n-# Configure logging\n-logging.basicConfig(level=logging.INFO)\n-logger = logging.getLogger(__name__)\n-\n-# Configure OpenTelemetry\n-trace.set_tracer_provider(TracerProvider())\n-trace.get_tracer_provider().add_span_processor(\n-    BatchSpanProcessor(ConsoleSpanExporter())\n-)\n-\n-metrics.set_meter_provider(MeterProvider())\n-meter = metrics.get_meter(__name__)\n-\n-# Telemetry instruments\n-process_counter = meter.create_counter(\n-    name=\"bpmn_processes_total\",\n-    description=\"Total number of BPMN processes executed\"\n-)\n-\n-task_duration_histogram = meter.create_histogram(\n-    name=\"bpmn_task_duration_seconds\",\n-    description=\"Duration of BPMN tasks in seconds\"\n-)\n-\n-error_counter = meter.create_counter(\n-    name=\"bpmn_errors_total\",\n-    description=\"Total number of BPMN execution errors\"\n-)\n-\n-class ProcessStatus(Enum):\n-    \"\"\"BPMN Process execution status\"\"\"\n-    PENDING = \"pending\"\n-    RUNNING = \"running\"\n-    COMPLETED = \"completed\"\n-    FAILED = \"failed\"\n-    SUSPENDED = \"suspended\"\n-    CANCELLED = \"cancelled\"\n-\n-class TaskStatus(Enum):\n-    \"\"\"BPMN Task execution status\"\"\"\n-    PENDING = \"pending\"\n-    READY = \"ready\"\n-    RUNNING = \"running\"\n-    COMPLETED = \"completed\"\n-    FAILED = \"failed\"\n-    SKIPPED = \"skipped\"\n-    CANCELLED = \"cancelled\"\n-\n-@dataclass\n-class ProcessInstance:\n-    \"\"\"BPMN Process instance with execution state and telemetry\"\"\"\n-    instance_id: str\n-    process_definition_id: str\n-    workflow: BpmnWorkflow\n-    start_time: datetime\n-    end_time: Optional[datetime] = None\n-    status: ProcessStatus = ProcessStatus.PENDING\n-    variables: Dict[str, Any] = field(default_factory=dict)\n-    execution_path: List[str] = field(default_factory=list)\n-    error_message: Optional[str] = None\n-    telemetry_data: Dict[str, Any] = field(default_factory=dict)\n-    \n-    def to_dict(self) -> Dict[str, Any]:\n-        \"\"\"Convert process instance to dictionary for serialization\"\"\"\n-        return {\n-            \"instance_id\": self.instance_id,\n-            \"process_definition_id\": self.process_definition_id,\n-            \"start_time\": self.start_time.isoformat(),\n-            \"end_time\": self.end_time.isoformat() if self.end_time else None,\n-            \"status\": self.status.value,\n-            \"variables\": self.variables,\n-            \"execution_path\": self.execution_path,\n-            \"error_message\": self.error_message,\n-            \"telemetry_data\": self.telemetry_data\n-        }\n-\n-@dataclass\n-class TaskExecution:\n-    \"\"\"BPMN Task execution context with telemetry\"\"\"\n-    task_id: str\n-    task_name: str\n-    task_type: str\n-    start_time: datetime\n-    end_time: Optional[datetime] = None\n-    status: TaskStatus = TaskStatus.PENDING\n-    duration: Optional[float] = None\n-    error_message: Optional[str] = None\n-    input_data: Dict[str, Any] = field(default_factory=dict)\n-    output_data: Dict[str, Any] = field(default_factory=dict)\n-    telemetry_data: Dict[str, Any] = field(default_factory=dict)\n-\n-class BPMNOrchestrator:\n-    \"\"\"\n-    Enterprise-grade BPMN 2.0 Orchestrator powered by SpiffWorkflow\n-    \n-    Features:\n-    - Full BPMN 2.0 specification compliance\n-    - Zero-touch telemetry integration\n-    - Process persistence and recovery\n-    - Advanced task execution monitoring\n-    - Multi-instance process support\n-    - Event-driven execution\n-    - Error handling and recovery\n-    \"\"\"\n-    \n-    def __init__(self, \n-                 bpmn_files_path: Optional[str] = None,\n-                 enable_telemetry: bool = True,\n-                 enable_persistence: bool = True):\n-        \"\"\"\n-        Initialize the BPMN Orchestrator\n-        \n-        Args:\n-            bpmn_files_path: Path to BPMN XML files\n-            enable_telemetry: Enable OpenTelemetry integration\n-            enable_persistence: Enable process state persistence\n-        \"\"\"\n-        self.tracer = trace.get_tracer(__name__)\n-        self.bpmn_files_path = Path(bpmn_files_path) if bpmn_files_path else Path(\"bpmn\")\n-        self.enable_telemetry = enable_telemetry\n-        self.enable_persistence = enable_persistence\n-        \n-        # Initialize SpiffWorkflow parser\n-        self.parser = BpmnParser()\n-        self.parser.add_bpmn_files_by_glob(str(self.bpmn_files_path / \"*.bpmn\"))\n-        \n-        # Process definitions cache\n-        self.process_definitions: Dict[str, BpmnProcessSpec] = {}\n-        self.active_instances: Dict[str, ProcessInstance] = {}\n-        \n-        # Load process definitions\n-        self._load_process_definitions()\n-        \n-        logger.info(f\"BPMN Orchestrator initialized with {len(self.process_definitions)} process definitions\")\n-    \n-    def _load_process_definitions(self) -> None:\n-        \"\"\"Load all BPMN process definitions from the configured path\"\"\"\n-        with self.tracer.start_as_current_span(\"load_process_definitions\") as span:\n-            try:\n-                # Get all process specs from the parser\n-                for process_id, process_spec in self.parser.find_all_specs().items():\n-                    self.process_definitions[process_id] = process_spec\n-                    span.add_event(f\"Loaded process definition: {process_id}\")\n-                \n-                logger.info(f\"Loaded {len(self.process_definitions)} process definitions\")\n-                \n-            except Exception as e:\n-                span.set_status(Status(StatusCode.ERROR, str(e)))\n-                logger.error(f\"Failed to load process definitions: {e}\")\n-                raise\n-    \n-    def start_process(self, \n-                     process_id: str, \n-                     variables: Optional[Dict[str, Any]] = None,\n-                     instance_id: Optional[str] = None) -> ProcessInstance:\n-        \"\"\"\n-        Start a new BPMN process instance\n-        \n-        Args:\n-            process_id: ID of the process to start\n-            variables: Initial process variables\n-            instance_id: Optional custom instance ID\n-            \n-        Returns:\n-            ProcessInstance: The started process instance\n-        \"\"\"\n-        with self.tracer.start_as_current_span(\"start_process\") as span:\n-            span.set_attributes({\n-                \"process.id\": process_id,\n-                \"process.variables\": json.dumps(variables or {})\n-            })\n-            \n-            try:\n-                # Validate process exists\n-                if process_id not in self.process_definitions:\n-                    raise ValueError(f\"Process definition '{process_id}' not found\")\n-                \n-                # Generate instance ID if not provided\n-                if not instance_id:\n-                    instance_id = f\"{process_id}_{uuid.uuid4().hex[:8]}\"\n-                \n-                # Create workflow instance\n-                workflow = BpmnWorkflow(self.process_definitions[process_id])\n-                \n-                # Set initial variables\n-                if variables:\n-                    workflow.data.update(variables)\n-                \n-                # Create process instance\n-                process_instance = ProcessInstance(\n-                    instance_id=instance_id,\n-                    process_definition_id=process_id,\n-                    workflow=workflow,\n-                    start_time=datetime.utcnow(),\n-                    variables=variables or {},\n-                    status=ProcessStatus.RUNNING\n-                )\n-                \n-                # Store instance\n-                self.active_instances[instance_id] = process_instance\n-                \n-                # Update telemetry\n-                process_counter.add(1, {\"process.id\": process_id, \"action\": \"start\"})\n-                \n-                span.set_attributes({\n-                    \"instance.id\": instance_id,\n-                    \"instance.status\": ProcessStatus.RUNNING.value\n-                })\n-                \n-                logger.info(f\"Started process instance {instance_id} for process {process_id}\")\n-                return process_instance\n-                \n-            except Exception as e:\n-                span.set_status(Status(StatusCode.ERROR, str(e)))\n-                error_counter.add(1, {\"process.id\": process_id, \"error.type\": \"start_failed\"})\n-                logger.error(f\"Failed to start process {process_id}: {e}\")\n-                raise\n-    \n-    def execute_process(self, instance_id: str, max_steps: int = 100) -> ProcessInstance:\n-        \"\"\"\n-        Execute a BPMN process instance\n-        \n-        Args:\n-            instance_id: ID of the process instance to execute\n-            max_steps: Maximum number of execution steps\n-            \n-        Returns:\n-            ProcessInstance: Updated process instance\n-        \"\"\"\n-        with self.tracer.start_as_current_span(\"execute_process\") as span:\n-            span.set_attributes({\n-                \"instance.id\": instance_id,\n-                \"execution.max_steps\": max_steps\n-            })\n-            \n-            try:\n-                # Get process instance\n-                if instance_id not in self.active_instances:\n-                    raise ValueError(f\"Process instance '{instance_id}' not found\")\n-                \n-                process_instance = self.active_instances[instance_id]\n-                workflow = process_instance.workflow\n-                \n-                # Execute workflow\n-                step_count = 0\n-                while not workflow.is_completed() and step_count < max_steps:\n-                    # Get ready tasks\n-                    ready_tasks = workflow.get_tasks(state=TaskStatus.READY.value)\n-                    \n-                    if not ready_tasks:\n-                        # No ready tasks, workflow might be waiting for external events\n-                        break\n-                    \n-                    # Execute each ready task\n-                    for task in ready_tasks:\n-                        self._execute_task(task, process_instance)\n-                        step_count += 1\n-                        \n-                        if workflow.is_completed():\n-                            break\n-                \n-                # Update process status\n-                if workflow.is_completed():\n-                    process_instance.status = ProcessStatus.COMPLETED\n-                    process_instance.end_time = datetime.utcnow()\n-                elif step_count >= max_steps:\n-                    process_instance.status = ProcessStatus.SUSPENDED\n-                \n-                # Update execution path\n-                process_instance.execution_path = self._get_execution_path(workflow)\n-                \n-                span.set_attributes({\n-                    \"execution.steps\": step_count,\n-                    \"instance.status\": process_instance.status.value\n-                })\n-                \n-                logger.info(f\"Executed process instance {instance_id} ({step_count} steps)\")\n-                return process_instance\n-                \n-            except Exception as e:\n-                span.set_status(Status(StatusCode.ERROR, str(e)))\n-                error_counter.add(1, {\"instance.id\": instance_id, \"error.type\": \"execution_failed\"})\n-                logger.error(f\"Failed to execute process instance {instance_id}: {e}\")\n-                raise\n-    \n-    def _execute_task(self, task: BpmnTaskSpec, process_instance: ProcessInstance) -> None:\n-        \"\"\"Execute a single BPMN task with telemetry\"\"\"\n-        with self.tracer.start_as_current_span(\"execute_task\") as span:\n-            task_start_time = datetime.utcnow()\n-            \n-            span.set_attributes({\n-                \"task.id\": task.id,\n-                \"task.name\": task.name,\n-                \"task.type\": task.__class__.__name__\n-            })\n-            \n-            try:\n-                # Create task execution context\n-                task_execution = TaskExecution(\n-                    task_id=task.id,\n-                    task_name=task.name,\n-                    task_type=task.__class__.__name__,\n-                    start_time=task_start_time,\n-                    status=TaskStatus.RUNNING,\n-                    input_data=dict(task.data) if hasattr(task, 'data') else {}\n-                )\n-                \n-                # Execute the task\n-                task.complete()\n-                \n-                # Update task execution\n-                task_execution.end_time = datetime.utcnow()\n-                task_execution.duration = (task_execution.end_time - task_execution.start_time).total_seconds()\n-                task_execution.status = TaskStatus.COMPLETED\n-                task_execution.output_data = dict(task.data) if hasattr(task, 'data') else {}\n-                \n-                # Record telemetry\n-                if self.enable_telemetry:\n-                    task_duration_histogram.record(\n-                        task_execution.duration,\n-                        {\n-                            \"task.id\": task.id,\n-                            \"task.type\": task_execution.task_type,\n-                            \"process.id\": process_instance.process_definition_id\n-                        }\n-                    )\n-                \n-                span.set_attributes({\n-                    \"task.duration\": task_execution.duration,\n-                    \"task.status\": TaskStatus.COMPLETED.value\n-                })\n-                \n-                logger.debug(f\"Executed task {task.id} ({task_execution.duration:.3f}s)\")\n-                \n-            except Exception as e:\n-                task_execution.end_time = datetime.utcnow()\n-                task_execution.duration = (task_execution.end_time - task_execution.start_time).total_seconds()\n-                task_execution.status = TaskStatus.FAILED\n-                task_execution.error_message = str(e)\n-                \n-                span.set_status(Status(StatusCode.ERROR, str(e)))\n-                error_counter.add(1, {\n-                    \"task.id\": task.id,\n-                    \"task.type\": task.__class__.__name__,\n-                    \"error.type\": \"task_execution_failed\"\n-                })\n-                \n-                logger.error(f\"Failed to execute task {task.id}: {e}\")\n-                raise\n-    \n-    def _get_execution_path(self, workflow: BpmnWorkflow) -> List[str]:\n-        \"\"\"Get the execution path of completed tasks\"\"\"\n-        try:\n-            completed_tasks = workflow.get_tasks(state=TaskStatus.COMPLETED.value)\n-            return [task.id for task in completed_tasks]\n-        except Exception as e:\n-            logger.warning(f\"Failed to get execution path: {e}\")\n-            return []\n-    \n-    def get_process_instance(self, instance_id: str) -> Optional[ProcessInstance]:\n-        \"\"\"Get a process instance by ID\"\"\"\n-        return self.active_instances.get(instance_id)\n-    \n-    def list_process_instances(self, \n-                             status: Optional[ProcessStatus] = None,\n-                             process_id: Optional[str] = None) -> List[ProcessInstance]:\n-        \"\"\"List process instances with optional filtering\"\"\"\n-        instances = list(self.active_instances.values())\n-        \n-        if status:\n-            instances = [i for i in instances if i.status == status]\n-        \n-        if process_id:\n-            instances = [i for i in instances if i.process_definition_id == process_id]\n-        \n-        return instances\n-    \n-    def cancel_process(self, instance_id: str) -> ProcessInstance:\n-        \"\"\"Cancel a running process instance\"\"\"\n-        with self.tracer.start_as_current_span(\"cancel_process\") as span:\n-            span.set_attributes({\"instance.id\": instance_id})\n-            \n-            try:\n-                if instance_id not in self.active_instances:\n-                    raise ValueError(f\"Process instance '{instance_id}' not found\")\n-                \n-                process_instance = self.active_instances[instance_id]\n-                \n-                if process_instance.status not in [ProcessStatus.RUNNING, ProcessStatus.SUSPENDED]:\n-                    raise ValueError(f\"Cannot cancel process in status: {process_instance.status}\")\n-                \n-                # Cancel the workflow\n-                process_instance.workflow.cancel()\n-                process_instance.status = ProcessStatus.CANCELLED\n-                process_instance.end_time = datetime.utcnow()\n-                \n-                span.set_attributes({\"instance.status\": ProcessStatus.CANCELLED.value})\n-                \n-                logger.info(f\"Cancelled process instance {instance_id}\")\n-                return process_instance\n-                \n-            except Exception as e:\n-                span.set_status(Status(StatusCode.ERROR, str(e)))\n-                error_counter.add(1, {\"instance.id\": instance_id, \"error.type\": \"cancel_failed\"})\n-                logger.error(f\"Failed to cancel process instance {instance_id}: {e}\")\n-                raise\n-    \n-    def get_process_variables(self, instance_id: str) -> Dict[str, Any]:\n-        \"\"\"Get process variables for an instance\"\"\"\n-        if instance_id not in self.active_instances:\n-            raise ValueError(f\"Process instance '{instance_id}' not found\")\n-        \n-        process_instance = self.active_instances[instance_id]\n-        return dict(process_instance.workflow.data)\n-    \n-    def set_process_variables(self, instance_id: str, variables: Dict[str, Any]) -> None:\n-        \"\"\"Set process variables for an instance\"\"\"\n-        if instance_id not in self.active_instances:\n-            raise ValueError(f\"Process instance '{instance_id}' not found\")\n-        \n-        process_instance = self.active_instances[instance_id]\n-        process_instance.workflow.data.update(variables)\n-        process_instance.variables.update(variables)\n-    \n-    def get_ready_tasks(self, instance_id: str) -> List[Dict[str, Any]]:\n-        \"\"\"Get ready tasks for a process instance\"\"\"\n-        if instance_id not in self.active_instances:\n-            raise ValueError(f\"Process instance '{instance_id}' not found\")\n-        \n-        process_instance = self.active_instances[instance_id]\n-        ready_tasks = process_instance.workflow.get_tasks(state=TaskStatus.READY.value)\n-        \n-        return [\n-            {\n-                \"task_id\": task.id,\n-                \"task_name\": task.name,\n-                \"task_type\": task.__class__.__name__,\n-                \"data\": dict(task.data) if hasattr(task, 'data') else {}\n-            }\n-            for task in ready_tasks\n-        ]\n-    \n-    def complete_task(self, instance_id: str, task_id: str, data: Optional[Dict[str, Any]] = None) -> None:\n-        \"\"\"Complete a specific task in a process instance\"\"\"\n-        with self.tracer.start_as_current_span(\"complete_task\") as span:\n-            span.set_attributes({\n-                \"instance.id\": instance_id,\n-                \"task.id\": task_id\n-            })\n-            \n-            try:\n-                if instance_id not in self.active_instances:\n-                    raise ValueError(f\"Process instance '{instance_id}' not found\")\n-                \n-                process_instance = self.active_instances[instance_id]\n-                workflow = process_instance.workflow\n-                \n-                # Find the task\n-                tasks = workflow.get_tasks()\n-                target_task = None\n-                \n-                for task in tasks:\n-                    if task.id == task_id:\n-                        target_task = task\n-                        break\n-                \n-                if not target_task:\n-                    raise ValueError(f\"Task '{task_id}' not found in process instance '{instance_id}'\")\n-                \n-                # Set task data if provided\n-                if data and hasattr(target_task, 'data'):\n-                    target_task.data.update(data)\n-                \n-                # Complete the task\n-                target_task.complete()\n-                \n-                span.set_attributes({\"task.status\": \"completed\"})\n-                \n-                logger.info(f\"Completed task {task_id} in process instance {instance_id}\")\n-                \n-            except Exception as e:\n-                span.set_status(Status(StatusCode.ERROR, str(e)))\n-                error_counter.add(1, {\n-                    \"instance.id\": instance_id,\n-                    \"task.id\": task_id,\n-                    \"error.type\": \"task_completion_failed\"\n-                })\n-                logger.error(f\"Failed to complete task {task_id} in process instance {instance_id}: {e}\")\n-                raise\n-    \n-    def serialize_workflow(self, instance_id: str) -> str:\n-        \"\"\"Serialize a workflow instance to JSON\"\"\"\n-        if instance_id not in self.active_instances:\n-            raise ValueError(f\"Process instance '{instance_id}' not found\")\n-        \n-        process_instance = self.active_instances[instance_id]\n-        serializer = BpmnWorkflowSerializer(DEFAULT_CONFIG)\n-        return serializer.serialize_workflow(process_instance.workflow)\n-    \n-    def deserialize_workflow(self, instance_id: str, serialized_data: str) -> ProcessInstance:\n-        \"\"\"Deserialize a workflow instance from JSON\"\"\"\n-        serializer = BpmnWorkflowSerializer(DEFAULT_CONFIG)\n-        workflow = serializer.deserialize_workflow(serialized_data)\n-        \n-        process_instance = ProcessInstance(\n-            instance_id=instance_id,\n-            process_definition_id=workflow.spec.name,\n-            workflow=workflow,\n-            start_time=datetime.utcnow(),\n-            status=ProcessStatus.RUNNING\n-        )\n-        \n-        self.active_instances[instance_id] = process_instance\n-        return process_instance\n-    \n-    def get_process_statistics(self) -> Dict[str, Any]:\n-        \"\"\"Get orchestrator statistics\"\"\"\n-        total_instances = len(self.active_instances)\n-        status_counts = {}\n-        \n-        for status in ProcessStatus:\n-            status_counts[status.value] = len([\n-                i for i in self.active_instances.values() \n-                if i.status == status\n-            ])\n-        \n-        return {\n-            \"total_process_definitions\": len(self.process_definitions),\n-            \"total_instances\": total_instances,\n-            \"status_distribution\": status_counts,\n-            \"process_definitions\": list(self.process_definitions.keys())\n-        }\n-    \n-    def cleanup_completed_processes(self, max_age_hours: int = 24) -> int:\n-        \"\"\"Clean up completed process instances older than specified age\"\"\"\n-        cutoff_time = datetime.utcnow() - timedelta(hours=max_age_hours)\n-        removed_count = 0\n-        \n-        instances_to_remove = []\n-        \n-        for instance_id, process_instance in self.active_instances.items():\n-            if (process_instance.status in [ProcessStatus.COMPLETED, ProcessStatus.FAILED, ProcessStatus.CANCELLED] and\n-                process_instance.end_time and process_instance.end_time < cutoff_time):\n-                instances_to_remove.append(instance_id)\n-        \n-        for instance_id in instances_to_remove:\n-            del self.active_instances[instance_id]\n-            removed_count += 1\n-        \n-        logger.info(f\"Cleaned up {removed_count} completed process instances\")\n-        return removed_count\n-\n-# Example usage and utility functions\n-def create_sample_bpmn_file() -> str:\n-    \"\"\"Create a sample BPMN file for testing\"\"\"\n-    sample_bpmn = '''<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n-<bpmn:definitions xmlns:bpmn=\"http://www.omg.org/spec/BPMN/20100524/MODEL\"\n-                  xmlns:bpmndi=\"http://www.omg.org/spec/BPMN/20100524/DI\"\n-                  xmlns:dc=\"http://www.omg.org/spec/DD/20100524/DC\"\n-                  xmlns:di=\"http://www.omg.org/spec/DD/20100524/DI\"\n-                  id=\"Definitions_1\"\n-                  targetNamespace=\"http://bpmn.io/schema/bpmn\">\n-  <bpmn:process id=\"Process_1\" isExecutable=\"true\">\n-    <bpmn:startEvent id=\"StartEvent_1\" name=\"Start\">\n-      <bpmn:outgoing>Flow_1</bpmn:outgoing>\n-    </bpmn:startEvent>\n-    <bpmn:task id=\"Task_1\" name=\"Sample Task\">\n-      <bpmn:incoming>Flow_1</bpmn:incoming>\n-      <bpmn:outgoing>Flow_2</bpmn:outgoing>\n-    </bpmn:task>\n-    <bpmn:endEvent id=\"EndEvent_1\" name=\"End\">\n-      <bpmn:incoming>Flow_2</bpmn:incoming>\n-    </bpmn:endEvent>\n-    <bpmn:sequenceFlow id=\"Flow_1\" sourceRef=\"StartEvent_1\" targetRef=\"Task_1\" />\n-    <bpmn:sequenceFlow id=\"Flow_2\" sourceRef=\"Task_1\" targetRef=\"EndEvent_1\" />\n-  </bpmn:process>\n-  <bpmndi:BPMNDiagram id=\"BPMNDiagram_1\">\n-    <bpmndi:BPMNPlane id=\"BPMNPlane_1\" bpmnElement=\"Process_1\">\n-      <bpmndi:BPMNShape id=\"StartEvent_1_di\" bpmnElement=\"StartEvent_1\">\n-        <dc:Bounds x=\"152\" y=\"102\" width=\"36\" height=\"36\" />\n-      </bpmndi:BPMNShape>\n-      <bpmndi:BPMNShape id=\"Task_1_di\" bpmnElement=\"Task_1\">\n-        <dc:Bounds x=\"240\" y=\"80\" width=\"100\" height=\"80\" />\n-      </bpmndi:BPMNShape>\n-      <bpmndi:BPMNShape id=\"EndEvent_1_di\" bpmnElement=\"EndEvent_1\">\n-        <dc:Bounds x=\"392\" y=\"102\" width=\"36\" height=\"36\" />\n-      </bpmndi:BPMNShape>\n-      <bpmndi:BPMNEdge id=\"Flow_1_di\" bpmnElement=\"Flow_1\">\n-        <di:waypoint x=\"188\" y=\"120\" />\n-        <di:waypoint x=\"240\" y=\"120\" />\n-      </bpmndi:BPMNEdge>\n-      <bpmndi:BPMNEdge id=\"Flow_2_di\" bpmnElement=\"Flow_2\">\n-        <di:waypoint x=\"340\" y=\"120\" />\n-        <di:waypoint x=\"392\" y=\"120\" />\n-      </bpmndi:BPMNEdge>\n-    </bpmndi:BPMNPlane>\n-  </bpmndi:BPMNDiagram>\n-</bpmn:definitions>'''\n-    \n-    return sample_bpmn\n-\n-if __name__ == \"__main__\":\n-    # Example usage\n-    print(\"AutoTel BPMN Orchestrator - SpiffWorkflow Integration\")\n-    print(\"=\" * 60)\n-    \n-    # Create sample BPMN file\n-    sample_bpmn = create_sample_bpmn_file()\n-    bpmn_path = Path(\"bpmn\")\n-    bpmn_path.mkdir(exist_ok=True)\n-    \n-    with open(bpmn_path / \"sample_process.bpmn\", \"w\") as f:\n-        f.write(sample_bpmn)\n-    \n-    # Initialize orchestrator\n-    orchestrator = BPMNOrchestrator(bpmn_files_path=\"bpmn\")\n-    \n-    # Start a process\n-    instance = orchestrator.start_process(\"Process_1\", {\"input\": \"test\"})\n-    print(f\"Started process instance: {instance.instance_id}\")\n-    \n-    # Execute the process\n-    result = orchestrator.execute_process(instance.instance_id)\n-    print(f\"Process execution completed. Status: {result.status.value}\")\n-    \n-    # Get statistics\n-    stats = orchestrator.get_process_statistics()\n-    print(f\"Orchestrator statistics: {stats}\")\n-#!/usr/bin/env python3\n-\"\"\"\n-AutoTel BPMN Orchestrator - Enterprise BPMN 2.0 Execution Engine\n-Powered by SpiffWorkflow with zero-touch telemetry integration\n-\"\"\"\n-\n-import json\n-import uuid\n-import logging\n-from datetime import datetime, timedelta\n-from typing import Dict, List, Any, Optional, Callable, Union\n-from dataclasses import dataclass, field\n-from enum import Enum\n-from pathlib import Path\n-import xml.etree.ElementTree as ET\n-\n-# SpiffWorkflow imports\n-from SpiffWorkflow.bpmn import BpmnWorkflow\n-from SpiffWorkflow.bpmn.parser.BpmnParser import BpmnParser\n-from SpiffWorkflow.bpmn.specs.bpmn_task_spec import BpmnTaskSpec\n-from SpiffWorkflow.bpmn.specs.bpmn_process_spec import BpmnProcessSpec\n-from SpiffWorkflow.bpmn.util.task import BpmnTaskFilter\n-from SpiffWorkflow.bpmn.util.subworkflow import BpmnSubWorkflow\n-from SpiffWorkflow.bpmn.serializer import BpmnWorkflowSerializer\n-from SpiffWorkflow.bpmn.serializer.config import DEFAULT_CONFIG\n-\n-# OpenTelemetry for observability\n-from opentelemetry import trace, metrics\n-from opentelemetry.trace import Status, StatusCode\n-from opentelemetry.metrics import Counter, Histogram\n-from opentelemetry.sdk.trace import TracerProvider\n-from opentelemetry.sdk.trace.export import ConsoleSpanExporter, BatchSpanProcessor\n-from opentelemetry.sdk.metrics import MeterProvider\n-from opentelemetry.sdk.metrics.export import ConsoleMetricExporter, PeriodicExportingMetricReader\n-\n-# Configure logging\n-logging.basicConfig(level=logging.INFO)\n-logger = logging.getLogger(__name__)\n-\n-# Configure OpenTelemetry\n-trace.set_tracer_provider(TracerProvider())\n-trace.get_tracer_provider().add_span_processor(\n-    BatchSpanProcessor(ConsoleSpanExporter())\n-)\n-\n-metrics.set_meter_provider(MeterProvider())\n-meter = metrics.get_meter(__name__)\n-\n-# Telemetry instruments\n-process_counter = meter.create_counter(\n-    name=\"bpmn_processes_total\",\n-    description=\"Total number of BPMN processes executed\"\n-)\n-\n-task_duration_histogram = meter.create_histogram(\n-    name=\"bpmn_task_duration_seconds\",\n-    description=\"Duration of BPMN tasks in seconds\"\n-)\n-\n-error_counter = meter.create_counter(\n-    name=\"bpmn_errors_total\",\n-    description=\"Total number of BPMN execution errors\"\n-)\n-\n-class ProcessStatus(Enum):\n-    \"\"\"BPMN Process execution status\"\"\"\n-    PENDING = \"pending\"\n-    RUNNING = \"running\"\n-    COMPLETED = \"completed\"\n-    FAILED = \"failed\"\n-    SUSPENDED = \"suspended\"\n-    CANCELLED = \"cancelled\"\n-\n-class TaskStatus(Enum):\n-    \"\"\"BPMN Task execution status\"\"\"\n-    PENDING = \"pending\"\n-    READY = \"ready\"\n-    RUNNING = \"running\"\n-    COMPLETED = \"completed\"\n-    FAILED = \"failed\"\n-    SKIPPED = \"skipped\"\n-    CANCELLED = \"cancelled\"\n-\n-@dataclass\n-class ProcessInstance:\n-    \"\"\"BPMN Process instance with execution state and telemetry\"\"\"\n-    instance_id: str\n-    process_definition_id: str\n-    workflow: BpmnWorkflow\n-    start_time: datetime\n-    end_time: Optional[datetime] = None\n-    status: ProcessStatus = ProcessStatus.PENDING\n-    variables: Dict[str, Any] = field(default_factory=dict)\n-    execution_path: List[str] = field(default_factory=list)\n-    error_message: Optional[str] = None\n-    telemetry_data: Dict[str, Any] = field(default_factory=dict)\n-    \n-    def to_dict(self) -> Dict[str, Any]:\n-        \"\"\"Convert process instance to dictionary for serialization\"\"\"\n-        return {\n-            \"instance_id\": self.instance_id,\n-            \"process_definition_id\": self.process_definition_id,\n-            \"start_time\": self.start_time.isoformat(),\n-            \"end_time\": self.end_time.isoformat() if self.end_time else None,\n-            \"status\": self.status.value,\n-            \"variables\": self.variables,\n-            \"execution_path\": self.execution_path,\n-            \"error_message\": self.error_message,\n-            \"telemetry_data\": self.telemetry_data\n-        }\n-\n-@dataclass\n-class TaskExecution:\n-    \"\"\"BPMN Task execution context with telemetry\"\"\"\n-    task_id: str\n-    task_name: str\n-    task_type: str\n-    start_time: datetime\n-    end_time: Optional[datetime] = None\n-    status: TaskStatus = TaskStatus.PENDING\n-    duration: Optional[float] = None\n-    error_message: Optional[str] = None\n-    input_data: Dict[str, Any] = field(default_factory=dict)\n-    output_data: Dict[str, Any] = field(default_factory=dict)\n-    telemetry_data: Dict[str, Any] = field(default_factory=dict)\n-\n-class BPMNOrchestrator:\n-    \"\"\"\n-    Enterprise-grade BPMN 2.0 Orchestrator powered by SpiffWorkflow\n-    \n-    Features:\n-    - Full BPMN 2.0 specification compliance\n-    - Zero-touch telemetry integration\n-    - Process persistence and recovery\n-    - Advanced task execution monitoring\n-    - Multi-instance process support\n-    - Event-driven execution\n-    - Error handling and recovery\n-    \"\"\"\n-    \n-    def __init__(self, \n-                 bpmn_files_path: Optional[str] = None,\n-                 enable_telemetry: bool = True,\n-                 enable_persistence: bool = True):\n-        \"\"\"\n-        Initialize the BPMN Orchestrator\n-        \n-        Args:\n-            bpmn_files_path: Path to BPMN XML files\n-            enable_telemetry: Enable OpenTelemetry integration\n-            enable_persistence: Enable process state persistence\n-        \"\"\"\n-        self.tracer = trace.get_tracer(__name__)\n-        self.bpmn_files_path = Path(bpmn_files_path) if bpmn_files_path else Path(\"bpmn\")\n-        self.enable_telemetry = enable_telemetry\n-        self.enable_persistence = enable_persistence\n-        \n-        # Initialize SpiffWorkflow parser\n-        self.parser = BpmnParser()\n-        self.parser.add_bpmn_files_by_glob(str(self.bpmn_files_path / \"*.bpmn\"))\n-        \n-        # Process definitions cache\n-        self.process_definitions: Dict[str, BpmnProcessSpec] = {}\n-        self.active_instances: Dict[str, ProcessInstance] = {}\n-        \n-        # Load process definitions\n-        self._load_process_definitions()\n-        \n-        logger.info(f\"BPMN Orchestrator initialized with {len(self.process_definitions)} process definitions\")\n-    \n-    def _load_process_definitions(self) -> None:\n-        \"\"\"Load all BPMN process definitions from the configured path\"\"\"\n-        with self.tracer.start_as_current_span(\"load_process_definitions\") as span:\n-            try:\n-                # Get all process specs from the parser\n-                for process_id, process_spec in self.parser.get_process_specs().items():\n-                    self.process_definitions[process_id] = process_spec\n-                    span.add_event(f\"Loaded process definition: {process_id}\")\n-                \n-                logger.info(f\"Loaded {len(self.process_definitions)} process definitions\")\n-                \n-            except Exception as e:\n-                span.set_status(Status(StatusCode.ERROR, str(e)))\n-                logger.error(f\"Failed to load process definitions: {e}\")\n-                raise\n-    \n-    def start_process(self, \n-                     process_id: str, \n-                     variables: Optional[Dict[str, Any]] = None,\n-                     instance_id: Optional[str] = None) -> ProcessInstance:\n-        \"\"\"\n-        Start a new BPMN process instance\n-        \n-        Args:\n-            process_id: ID of the process to start\n-            variables: Initial process variables\n-            instance_id: Optional custom instance ID\n-            \n-        Returns:\n-            ProcessInstance: The started process instance\n-        \"\"\"\n-        with self.tracer.start_as_current_span(\"start_process\") as span:\n-            span.set_attributes({\n-                \"process.id\": process_id,\n-                \"process.variables\": json.dumps(variables or {})\n-            })\n-            \n-            try:\n-                # Validate process exists\n-                if process_id not in self.process_definitions:\n-                    raise ValueError(f\"Process definition '{process_id}' not found\")\n-                \n-                # Generate instance ID if not provided\n-                if not instance_id:\n-                    instance_id = f\"{process_id}_{uuid.uuid4().hex[:8]}\"\n-                \n-                # Create workflow instance\n-                workflow = BpmnWorkflow(self.process_definitions[process_id])\n-                \n-                # Set initial variables\n-                if variables:\n-                    workflow.data.update(variables)\n-                \n-                # Create process instance\n-                process_instance = ProcessInstance(\n-                    instance_id=instance_id,\n-                    process_definition_id=process_id,\n-                    workflow=workflow,\n-                    start_time=datetime.utcnow(),\n-                    variables=variables or {},\n-                    status=ProcessStatus.RUNNING\n-                )\n-                \n-                # Store instance\n-                self.active_instances[instance_id] = process_instance\n-                \n-                # Update telemetry\n-                process_counter.add(1, {\"process.id\": process_id, \"action\": \"start\"})\n-                \n-                span.set_attributes({\n-                    \"instance.id\": instance_id,\n-                    \"instance.status\": ProcessStatus.RUNNING.value\n-                })\n-                \n-                logger.info(f\"Started process instance {instance_id} for process {process_id}\")\n-                return process_instance\n-                \n-            except Exception as e:\n-                span.set_status(Status(StatusCode.ERROR, str(e)))\n-                error_counter.add(1, {\"process.id\": process_id, \"error.type\": \"start_failed\"})\n-                logger.error(f\"Failed to start process {process_id}: {e}\")\n-                raise\n-    \n-    def execute_process(self, instance_id: str, max_steps: int = 100) -> ProcessInstance:\n-        \"\"\"\n-        Execute a BPMN process instance\n-        \n-        Args:\n-            instance_id: ID of the process instance to execute\n-            max_steps: Maximum number of execution steps\n-            \n-        Returns:\n-            ProcessInstance: Updated process instance\n-        \"\"\"\n-        with self.tracer.start_as_current_span(\"execute_process\") as span:\n-            span.set_attributes({\n-                \"instance.id\": instance_id,\n-                \"execution.max_steps\": max_steps\n-            })\n-            \n-            try:\n-                # Get process instance\n-                if instance_id not in self.active_instances:\n-                    raise ValueError(f\"Process instance '{instance_id}' not found\")\n-                \n-                process_instance = self.active_instances[instance_id]\n-                workflow = process_instance.workflow\n-                \n-                # Execute workflow\n-                step_count = 0\n-                while not workflow.is_completed() and step_count < max_steps:\n-                    # Get ready tasks\n-                    ready_tasks = workflow.get_tasks(state=TaskStatus.READY.value)\n-                    \n-                    if not ready_tasks:\n-                        # No ready tasks, workflow might be waiting for external events\n-                        break\n-                    \n-                    # Execute each ready task\n-                    for task in ready_tasks:\n-                        self._execute_task(task, process_instance)\n-                        step_count += 1\n-                        \n-                        if workflow.is_completed():\n-                            break\n-                \n-                # Update process status\n-                if workflow.is_completed():\n-                    process_instance.status = ProcessStatus.COMPLETED\n-                    process_instance.end_time = datetime.utcnow()\n-                elif step_count >= max_steps:\n-                    process_instance.status = ProcessStatus.SUSPENDED\n-                \n-                # Update execution path\n-                process_instance.execution_path = self._get_execution_path(workflow)\n-                \n-                span.set_attributes({\n-                    \"execution.steps\": step_count,\n-                    \"instance.status\": process_instance.status.value\n-                })\n-                \n-                logger.info(f\"Executed process instance {instance_id} ({step_count} steps)\")\n-                return process_instance\n-                \n-            except Exception as e:\n-                span.set_status(Status(StatusCode.ERROR, str(e)))\n-                error_counter.add(1, {\"instance.id\": instance_id, \"error.type\": \"execution_failed\"})\n-                logger.error(f\"Failed to execute process instance {instance_id}: {e}\")\n-                raise\n-    \n-    def _execute_task(self, task: BpmnTaskSpec, process_instance: ProcessInstance) -> None:\n-        \"\"\"Execute a single BPMN task with telemetry\"\"\"\n-        with self.tracer.start_as_current_span(\"execute_task\") as span:\n-            task_start_time = datetime.utcnow()\n-            \n-            span.set_attributes({\n-                \"task.id\": task.id,\n-                \"task.name\": task.name,\n-                \"task.type\": task.__class__.__name__\n-            })\n-            \n-            try:\n-                # Create task execution context\n-                task_execution = TaskExecution(\n-                    task_id=task.id,\n-                    task_name=task.name,\n-                    task_type=task.__class__.__name__,\n-                    start_time=task_start_time,\n-                    status=TaskStatus.RUNNING,\n-                    input_data=dict(task.data) if hasattr(task, 'data') else {}\n-                )\n-                \n-                # Execute the task\n-                task.complete()\n-                \n-                # Update task execution\n-                task_execution.end_time = datetime.utcnow()\n-                task_execution.duration = (task_execution.end_time - task_execution.start_time).total_seconds()\n-                task_execution.status = TaskStatus.COMPLETED\n-                task_execution.output_data = dict(task.data) if hasattr(task, 'data') else {}\n-                \n-                # Record telemetry\n-                if self.enable_telemetry:\n-                    task_duration_histogram.record(\n-                        task_execution.duration,\n-                        {\n-                            \"task.id\": task.id,\n-                            \"task.type\": task_execution.task_type,\n-                            \"process.id\": process_instance.process_definition_id\n-                        }\n-                    )\n-                \n-                span.set_attributes({\n-                    \"task.duration\": task_execution.duration,\n-                    \"task.status\": TaskStatus.COMPLETED.value\n-                })\n-                \n-                logger.debug(f\"Executed task {task.id} ({task_execution.duration:.3f}s)\")\n-                \n-            except Exception as e:\n-                task_execution.end_time = datetime.utcnow()\n-                task_execution.duration = (task_execution.end_time - task_execution.start_time).total_seconds()\n-                task_execution.status = TaskStatus.FAILED\n-                task_execution.error_message = str(e)\n-                \n-                span.set_status(Status(StatusCode.ERROR, str(e)))\n-                error_counter.add(1, {\n-                    \"task.id\": task.id,\n-                    \"task.type\": task.__class__.__name__,\n-                    \"error.type\": \"task_execution_failed\"\n-                })\n-                \n-                logger.error(f\"Failed to execute task {task.id}: {e}\")\n-                raise\n-    \n-    def _get_execution_path(self, workflow: BpmnWorkflow) -> List[str]:\n-        \"\"\"Get the execution path of completed tasks\"\"\"\n-        try:\n-            completed_tasks = workflow.get_tasks(state=TaskStatus.COMPLETED.value)\n-            return [task.id for task in completed_tasks]\n-        except Exception as e:\n-            logger.warning(f\"Failed to get execution path: {e}\")\n-            return []\n-    \n-    def get_process_instance(self, instance_id: str) -> Optional[ProcessInstance]:\n-        \"\"\"Get a process instance by ID\"\"\"\n-        return self.active_instances.get(instance_id)\n-    \n-    def list_process_instances(self, \n-                             status: Optional[ProcessStatus] = None,\n-                             process_id: Optional[str] = None) -> List[ProcessInstance]:\n-        \"\"\"List process instances with optional filtering\"\"\"\n-        instances = list(self.active_instances.values())\n-        \n-        if status:\n-            instances = [i for i in instances if i.status == status]\n-        \n-        if process_id:\n-            instances = [i for i in instances if i.process_definition_id == process_id]\n-        \n-        return instances\n-    \n-    def cancel_process(self, instance_id: str) -> ProcessInstance:\n-        \"\"\"Cancel a running process instance\"\"\"\n-        with self.tracer.start_as_current_span(\"cancel_process\") as span:\n-            span.set_attributes({\"instance.id\": instance_id})\n-            \n-            try:\n-                if instance_id not in self.active_instances:\n-                    raise ValueError(f\"Process instance '{instance_id}' not found\")\n-                \n-                process_instance = self.active_instances[instance_id]\n-                \n-                if process_instance.status not in [ProcessStatus.RUNNING, ProcessStatus.SUSPENDED]:\n-                    raise ValueError(f\"Cannot cancel process in status: {process_instance.status}\")\n-                \n-                # Cancel the workflow\n-                process_instance.workflow.cancel()\n-                process_instance.status = ProcessStatus.CANCELLED\n-                process_instance.end_time = datetime.utcnow()\n-                \n-                span.set_attributes({\"instance.status\": ProcessStatus.CANCELLED.value})\n-                \n-                logger.info(f\"Cancelled process instance {instance_id}\")\n-                return process_instance\n-                \n-            except Exception as e:\n-                span.set_status(Status(StatusCode.ERROR, str(e)))\n-                error_counter.add(1, {\"instance.id\": instance_id, \"error.type\": \"cancel_failed\"})\n-                logger.error(f\"Failed to cancel process instance {instance_id}: {e}\")\n-                raise\n-    \n-    def get_process_variables(self, instance_id: str) -> Dict[str, Any]:\n-        \"\"\"Get process variables for an instance\"\"\"\n-        if instance_id not in self.active_instances:\n-            raise ValueError(f\"Process instance '{instance_id}' not found\")\n-        \n-        process_instance = self.active_instances[instance_id]\n-        return dict(process_instance.workflow.data)\n-    \n-    def set_process_variables(self, instance_id: str, variables: Dict[str, Any]) -> None:\n-        \"\"\"Set process variables for an instance\"\"\"\n-        if instance_id not in self.active_instances:\n-            raise ValueError(f\"Process instance '{instance_id}' not found\")\n-        \n-        process_instance = self.active_instances[instance_id]\n-        process_instance.workflow.data.update(variables)\n-        process_instance.variables.update(variables)\n-    \n-    def get_ready_tasks(self, instance_id: str) -> List[Dict[str, Any]]:\n-        \"\"\"Get ready tasks for a process instance\"\"\"\n-        if instance_id not in self.active_instances:\n-            raise ValueError(f\"Process instance '{instance_id}' not found\")\n-        \n-        process_instance = self.active_instances[instance_id]\n-        ready_tasks = process_instance.workflow.get_tasks(state=TaskStatus.READY.value)\n-        \n-        return [\n-            {\n-                \"task_id\": task.id,\n-                \"task_name\": task.name,\n-                \"task_type\": task.__class__.__name__,\n-                \"data\": dict(task.data) if hasattr(task, 'data') else {}\n-            }\n-            for task in ready_tasks\n-        ]\n-    \n-    def complete_task(self, instance_id: str, task_id: str, data: Optional[Dict[str, Any]] = None) -> None:\n-        \"\"\"Complete a specific task in a process instance\"\"\"\n-        with self.tracer.start_as_current_span(\"complete_task\") as span:\n-            span.set_attributes({\n-                \"instance.id\": instance_id,\n-                \"task.id\": task_id\n-            })\n-            \n-            try:\n-                if instance_id not in self.active_instances:\n-                    raise ValueError(f\"Process instance '{instance_id}' not found\")\n-                \n-                process_instance = self.active_instances[instance_id]\n-                workflow = process_instance.workflow\n-                \n-                # Find the task\n-                tasks = workflow.get_tasks()\n-                target_task = None\n-                \n-                for task in tasks:\n-                    if task.id == task_id:\n-                        target_task = task\n-                        break\n-                \n-                if not target_task:\n-                    raise ValueError(f\"Task '{task_id}' not found in process instance '{instance_id}'\")\n-                \n-                # Set task data if provided\n-                if data and hasattr(target_task, 'data'):\n-                    target_task.data.update(data)\n-                \n-                # Complete the task\n-                target_task.complete()\n-                \n-                span.set_attributes({\"task.status\": \"completed\"})\n-                \n-                logger.info(f\"Completed task {task_id} in process instance {instance_id}\")\n-                \n-            except Exception as e:\n-                span.set_status(Status(StatusCode.ERROR, str(e)))\n-                error_counter.add(1, {\n-                    \"instance.id\": instance_id,\n-                    \"task.id\": task_id,\n-                    \"error.type\": \"task_completion_failed\"\n-                })\n-                logger.error(f\"Failed to complete task {task_id} in process instance {instance_id}: {e}\")\n-                raise\n-    \n-    def serialize_workflow(self, instance_id: str) -> str:\n-        \"\"\"Serialize a workflow instance to JSON\"\"\"\n-        if instance_id not in self.active_instances:\n-            raise ValueError(f\"Process instance '{instance_id}' not found\")\n-        \n-        process_instance = self.active_instances[instance_id]\n-        serializer = BpmnWorkflowSerializer(DEFAULT_CONFIG)\n-        return serializer.serialize_workflow(process_instance.workflow)\n-    \n-    def deserialize_workflow(self, instance_id: str, serialized_data: str) -> ProcessInstance:\n-        \"\"\"Deserialize a workflow instance from JSON\"\"\"\n-        serializer = BpmnWorkflowSerializer(BPMN_CONFIG)\n-        workflow = serializer.deserialize_workflow(serialized_data)\n-        \n-        process_instance = ProcessInstance(\n-            instance_id=instance_id,\n-            process_definition_id=workflow.spec.name,\n-            workflow=workflow,\n-            start_time=datetime.utcnow(),\n-            status=ProcessStatus.RUNNING\n-        )\n-        \n-        self.active_instances[instance_id] = process_instance\n-        return process_instance\n-    \n-    def get_process_statistics(self) -> Dict[str, Any]:\n-        \"\"\"Get orchestrator statistics\"\"\"\n-        total_instances = len(self.active_instances)\n-        status_counts = {}\n-        \n-        for status in ProcessStatus:\n-            status_counts[status.value] = len([\n-                i for i in self.active_instances.values() \n-                if i.status == status\n-            ])\n-        \n-        return {\n-            \"total_process_definitions\": len(self.process_definitions),\n-            \"total_instances\": total_instances,\n-            \"status_distribution\": status_counts,\n-            \"process_definitions\": list(self.process_definitions.keys())\n-        }\n-    \n-    def cleanup_completed_processes(self, max_age_hours: int = 24) -> int:\n-        \"\"\"Clean up completed process instances older than specified age\"\"\"\n-        cutoff_time = datetime.utcnow() - timedelta(hours=max_age_hours)\n-        removed_count = 0\n-        \n-        instances_to_remove = []\n-        \n-        for instance_id, process_instance in self.active_instances.items():\n-            if (process_instance.status in [ProcessStatus.COMPLETED, ProcessStatus.FAILED, ProcessStatus.CANCELLED] and\n-                process_instance.end_time and process_instance.end_time < cutoff_time):\n-                instances_to_remove.append(instance_id)\n-        \n-        for instance_id in instances_to_remove:\n-            del self.active_instances[instance_id]\n-            removed_count += 1\n-        \n-        logger.info(f\"Cleaned up {removed_count} completed process instances\")\n-        return removed_count\n-\n-# Example usage and utility functions\n-def create_sample_bpmn_file() -> str:\n-    \"\"\"Create a sample BPMN file for testing\"\"\"\n-    sample_bpmn = '''<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n-<bpmn:definitions xmlns:bpmn=\"http://www.omg.org/spec/BPMN/20100524/MODEL\"\n-                  xmlns:bpmndi=\"http://www.omg.org/spec/BPMN/20100524/DI\"\n-                  xmlns:dc=\"http://www.omg.org/spec/DD/20100524/DC\"\n-                  xmlns:di=\"http://www.omg.org/spec/DD/20100524/DI\"\n-                  id=\"Definitions_1\"\n-                  targetNamespace=\"http://bpmn.io/schema/bpmn\">\n-  <bpmn:process id=\"Process_1\" isExecutable=\"true\">\n-    <bpmn:startEvent id=\"StartEvent_1\" name=\"Start\">\n-      <bpmn:outgoing>Flow_1</bpmn:outgoing>\n-    </bpmn:startEvent>\n-    <bpmn:task id=\"Task_1\" name=\"Sample Task\">\n-      <bpmn:incoming>Flow_1</bpmn:incoming>\n-      <bpmn:outgoing>Flow_2</bpmn:outgoing>\n-    </bpmn:task>\n-    <bpmn:endEvent id=\"EndEvent_1\" name=\"End\">\n-      <bpmn:incoming>Flow_2</bpmn:incoming>\n-    </bpmn:endEvent>\n-    <bpmn:sequenceFlow id=\"Flow_1\" sourceRef=\"StartEvent_1\" targetRef=\"Task_1\" />\n-    <bpmn:sequenceFlow id=\"Flow_2\" sourceRef=\"Task_1\" targetRef=\"EndEvent_1\" />\n-  </bpmn:process>\n-  <bpmndi:BPMNDiagram id=\"BPMNDiagram_1\">\n-    <bpmndi:BPMNPlane id=\"BPMNPlane_1\" bpmnElement=\"Process_1\">\n-      <bpmndi:BPMNShape id=\"StartEvent_1_di\" bpmnElement=\"StartEvent_1\">\n-        <dc:Bounds x=\"152\" y=\"102\" width=\"36\" height=\"36\" />\n-      </bpmndi:BPMNShape>\n-      <bpmndi:BPMNShape id=\"Task_1_di\" bpmnElement=\"Task_1\">\n-        <dc:Bounds x=\"240\" y=\"80\" width=\"100\" height=\"80\" />\n-      </bpmndi:BPMNShape>\n-      <bpmndi:BPMNShape id=\"EndEvent_1_di\" bpmnElement=\"EndEvent_1\">\n-        <dc:Bounds x=\"392\" y=\"102\" width=\"36\" height=\"36\" />\n-      </bpmndi:BPMNShape>\n-      <bpmndi:BPMNEdge id=\"Flow_1_di\" bpmnElement=\"Flow_1\">\n-        <di:waypoint x=\"188\" y=\"120\" />\n-        <di:waypoint x=\"240\" y=\"120\" />\n-      </bpmndi:BPMNEdge>\n-      <bpmndi:BPMNEdge id=\"Flow_2_di\" bpmnElement=\"Flow_2\">\n-        <di:waypoint x=\"340\" y=\"120\" />\n-        <di:waypoint x=\"392\" y=\"120\" />\n-      </bpmndi:BPMNEdge>\n-    </bpmndi:BPMNPlane>\n-  </bpmndi:BPMNDiagram>\n-</bpmn:definitions>'''\n-    \n-    return sample_bpmn\n-\n-if __name__ == \"__main__\":\n-    # Example usage\n-    print(\"AutoTel BPMN Orchestrator - SpiffWorkflow Integration\")\n-    print(\"=\" * 60)\n-    \n-    # Create sample BPMN file\n-    sample_bpmn = create_sample_bpmn_file()\n-    bpmn_path = Path(\"bpmn\")\n-    bpmn_path.mkdir(exist_ok=True)\n-    \n-    with open(bpmn_path / \"sample_process.bpmn\", \"w\") as f:\n-        f.write(sample_bpmn)\n-    \n-    # Initialize orchestrator\n-    orchestrator = BPMNOrchestrator(bpmn_files_path=\"bpmn\")\n-    \n-    # Start a process\n-    instance = orchestrator.start_process(\"Process_1\", {\"input\": \"test\"})\n-    print(f\"Started process instance: {instance.instance_id}\")\n-    \n-    # Execute the process\n-    result = orchestrator.execute_process(instance.instance_id)\n-    print(f\"Process execution completed. Status: {result.status.value}\")\n-    \n-    # Get statistics\n-    stats = orchestrator.get_process_statistics()\n     print(f\"Orchestrator statistics: {stats}\")\n\\ No newline at end of file\n"
                }
            ],
            "date": 1752191746285,
            "name": "Commit-0",
            "content": "#!/usr/bin/env python3\n\"\"\"\nAutoTel BPMN Orchestrator - Enterprise BPMN 2.0 Execution Engine\nPowered by SpiffWorkflow with zero-touch telemetry integration\n\"\"\"\n\nimport json\nimport uuid\nimport logging\nfrom datetime import datetime, timedelta\nfrom typing import Dict, List, Any, Optional, Callable, Union\nfrom dataclasses import dataclass, field\nfrom enum import Enum\nfrom pathlib import Path\nimport xml.etree.ElementTree as ET\n\n# SpiffWorkflow imports\nfrom SpiffWorkflow.bpmn import BpmnWorkflow\nfrom SpiffWorkflow.bpmn.parser.BpmnParser import BpmnParser\nfrom SpiffWorkflow.bpmn.specs.bpmn_task_spec import BpmnTaskSpec\nfrom SpiffWorkflow.bpmn.specs.bpmn_process_spec import BpmnProcessSpec\nfrom SpiffWorkflow.bpmn.util.task import BpmnTaskFilter\nfrom SpiffWorkflow.bpmn.util.subworkflow import BpmnSubWorkflow\nfrom SpiffWorkflow.bpmn.serializer import BpmnWorkflowSerializer\nfrom SpiffWorkflow.bpmn.serializer.config import BPMN_CONFIG\n\n# OpenTelemetry for observability\nfrom opentelemetry import trace, metrics\nfrom opentelemetry.trace import Status, StatusCode\nfrom opentelemetry.metrics import Counter, Histogram\nfrom opentelemetry.sdk.trace import TracerProvider\nfrom opentelemetry.sdk.trace.export import ConsoleSpanExporter, BatchSpanProcessor\nfrom opentelemetry.sdk.metrics import MeterProvider\nfrom opentelemetry.sdk.metrics.export import ConsoleMetricExporter, PeriodicExportingMetricReader\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n# Configure OpenTelemetry\ntrace.set_tracer_provider(TracerProvider())\ntrace.get_tracer_provider().add_span_processor(\n    BatchSpanProcessor(ConsoleSpanExporter())\n)\n\nmetrics.set_meter_provider(MeterProvider())\nmeter = metrics.get_meter(__name__)\n\n# Telemetry instruments\nprocess_counter = meter.create_counter(\n    name=\"bpmn_processes_total\",\n    description=\"Total number of BPMN processes executed\"\n)\n\ntask_duration_histogram = meter.create_histogram(\n    name=\"bpmn_task_duration_seconds\",\n    description=\"Duration of BPMN tasks in seconds\"\n)\n\nerror_counter = meter.create_counter(\n    name=\"bpmn_errors_total\",\n    description=\"Total number of BPMN execution errors\"\n)\n\nclass ProcessStatus(Enum):\n    \"\"\"BPMN Process execution status\"\"\"\n    PENDING = \"pending\"\n    RUNNING = \"running\"\n    COMPLETED = \"completed\"\n    FAILED = \"failed\"\n    SUSPENDED = \"suspended\"\n    CANCELLED = \"cancelled\"\n\nclass TaskStatus(Enum):\n    \"\"\"BPMN Task execution status\"\"\"\n    PENDING = \"pending\"\n    READY = \"ready\"\n    RUNNING = \"running\"\n    COMPLETED = \"completed\"\n    FAILED = \"failed\"\n    SKIPPED = \"skipped\"\n    CANCELLED = \"cancelled\"\n\n@dataclass\nclass ProcessInstance:\n    \"\"\"BPMN Process instance with execution state and telemetry\"\"\"\n    instance_id: str\n    process_definition_id: str\n    workflow: BpmnWorkflow\n    start_time: datetime\n    end_time: Optional[datetime] = None\n    status: ProcessStatus = ProcessStatus.PENDING\n    variables: Dict[str, Any] = field(default_factory=dict)\n    execution_path: List[str] = field(default_factory=list)\n    error_message: Optional[str] = None\n    telemetry_data: Dict[str, Any] = field(default_factory=dict)\n    \n    def to_dict(self) -> Dict[str, Any]:\n        \"\"\"Convert process instance to dictionary for serialization\"\"\"\n        return {\n            \"instance_id\": self.instance_id,\n            \"process_definition_id\": self.process_definition_id,\n            \"start_time\": self.start_time.isoformat(),\n            \"end_time\": self.end_time.isoformat() if self.end_time else None,\n            \"status\": self.status.value,\n            \"variables\": self.variables,\n            \"execution_path\": self.execution_path,\n            \"error_message\": self.error_message,\n            \"telemetry_data\": self.telemetry_data\n        }\n\n@dataclass\nclass TaskExecution:\n    \"\"\"BPMN Task execution context with telemetry\"\"\"\n    task_id: str\n    task_name: str\n    task_type: str\n    start_time: datetime\n    end_time: Optional[datetime] = None\n    status: TaskStatus = TaskStatus.PENDING\n    duration: Optional[float] = None\n    error_message: Optional[str] = None\n    input_data: Dict[str, Any] = field(default_factory=dict)\n    output_data: Dict[str, Any] = field(default_factory=dict)\n    telemetry_data: Dict[str, Any] = field(default_factory=dict)\n\nclass BPMNOrchestrator:\n    \"\"\"\n    Enterprise-grade BPMN 2.0 Orchestrator powered by SpiffWorkflow\n    \n    Features:\n    - Full BPMN 2.0 specification compliance\n    - Zero-touch telemetry integration\n    - Process persistence and recovery\n    - Advanced task execution monitoring\n    - Multi-instance process support\n    - Event-driven execution\n    - Error handling and recovery\n    \"\"\"\n    \n    def __init__(self, \n                 bpmn_files_path: Optional[str] = None,\n                 enable_telemetry: bool = True,\n                 enable_persistence: bool = True):\n        \"\"\"\n        Initialize the BPMN Orchestrator\n        \n        Args:\n            bpmn_files_path: Path to BPMN XML files\n            enable_telemetry: Enable OpenTelemetry integration\n            enable_persistence: Enable process state persistence\n        \"\"\"\n        self.tracer = trace.get_tracer(__name__)\n        self.bpmn_files_path = Path(bpmn_files_path) if bpmn_files_path else Path(\"bpmn\")\n        self.enable_telemetry = enable_telemetry\n        self.enable_persistence = enable_persistence\n        \n        # Initialize SpiffWorkflow parser\n        self.parser = BpmnParser()\n        self.parser.add_bpmn_files_by_glob(str(self.bpmn_files_path / \"*.bpmn\"))\n        \n        # Process definitions cache\n        self.process_definitions: Dict[str, BpmnProcessSpec] = {}\n        self.active_instances: Dict[str, ProcessInstance] = {}\n        \n        # Load process definitions\n        self._load_process_definitions()\n        \n        logger.info(f\"BPMN Orchestrator initialized with {len(self.process_definitions)} process definitions\")\n    \n    def _load_process_definitions(self) -> None:\n        \"\"\"Load all BPMN process definitions from the configured path\"\"\"\n        with self.tracer.start_as_current_span(\"load_process_definitions\") as span:\n            try:\n                # Get all process specs from the parser\n                for process_id, process_spec in self.parser.get_process_specs().items():\n                    self.process_definitions[process_id] = process_spec\n                    span.add_event(f\"Loaded process definition: {process_id}\")\n                \n                logger.info(f\"Loaded {len(self.process_definitions)} process definitions\")\n                \n            except Exception as e:\n                span.set_status(Status(StatusCode.ERROR, str(e)))\n                logger.error(f\"Failed to load process definitions: {e}\")\n                raise\n    \n    def start_process(self, \n                     process_id: str, \n                     variables: Optional[Dict[str, Any]] = None,\n                     instance_id: Optional[str] = None) -> ProcessInstance:\n        \"\"\"\n        Start a new BPMN process instance\n        \n        Args:\n            process_id: ID of the process to start\n            variables: Initial process variables\n            instance_id: Optional custom instance ID\n            \n        Returns:\n            ProcessInstance: The started process instance\n        \"\"\"\n        with self.tracer.start_as_current_span(\"start_process\") as span:\n            span.set_attributes({\n                \"process.id\": process_id,\n                \"process.variables\": json.dumps(variables or {})\n            })\n            \n            try:\n                # Validate process exists\n                if process_id not in self.process_definitions:\n                    raise ValueError(f\"Process definition '{process_id}' not found\")\n                \n                # Generate instance ID if not provided\n                if not instance_id:\n                    instance_id = f\"{process_id}_{uuid.uuid4().hex[:8]}\"\n                \n                # Create workflow instance\n                workflow = BpmnWorkflow(self.process_definitions[process_id])\n                \n                # Set initial variables\n                if variables:\n                    workflow.data.update(variables)\n                \n                # Create process instance\n                process_instance = ProcessInstance(\n                    instance_id=instance_id,\n                    process_definition_id=process_id,\n                    workflow=workflow,\n                    start_time=datetime.utcnow(),\n                    variables=variables or {},\n                    status=ProcessStatus.RUNNING\n                )\n                \n                # Store instance\n                self.active_instances[instance_id] = process_instance\n                \n                # Update telemetry\n                process_counter.add(1, {\"process.id\": process_id, \"action\": \"start\"})\n                \n                span.set_attributes({\n                    \"instance.id\": instance_id,\n                    \"instance.status\": ProcessStatus.RUNNING.value\n                })\n                \n                logger.info(f\"Started process instance {instance_id} for process {process_id}\")\n                return process_instance\n                \n            except Exception as e:\n                span.set_status(Status(StatusCode.ERROR, str(e)))\n                error_counter.add(1, {\"process.id\": process_id, \"error.type\": \"start_failed\"})\n                logger.error(f\"Failed to start process {process_id}: {e}\")\n                raise\n    \n    def execute_process(self, instance_id: str, max_steps: int = 100) -> ProcessInstance:\n        \"\"\"\n        Execute a BPMN process instance\n        \n        Args:\n            instance_id: ID of the process instance to execute\n            max_steps: Maximum number of execution steps\n            \n        Returns:\n            ProcessInstance: Updated process instance\n        \"\"\"\n        with self.tracer.start_as_current_span(\"execute_process\") as span:\n            span.set_attributes({\n                \"instance.id\": instance_id,\n                \"execution.max_steps\": max_steps\n            })\n            \n            try:\n                # Get process instance\n                if instance_id not in self.active_instances:\n                    raise ValueError(f\"Process instance '{instance_id}' not found\")\n                \n                process_instance = self.active_instances[instance_id]\n                workflow = process_instance.workflow\n                \n                # Execute workflow\n                step_count = 0\n                while not workflow.is_completed() and step_count < max_steps:\n                    # Get ready tasks\n                    ready_tasks = workflow.get_tasks(state=TaskStatus.READY.value)\n                    \n                    if not ready_tasks:\n                        # No ready tasks, workflow might be waiting for external events\n                        break\n                    \n                    # Execute each ready task\n                    for task in ready_tasks:\n                        self._execute_task(task, process_instance)\n                        step_count += 1\n                        \n                        if workflow.is_completed():\n                            break\n                \n                # Update process status\n                if workflow.is_completed():\n                    process_instance.status = ProcessStatus.COMPLETED\n                    process_instance.end_time = datetime.utcnow()\n                elif step_count >= max_steps:\n                    process_instance.status = ProcessStatus.SUSPENDED\n                \n                # Update execution path\n                process_instance.execution_path = self._get_execution_path(workflow)\n                \n                span.set_attributes({\n                    \"execution.steps\": step_count,\n                    \"instance.status\": process_instance.status.value\n                })\n                \n                logger.info(f\"Executed process instance {instance_id} ({step_count} steps)\")\n                return process_instance\n                \n            except Exception as e:\n                span.set_status(Status(StatusCode.ERROR, str(e)))\n                error_counter.add(1, {\"instance.id\": instance_id, \"error.type\": \"execution_failed\"})\n                logger.error(f\"Failed to execute process instance {instance_id}: {e}\")\n                raise\n    \n    def _execute_task(self, task: BpmnTaskSpec, process_instance: ProcessInstance) -> None:\n        \"\"\"Execute a single BPMN task with telemetry\"\"\"\n        with self.tracer.start_as_current_span(\"execute_task\") as span:\n            task_start_time = datetime.utcnow()\n            \n            span.set_attributes({\n                \"task.id\": task.id,\n                \"task.name\": task.name,\n                \"task.type\": task.__class__.__name__\n            })\n            \n            try:\n                # Create task execution context\n                task_execution = TaskExecution(\n                    task_id=task.id,\n                    task_name=task.name,\n                    task_type=task.__class__.__name__,\n                    start_time=task_start_time,\n                    status=TaskStatus.RUNNING,\n                    input_data=dict(task.data) if hasattr(task, 'data') else {}\n                )\n                \n                # Execute the task\n                task.complete()\n                \n                # Update task execution\n                task_execution.end_time = datetime.utcnow()\n                task_execution.duration = (task_execution.end_time - task_execution.start_time).total_seconds()\n                task_execution.status = TaskStatus.COMPLETED\n                task_execution.output_data = dict(task.data) if hasattr(task, 'data') else {}\n                \n                # Record telemetry\n                if self.enable_telemetry:\n                    task_duration_histogram.record(\n                        task_execution.duration,\n                        {\n                            \"task.id\": task.id,\n                            \"task.type\": task_execution.task_type,\n                            \"process.id\": process_instance.process_definition_id\n                        }\n                    )\n                \n                span.set_attributes({\n                    \"task.duration\": task_execution.duration,\n                    \"task.status\": TaskStatus.COMPLETED.value\n                })\n                \n                logger.debug(f\"Executed task {task.id} ({task_execution.duration:.3f}s)\")\n                \n            except Exception as e:\n                task_execution.end_time = datetime.utcnow()\n                task_execution.duration = (task_execution.end_time - task_execution.start_time).total_seconds()\n                task_execution.status = TaskStatus.FAILED\n                task_execution.error_message = str(e)\n                \n                span.set_status(Status(StatusCode.ERROR, str(e)))\n                error_counter.add(1, {\n                    \"task.id\": task.id,\n                    \"task.type\": task.__class__.__name__,\n                    \"error.type\": \"task_execution_failed\"\n                })\n                \n                logger.error(f\"Failed to execute task {task.id}: {e}\")\n                raise\n    \n    def _get_execution_path(self, workflow: BpmnWorkflow) -> List[str]:\n        \"\"\"Get the execution path of completed tasks\"\"\"\n        try:\n            completed_tasks = workflow.get_tasks(state=TaskStatus.COMPLETED.value)\n            return [task.id for task in completed_tasks]\n        except Exception as e:\n            logger.warning(f\"Failed to get execution path: {e}\")\n            return []\n    \n    def get_process_instance(self, instance_id: str) -> Optional[ProcessInstance]:\n        \"\"\"Get a process instance by ID\"\"\"\n        return self.active_instances.get(instance_id)\n    \n    def list_process_instances(self, \n                             status: Optional[ProcessStatus] = None,\n                             process_id: Optional[str] = None) -> List[ProcessInstance]:\n        \"\"\"List process instances with optional filtering\"\"\"\n        instances = list(self.active_instances.values())\n        \n        if status:\n            instances = [i for i in instances if i.status == status]\n        \n        if process_id:\n            instances = [i for i in instances if i.process_definition_id == process_id]\n        \n        return instances\n    \n    def cancel_process(self, instance_id: str) -> ProcessInstance:\n        \"\"\"Cancel a running process instance\"\"\"\n        with self.tracer.start_as_current_span(\"cancel_process\") as span:\n            span.set_attributes({\"instance.id\": instance_id})\n            \n            try:\n                if instance_id not in self.active_instances:\n                    raise ValueError(f\"Process instance '{instance_id}' not found\")\n                \n                process_instance = self.active_instances[instance_id]\n                \n                if process_instance.status not in [ProcessStatus.RUNNING, ProcessStatus.SUSPENDED]:\n                    raise ValueError(f\"Cannot cancel process in status: {process_instance.status}\")\n                \n                # Cancel the workflow\n                process_instance.workflow.cancel()\n                process_instance.status = ProcessStatus.CANCELLED\n                process_instance.end_time = datetime.utcnow()\n                \n                span.set_attributes({\"instance.status\": ProcessStatus.CANCELLED.value})\n                \n                logger.info(f\"Cancelled process instance {instance_id}\")\n                return process_instance\n                \n            except Exception as e:\n                span.set_status(Status(StatusCode.ERROR, str(e)))\n                error_counter.add(1, {\"instance.id\": instance_id, \"error.type\": \"cancel_failed\"})\n                logger.error(f\"Failed to cancel process instance {instance_id}: {e}\")\n                raise\n    \n    def get_process_variables(self, instance_id: str) -> Dict[str, Any]:\n        \"\"\"Get process variables for an instance\"\"\"\n        if instance_id not in self.active_instances:\n            raise ValueError(f\"Process instance '{instance_id}' not found\")\n        \n        process_instance = self.active_instances[instance_id]\n        return dict(process_instance.workflow.data)\n    \n    def set_process_variables(self, instance_id: str, variables: Dict[str, Any]) -> None:\n        \"\"\"Set process variables for an instance\"\"\"\n        if instance_id not in self.active_instances:\n            raise ValueError(f\"Process instance '{instance_id}' not found\")\n        \n        process_instance = self.active_instances[instance_id]\n        process_instance.workflow.data.update(variables)\n        process_instance.variables.update(variables)\n    \n    def get_ready_tasks(self, instance_id: str) -> List[Dict[str, Any]]:\n        \"\"\"Get ready tasks for a process instance\"\"\"\n        if instance_id not in self.active_instances:\n            raise ValueError(f\"Process instance '{instance_id}' not found\")\n        \n        process_instance = self.active_instances[instance_id]\n        ready_tasks = process_instance.workflow.get_tasks(state=TaskStatus.READY.value)\n        \n        return [\n            {\n                \"task_id\": task.id,\n                \"task_name\": task.name,\n                \"task_type\": task.__class__.__name__,\n                \"data\": dict(task.data) if hasattr(task, 'data') else {}\n            }\n            for task in ready_tasks\n        ]\n    \n    def complete_task(self, instance_id: str, task_id: str, data: Optional[Dict[str, Any]] = None) -> None:\n        \"\"\"Complete a specific task in a process instance\"\"\"\n        with self.tracer.start_as_current_span(\"complete_task\") as span:\n            span.set_attributes({\n                \"instance.id\": instance_id,\n                \"task.id\": task_id\n            })\n            \n            try:\n                if instance_id not in self.active_instances:\n                    raise ValueError(f\"Process instance '{instance_id}' not found\")\n                \n                process_instance = self.active_instances[instance_id]\n                workflow = process_instance.workflow\n                \n                # Find the task\n                tasks = workflow.get_tasks()\n                target_task = None\n                \n                for task in tasks:\n                    if task.id == task_id:\n                        target_task = task\n                        break\n                \n                if not target_task:\n                    raise ValueError(f\"Task '{task_id}' not found in process instance '{instance_id}'\")\n                \n                # Set task data if provided\n                if data and hasattr(target_task, 'data'):\n                    target_task.data.update(data)\n                \n                # Complete the task\n                target_task.complete()\n                \n                span.set_attributes({\"task.status\": \"completed\"})\n                \n                logger.info(f\"Completed task {task_id} in process instance {instance_id}\")\n                \n            except Exception as e:\n                span.set_status(Status(StatusCode.ERROR, str(e)))\n                error_counter.add(1, {\n                    \"instance.id\": instance_id,\n                    \"task.id\": task_id,\n                    \"error.type\": \"task_completion_failed\"\n                })\n                logger.error(f\"Failed to complete task {task_id} in process instance {instance_id}: {e}\")\n                raise\n    \n    def serialize_workflow(self, instance_id: str) -> str:\n        \"\"\"Serialize a workflow instance to JSON\"\"\"\n        if instance_id not in self.active_instances:\n            raise ValueError(f\"Process instance '{instance_id}' not found\")\n        \n        process_instance = self.active_instances[instance_id]\n        serializer = BpmnWorkflowSerializer(BPMN_CONFIG)\n        return serializer.serialize_workflow(process_instance.workflow)\n    \n    def deserialize_workflow(self, instance_id: str, serialized_data: str) -> ProcessInstance:\n        \"\"\"Deserialize a workflow instance from JSON\"\"\"\n        serializer = BpmnWorkflowSerializer(BPMN_CONFIG)\n        workflow = serializer.deserialize_workflow(serialized_data)\n        \n        process_instance = ProcessInstance(\n            instance_id=instance_id,\n            process_definition_id=workflow.spec.name,\n            workflow=workflow,\n            start_time=datetime.utcnow(),\n            status=ProcessStatus.RUNNING\n        )\n        \n        self.active_instances[instance_id] = process_instance\n        return process_instance\n    \n    def get_process_statistics(self) -> Dict[str, Any]:\n        \"\"\"Get orchestrator statistics\"\"\"\n        total_instances = len(self.active_instances)\n        status_counts = {}\n        \n        for status in ProcessStatus:\n            status_counts[status.value] = len([\n                i for i in self.active_instances.values() \n                if i.status == status\n            ])\n        \n        return {\n            \"total_process_definitions\": len(self.process_definitions),\n            \"total_instances\": total_instances,\n            \"status_distribution\": status_counts,\n            \"process_definitions\": list(self.process_definitions.keys())\n        }\n    \n    def cleanup_completed_processes(self, max_age_hours: int = 24) -> int:\n        \"\"\"Clean up completed process instances older than specified age\"\"\"\n        cutoff_time = datetime.utcnow() - timedelta(hours=max_age_hours)\n        removed_count = 0\n        \n        instances_to_remove = []\n        \n        for instance_id, process_instance in self.active_instances.items():\n            if (process_instance.status in [ProcessStatus.COMPLETED, ProcessStatus.FAILED, ProcessStatus.CANCELLED] and\n                process_instance.end_time and process_instance.end_time < cutoff_time):\n                instances_to_remove.append(instance_id)\n        \n        for instance_id in instances_to_remove:\n            del self.active_instances[instance_id]\n            removed_count += 1\n        \n        logger.info(f\"Cleaned up {removed_count} completed process instances\")\n        return removed_count\n\n# Example usage and utility functions\ndef create_sample_bpmn_file() -> str:\n    \"\"\"Create a sample BPMN file for testing\"\"\"\n    sample_bpmn = '''<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<bpmn:definitions xmlns:bpmn=\"http://www.omg.org/spec/BPMN/20100524/MODEL\"\n                  xmlns:bpmndi=\"http://www.omg.org/spec/BPMN/20100524/DI\"\n                  xmlns:dc=\"http://www.omg.org/spec/DD/20100524/DC\"\n                  xmlns:di=\"http://www.omg.org/spec/DD/20100524/DI\"\n                  id=\"Definitions_1\"\n                  targetNamespace=\"http://bpmn.io/schema/bpmn\">\n  <bpmn:process id=\"Process_1\" isExecutable=\"true\">\n    <bpmn:startEvent id=\"StartEvent_1\" name=\"Start\">\n      <bpmn:outgoing>Flow_1</bpmn:outgoing>\n    </bpmn:startEvent>\n    <bpmn:task id=\"Task_1\" name=\"Sample Task\">\n      <bpmn:incoming>Flow_1</bpmn:incoming>\n      <bpmn:outgoing>Flow_2</bpmn:outgoing>\n    </bpmn:task>\n    <bpmn:endEvent id=\"EndEvent_1\" name=\"End\">\n      <bpmn:incoming>Flow_2</bpmn:incoming>\n    </bpmn:endEvent>\n    <bpmn:sequenceFlow id=\"Flow_1\" sourceRef=\"StartEvent_1\" targetRef=\"Task_1\" />\n    <bpmn:sequenceFlow id=\"Flow_2\" sourceRef=\"Task_1\" targetRef=\"EndEvent_1\" />\n  </bpmn:process>\n  <bpmndi:BPMNDiagram id=\"BPMNDiagram_1\">\n    <bpmndi:BPMNPlane id=\"BPMNPlane_1\" bpmnElement=\"Process_1\">\n      <bpmndi:BPMNShape id=\"StartEvent_1_di\" bpmnElement=\"StartEvent_1\">\n        <dc:Bounds x=\"152\" y=\"102\" width=\"36\" height=\"36\" />\n      </bpmndi:BPMNShape>\n      <bpmndi:BPMNShape id=\"Task_1_di\" bpmnElement=\"Task_1\">\n        <dc:Bounds x=\"240\" y=\"80\" width=\"100\" height=\"80\" />\n      </bpmndi:BPMNShape>\n      <bpmndi:BPMNShape id=\"EndEvent_1_di\" bpmnElement=\"EndEvent_1\">\n        <dc:Bounds x=\"392\" y=\"102\" width=\"36\" height=\"36\" />\n      </bpmndi:BPMNShape>\n      <bpmndi:BPMNEdge id=\"Flow_1_di\" bpmnElement=\"Flow_1\">\n        <di:waypoint x=\"188\" y=\"120\" />\n        <di:waypoint x=\"240\" y=\"120\" />\n      </bpmndi:BPMNEdge>\n      <bpmndi:BPMNEdge id=\"Flow_2_di\" bpmnElement=\"Flow_2\">\n        <di:waypoint x=\"340\" y=\"120\" />\n        <di:waypoint x=\"392\" y=\"120\" />\n      </bpmndi:BPMNEdge>\n    </bpmndi:BPMNPlane>\n  </bpmndi:BPMNDiagram>\n</bpmn:definitions>'''\n    \n    return sample_bpmn\n\nif __name__ == \"__main__\":\n    # Example usage\n    print(\"AutoTel BPMN Orchestrator - SpiffWorkflow Integration\")\n    print(\"=\" * 60)\n    \n    # Create sample BPMN file\n    sample_bpmn = create_sample_bpmn_file()\n    bpmn_path = Path(\"bpmn\")\n    bpmn_path.mkdir(exist_ok=True)\n    \n    with open(bpmn_path / \"sample_process.bpmn\", \"w\") as f:\n        f.write(sample_bpmn)\n    \n    # Initialize orchestrator\n    orchestrator = BPMNOrchestrator(bpmn_files_path=\"bpmn\")\n    \n    # Start a process\n    instance = orchestrator.start_process(\"Process_1\", {\"input\": \"test\"})\n    print(f\"Started process instance: {instance.instance_id}\")\n    \n    # Execute the process\n    result = orchestrator.execute_process(instance.instance_id)\n    print(f\"Process execution completed. Status: {result.status.value}\")\n    \n    # Get statistics\n    stats = orchestrator.get_process_statistics()\n    print(f\"Orchestrator statistics: {stats}\")"
        }
    ]
}