{
    "sourceFile": "spiff_orchestration_examples.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 3,
            "patches": [
                {
                    "date": 1752192373198,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1752192391523,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -0,0 +1,845 @@\n+#!/usr/bin/env python3\n+\"\"\"\n+SpiffWorkflow Orchestration Examples - LinkML + OpenTelemetry Validation Paradigm\n+Demonstrates how code agents validate their work by generating structured telemetry\n+\"\"\"\n+\n+import json\n+import uuid\n+import logging\n+from datetime import datetime, timedelta\n+from typing import Dict, List, Any, Optional, Union\n+from dataclasses import dataclass, field\n+from pathlib import Path\n+import yaml\n+\n+# SpiffWorkflow imports\n+from SpiffWorkflow.bpmn import BpmnWorkflow\n+from SpiffWorkflow.bpmn.parser.BpmnParser import BpmnParser\n+from SpiffWorkflow.bpmn.specs.bpmn_task_spec import BpmnTaskSpec\n+from SpiffWorkflow.bpmn.specs.bpmn_process_spec import BpmnProcessSpec\n+from SpiffWorkflow.util.task import TaskState\n+from SpiffWorkflow.bpmn.serializer import BpmnWorkflowSerializer\n+from SpiffWorkflow.bpmn.serializer.config import DEFAULT_CONFIG\n+\n+# OpenTelemetry for structured telemetry\n+from opentelemetry import trace, metrics\n+from opentelemetry.trace import Status, StatusCode\n+from opentelemetry.metrics import Counter, Histogram\n+from opentelemetry.sdk.trace import TracerProvider\n+from opentelemetry.sdk.trace.export import ConsoleSpanExporter, BatchSpanProcessor\n+from opentelemetry.sdk.metrics import MeterProvider\n+\n+# LinkML for schema validation\n+try:\n+    from linkml_runtime.utils.schemaview import SchemaView\n+    from linkml_runtime.loaders import yaml_loader, json_loader\n+    from linkml_runtime.dumpers import yaml_dumper, json_dumper\n+    LINKML_AVAILABLE = True\n+except ImportError:\n+    LINKML_AVAILABLE = False\n+    print(\"⚠️ LinkML not available - telemetry validation disabled\")\n+\n+# Configure logging\n+logging.basicConfig(level=logging.INFO)\n+logger = logging.getLogger(__name__)\n+\n+# Configure OpenTelemetry\n+trace.set_tracer_provider(TracerProvider())\n+trace.get_tracer_provider().add_span_processor(\n+    BatchSpanProcessor(ConsoleSpanExporter())\n+)\n+\n+metrics.set_meter_provider(MeterProvider())\n+meter = metrics.get_meter(__name__)\n+\n+# Telemetry instruments\n+workflow_counter = meter.create_counter(\n+    name=\"spiff_workflows_total\",\n+    description=\"Total number of SpiffWorkflow instances\"\n+)\n+\n+task_duration_histogram = meter.create_histogram(\n+    name=\"spiff_task_duration_seconds\",\n+    description=\"Duration of SpiffWorkflow tasks\"\n+)\n+\n+# LinkML Schema for Workflow Telemetry\n+WORKFLOW_TELEMETRY_SCHEMA = \"\"\"\n+id: https://example.org/spiff-workflow-telemetry\n+name: spiff-workflow-telemetry\n+title: SpiffWorkflow Telemetry Schema\n+version: 1.0.0\n+\n+prefixes:\n+  linkml: https://w3id.org/linkml/\n+  spiff: https://example.org/spiff-workflow-telemetry/\n+  \n+default_prefix: spiff\n+default_range: string\n+\n+imports:\n+  - linkml:types\n+\n+classes:\n+  WorkflowInstance:\n+    description: A SpiffWorkflow instance with telemetry data\n+    attributes:\n+      instance_id:\n+        range: string\n+        required: true\n+        description: Unique identifier for the workflow instance\n+      process_id:\n+        range: string\n+        required: true\n+        description: BPMN process definition ID\n+      status:\n+        range: WorkflowStatus\n+        required: true\n+        description: Current status of the workflow\n+      start_time:\n+        range: datetime\n+        required: true\n+        description: When the workflow started\n+      end_time:\n+        range: datetime\n+        description: When the workflow completed\n+      total_tasks:\n+        range: integer\n+        description: Total number of tasks in the workflow\n+      completed_tasks:\n+        range: integer\n+        description: Number of completed tasks\n+      variables:\n+        range: string\n+        description: JSON string of workflow variables\n+      error_message:\n+        range: string\n+        description: Error message if workflow failed\n+        \n+  TaskExecution:\n+    description: Execution details for a single task\n+    attributes:\n+      task_id:\n+        range: string\n+        required: true\n+        description: Unique task identifier\n+      task_name:\n+        range: string\n+        required: true\n+        description: Human-readable task name\n+      task_type:\n+        range: string\n+        required: true\n+        description: Type of task (e.g., UserTask, ServiceTask)\n+      status:\n+        range: TaskStatus\n+        required: true\n+        description: Current task status\n+      start_time:\n+        range: datetime\n+        required: true\n+        description: When task execution started\n+      end_time:\n+        range: datetime\n+        description: When task execution completed\n+      duration_seconds:\n+        range: float\n+        description: Task execution duration\n+      input_data:\n+        range: string\n+        description: JSON string of input data\n+      output_data:\n+        range: string\n+        description: JSON string of output data\n+      error_message:\n+        range: string\n+        description: Error message if task failed\n+      workflow_instance_id:\n+        range: string\n+        required: true\n+        description: Reference to parent workflow instance\n+\n+enums:\n+  WorkflowStatus:\n+    permissible_values:\n+      pending:\n+        description: Workflow is pending execution\n+      running:\n+        description: Workflow is currently running\n+      completed:\n+        description: Workflow completed successfully\n+      failed:\n+        description: Workflow failed with error\n+      suspended:\n+        description: Workflow is suspended\n+      cancelled:\n+        description: Workflow was cancelled\n+        \n+  TaskStatus:\n+    permissible_values:\n+      pending:\n+        description: Task is pending execution\n+      ready:\n+        description: Task is ready to execute\n+      running:\n+        description: Task is currently running\n+      completed:\n+        description: Task completed successfully\n+      failed:\n+        description: Task failed with error\n+      cancelled:\n+        description: Task was cancelled\n+\"\"\"\n+\n+@dataclass\n+class WorkflowTelemetry:\n+    \"\"\"Structured telemetry data for SpiffWorkflow operations\"\"\"\n+    instance_id: str\n+    process_id: str\n+    status: str\n+    start_time: datetime\n+    end_time: Optional[datetime] = None\n+    total_tasks: int = 0\n+    completed_tasks: int = 0\n+    variables: str = \"\"\n+    error_message: Optional[str] = None\n+    \n+    def to_dict(self) -> Dict[str, Any]:\n+        \"\"\"Convert to dictionary for LinkML validation\"\"\"\n+        return {\n+            \"instance_id\": self.instance_id,\n+            \"process_id\": self.process_id,\n+            \"status\": self.status,\n+            \"start_time\": self.start_time.isoformat(),\n+            \"end_time\": self.end_time.isoformat() if self.end_time else None,\n+            \"total_tasks\": self.total_tasks,\n+            \"completed_tasks\": self.completed_tasks,\n+            \"variables\": self.variables,\n+            \"error_message\": self.error_message\n+        }\n+\n+@dataclass\n+class TaskTelemetry:\n+    \"\"\"Structured telemetry data for task execution\"\"\"\n+    task_id: str\n+    task_name: str\n+    task_type: str\n+    status: str\n+    start_time: datetime\n+    end_time: Optional[datetime] = None\n+    duration_seconds: Optional[float] = None\n+    input_data: str = \"\"\n+    output_data: str = \"\"\n+    error_message: Optional[str] = None\n+    workflow_instance_id: str = \"\"\n+    \n+    def to_dict(self) -> Dict[str, Any]:\n+        \"\"\"Convert to dictionary for LinkML validation\"\"\"\n+        return {\n+            \"task_id\": self.task_id,\n+            \"task_name\": self.task_name,\n+            \"task_type\": self.task_type,\n+            \"status\": self.status,\n+            \"start_time\": self.start_time.isoformat(),\n+            \"end_time\": self.end_time.isoformat() if self.end_time else None,\n+            \"duration_seconds\": self.duration_seconds,\n+            \"input_data\": self.input_data,\n+            \"output_data\": self.output_data,\n+            \"error_message\": self.error_message,\n+            \"workflow_instance_id\": self.workflow_instance_id\n+        }\n+\n+class SpiffOrchestrator:\n+    \"\"\"\n+    SpiffWorkflow Orchestrator with LinkML-validated OpenTelemetry\n+    \n+    This demonstrates the new paradigm where code agents validate their work\n+    by generating structured telemetry that conforms to LinkML schemas.\n+    \"\"\"\n+    \n+    def __init__(self, bpmn_files_path: str = \"bpmn\"):\n+        \"\"\"Initialize the orchestrator with LinkML schema validation\"\"\"\n+        self.tracer = trace.get_tracer(__name__)\n+        self.bpmn_files_path = Path(bpmn_files_path)\n+        \n+        # Initialize SpiffWorkflow parser\n+        self.parser = BpmnParser()\n+        self.parser.add_bpmn_files_by_glob(str(self.bpmn_files_path / \"*.bpmn\"))\n+        \n+        # Load LinkML schema for validation\n+        self.schema_view = None\n+        if LINKML_AVAILABLE:\n+            try:\n+                # Create temporary schema file\n+                schema_path = Path(\"workflow_telemetry_schema.yaml\")\n+                with open(schema_path, 'w') as f:\n+                    f.write(WORKFLOW_TELEMETRY_SCHEMA)\n+                \n+                self.schema_view = SchemaView(str(schema_path))\n+                logger.info(\"✅ LinkML schema loaded for telemetry validation\")\n+            except Exception as e:\n+                logger.warning(f\"⚠️ Failed to load LinkML schema: {e}\")\n+        \n+        # Process definitions cache\n+        self.process_definitions: Dict[str, BpmnProcessSpec] = {}\n+        self.active_workflows: Dict[str, BpmnWorkflow] = {}\n+        self.telemetry_data: List[WorkflowTelemetry] = []\n+        \n+        # Load process definitions\n+        self._load_process_definitions()\n+        \n+        logger.info(f\"SpiffOrchestrator initialized with {len(self.process_definitions)} process definitions\")\n+    \n+    def _load_process_definitions(self) -> None:\n+        \"\"\"Load BPMN process definitions with telemetry\"\"\"\n+        with self.tracer.start_as_current_span(\"load_process_definitions\") as span:\n+            try:\n+                for process_id, process_spec in self.parser.find_all_specs().items():\n+                    self.process_definitions[process_id] = process_spec\n+                    span.add_event(f\"Loaded process definition: {process_id}\")\n+                \n+                logger.info(f\"Loaded {len(self.process_definitions)} process definitions\")\n+                \n+            except Exception as e:\n+                span.set_status(Status(StatusCode.ERROR, str(e)))\n+                logger.error(f\"Failed to load process definitions: {e}\")\n+                raise\n+    \n+    def _validate_telemetry_with_linkml(self, telemetry_data: Dict[str, Any], class_name: str) -> bool:\n+        \"\"\"Validate telemetry data against LinkML schema\"\"\"\n+        if not LINKML_AVAILABLE or not self.schema_view:\n+            return True  # Skip validation if LinkML not available\n+        \n+        try:\n+            # Validate using LinkML loader\n+            if class_name == \"WorkflowInstance\":\n+                validated_obj = yaml_loader.loads(\n+                    yaml.dump(telemetry_data), \n+                    target_class=\"WorkflowInstance\", \n+                    schemaview=self.schema_view\n+                )\n+            elif class_name == \"TaskExecution\":\n+                validated_obj = yaml_loader.loads(\n+                    yaml.dump(telemetry_data), \n+                    target_class=\"TaskExecution\", \n+                    schemaview=self.schema_view\n+                )\n+            else:\n+                logger.warning(f\"Unknown class name for validation: {class_name}\")\n+                return False\n+            \n+            logger.debug(f\"✅ LinkML validation passed for {class_name}\")\n+            return True\n+            \n+        except Exception as e:\n+            logger.error(f\"❌ LinkML validation failed for {class_name}: {e}\")\n+            return False\n+    \n+    def start_workflow(self, process_id: str, variables: Optional[Dict[str, Any]] = None) -> str:\n+        \"\"\"\n+        Start a new workflow instance with validated telemetry\n+        \n+        This demonstrates the new paradigm: the code agent validates its work\n+        by generating structured telemetry that conforms to LinkML schemas.\n+        \"\"\"\n+        with self.tracer.start_as_current_span(\"start_workflow\") as span:\n+            span.set_attributes({\n+                \"process.id\": process_id,\n+                \"process.variables\": json.dumps(variables or {})\n+            })\n+            \n+            try:\n+                # Validate process exists\n+                if process_id not in self.process_definitions:\n+                    raise ValueError(f\"Process definition '{process_id}' not found\")\n+                \n+                # Generate instance ID\n+                instance_id = f\"{process_id}_{uuid.uuid4().hex[:8]}\"\n+                \n+                # Create workflow instance\n+                workflow = BpmnWorkflow(self.process_definitions[process_id])\n+                \n+                # Set initial variables\n+                if variables:\n+                    workflow.data.update(variables)\n+                \n+                # Store workflow\n+                self.active_workflows[instance_id] = workflow\n+                \n+                # Generate structured telemetry\n+                workflow_telemetry = WorkflowTelemetry(\n+                    instance_id=instance_id,\n+                    process_id=process_id,\n+                    status=\"running\",\n+                    start_time=datetime.utcnow(),\n+                    total_tasks=len(workflow.get_tasks()),\n+                    completed_tasks=0,\n+                    variables=json.dumps(variables or {})\n+                )\n+                \n+                # Validate telemetry with LinkML\n+                telemetry_dict = workflow_telemetry.to_dict()\n+                validation_passed = self._validate_telemetry_with_linkml(telemetry_dict, \"WorkflowInstance\")\n+                \n+                if validation_passed:\n+                    # Record validated telemetry\n+                    self.telemetry_data.append(workflow_telemetry)\n+                    \n+                    # Update OpenTelemetry metrics\n+                    workflow_counter.add(1, {\n+                        \"process.id\": process_id,\n+                        \"action\": \"start\",\n+                        \"linkml_validated\": \"true\"\n+                    })\n+                    \n+                    span.set_attributes({\n+                        \"instance.id\": instance_id,\n+                        \"linkml.validation\": \"passed\"\n+                    })\n+                    \n+                    logger.info(f\"✅ Started workflow {instance_id} with validated telemetry\")\n+                else:\n+                    span.set_attributes({\"linkml.validation\": \"failed\"})\n+                    logger.error(f\"❌ Telemetry validation failed for workflow {instance_id}\")\n+                \n+                return instance_id\n+                \n+            except Exception as e:\n+                span.set_status(Status(StatusCode.ERROR, str(e)))\n+                logger.error(f\"Failed to start workflow {process_id}: {e}\")\n+                raise\n+    \n+    def run_until_user_input_required(self, instance_id: str) -> List[Dict[str, Any]]:\n+        \"\"\"\n+        Run workflow until user input is required (Arena pattern)\n+        \n+        This demonstrates advanced orchestration with validated telemetry.\n+        \"\"\"\n+        with self.tracer.start_as_current_span(\"run_until_user_input\") as span:\n+            span.set_attributes({\"instance.id\": instance_id})\n+            \n+            try:\n+                if instance_id not in self.active_workflows:\n+                    raise ValueError(f\"Workflow instance '{instance_id}' not found\")\n+                \n+                workflow = self.active_workflows[instance_id]\n+                ready_tasks = []\n+                \n+                # Run automatic tasks (manual=False)\n+                task = workflow.get_next_task(state=TaskState.READY, manual=False)\n+                while task is not None:\n+                    # Execute task with telemetry\n+                    self._execute_task_with_telemetry(task, instance_id)\n+                    \n+                    # Refresh waiting tasks and handle events\n+                    self._run_ready_events(workflow, instance_id)\n+                    \n+                    # Get next automatic task\n+                    task = workflow.get_next_task(state=TaskState.READY, manual=False)\n+                \n+                # Get ready human tasks\n+                human_tasks = workflow.get_tasks(state=TaskState.READY, manual=True)\n+                for task in human_tasks:\n+                    ready_tasks.append({\n+                        \"task_id\": str(task.id),\n+                        \"task_name\": task.task_spec.name,\n+                        \"task_type\": task.task_spec.__class__.__name__,\n+                        \"data\": dict(task.data) if hasattr(task, 'data') else {}\n+                    })\n+                \n+                # Update workflow telemetry\n+                self._update_workflow_telemetry(instance_id, workflow)\n+                \n+                span.set_attributes({\n+                    \"ready_tasks_count\": len(ready_tasks),\n+                    \"workflow_completed\": workflow.is_completed()\n+                })\n+                \n+                return ready_tasks\n+                \n+            except Exception as e:\n+                span.set_status(Status(StatusCode.ERROR, str(e)))\n+                logger.error(f\"Failed to run workflow {instance_id}: {e}\")\n+                raise\n+    \n+    def _execute_task_with_telemetry(self, task: BpmnTaskSpec, instance_id: str) -> None:\n+        \"\"\"Execute a task with LinkML-validated telemetry\"\"\"\n+        with self.tracer.start_as_current_span(\"execute_task\") as span:\n+            task_start_time = datetime.utcnow()\n+            \n+            span.set_attributes({\n+                \"task.id\": str(task.id),\n+                \"task.name\": task.task_spec.name,\n+                \"task.type\": task.task_spec.__class__.__name__\n+            })\n+            \n+            try:\n+                # Create task telemetry\n+                task_telemetry = TaskTelemetry(\n+                    task_id=str(task.id),\n+                    task_name=task.task_spec.name,\n+                    task_type=task.task_spec.__class__.__name__,\n+                    status=\"running\",\n+                    start_time=task_start_time,\n+                    input_data=json.dumps(dict(task.data) if hasattr(task, 'data') else {}),\n+                    workflow_instance_id=instance_id\n+                )\n+                \n+                # Execute the task\n+                task.complete()\n+                \n+                # Update task telemetry\n+                task_telemetry.end_time = datetime.utcnow()\n+                task_telemetry.duration_seconds = (\n+                    task_telemetry.end_time - task_telemetry.start_time\n+                ).total_seconds()\n+                task_telemetry.status = \"completed\"\n+                task_telemetry.output_data = json.dumps(dict(task.data) if hasattr(task, 'data') else {})\n+                \n+                # Validate task telemetry with LinkML\n+                telemetry_dict = task_telemetry.to_dict()\n+                validation_passed = self._validate_telemetry_with_linkml(telemetry_dict, \"TaskExecution\")\n+                \n+                if validation_passed:\n+                    # Record task duration metric\n+                    task_duration_histogram.record(\n+                        task_telemetry.duration_seconds,\n+                        {\n+                            \"task.id\": str(task.id),\n+                            \"task.type\": task_telemetry.task_type,\n+                            \"linkml_validated\": \"true\"\n+                        }\n+                    )\n+                    \n+                    span.set_attributes({\n+                        \"task.duration\": task_telemetry.duration_seconds,\n+                        \"linkml.validation\": \"passed\"\n+                    })\n+                    \n+                    logger.debug(f\"✅ Executed task {task.id} with validated telemetry\")\n+                else:\n+                    span.set_attributes({\"linkml.validation\": \"failed\"})\n+                    logger.error(f\"❌ Task telemetry validation failed for {task.id}\")\n+                \n+            except Exception as e:\n+                # Record failed task telemetry\n+                task_telemetry.end_time = datetime.utcnow()\n+                task_telemetry.duration_seconds = (\n+                    task_telemetry.end_time - task_telemetry.start_time\n+                ).total_seconds()\n+                task_telemetry.status = \"failed\"\n+                task_telemetry.error_message = str(e)\n+                \n+                # Validate failed telemetry\n+                telemetry_dict = task_telemetry.to_dict()\n+                self._validate_telemetry_with_linkml(telemetry_dict, \"TaskExecution\")\n+                \n+                span.set_status(Status(StatusCode.ERROR, str(e)))\n+                logger.error(f\"Failed to execute task {task.id}: {e}\")\n+                raise\n+    \n+    def _run_ready_events(self, workflow: BpmnWorkflow, instance_id: str) -> None:\n+        \"\"\"Run ready events (Arena pattern)\"\"\"\n+        workflow.refresh_waiting_tasks()\n+        \n+        # Handle any ready tasks that might be events\n+        # Note: Simplified event handling for this example\n+        ready_tasks = workflow.get_tasks(state=TaskState.READY)\n+        for task in ready_tasks:\n+            if hasattr(task.task_spec, 'event_definition'):\n+                self._execute_task_with_telemetry(task, instance_id)\n+    \n+    def _update_workflow_telemetry(self, instance_id: str, workflow: BpmnWorkflow) -> None:\n+        \"\"\"Update workflow telemetry with current state\"\"\"\n+        # Find existing telemetry\n+        for telemetry in self.telemetry_data:\n+            if telemetry.instance_id == instance_id:\n+                telemetry.completed_tasks = len(workflow.get_tasks(state=TaskState.COMPLETED))\n+                \n+                if workflow.is_completed():\n+                    telemetry.status = \"completed\"\n+                    telemetry.end_time = datetime.utcnow()\n+                elif workflow.is_cancelled():\n+                    telemetry.status = \"cancelled\"\n+                    telemetry.end_time = datetime.utcnow()\n+                \n+                # Re-validate updated telemetry\n+                telemetry_dict = telemetry.to_dict()\n+                self._validate_telemetry_with_linkml(telemetry_dict, \"WorkflowInstance\")\n+                break\n+    \n+    def complete_task(self, instance_id: str, task_id: str, data: Optional[Dict[str, Any]] = None) -> None:\n+        \"\"\"Complete a human task with validated telemetry\"\"\"\n+        with self.tracer.start_as_current_span(\"complete_task\") as span:\n+            span.set_attributes({\n+                \"instance.id\": instance_id,\n+                \"task.id\": task_id\n+            })\n+            \n+            try:\n+                if instance_id not in self.active_workflows:\n+                    raise ValueError(f\"Workflow instance '{instance_id}' not found\")\n+                \n+                workflow = self.active_workflows[instance_id]\n+                \n+                # Find the task\n+                tasks = workflow.get_tasks()\n+                target_task = None\n+                \n+                for task in tasks:\n+                    if str(task.id) == task_id:\n+                        target_task = task\n+                        break\n+                \n+                if not target_task:\n+                    raise ValueError(f\"Task '{task_id}' not found\")\n+                \n+                # Set task data if provided\n+                if data and hasattr(target_task, 'data'):\n+                    target_task.data.update(data)\n+                \n+                # Execute task with telemetry\n+                self._execute_task_with_telemetry(target_task, instance_id)\n+                \n+                # Update workflow telemetry\n+                self._update_workflow_telemetry(instance_id, workflow)\n+                \n+                span.set_attributes({\"task.status\": \"completed\"})\n+                logger.info(f\"✅ Completed task {task_id} with validated telemetry\")\n+                \n+            except Exception as e:\n+                span.set_status(Status(StatusCode.ERROR, str(e)))\n+                logger.error(f\"Failed to complete task {task_id}: {e}\")\n+                raise\n+    \n+    def get_workflow_status(self, instance_id: str) -> Optional[Dict[str, Any]]:\n+        \"\"\"Get workflow status with telemetry validation\"\"\"\n+        if instance_id not in self.active_workflows:\n+            return None\n+        \n+        workflow = self.active_workflows[instance_id]\n+        \n+        # Find telemetry data\n+        for telemetry in self.telemetry_data:\n+            if telemetry.instance_id == instance_id:\n+                return {\n+                    \"instance_id\": instance_id,\n+                    \"status\": telemetry.status,\n+                    \"completed_tasks\": telemetry.completed_tasks,\n+                    \"total_tasks\": telemetry.total_tasks,\n+                    \"is_completed\": workflow.is_completed(),\n+                    \"linkml_validated\": True\n+                }\n+        \n+        return None\n+    \n+    def get_telemetry_summary(self) -> Dict[str, Any]:\n+        \"\"\"Get summary of all validated telemetry data\"\"\"\n+        return {\n+            \"total_workflows\": len(self.telemetry_data),\n+            \"workflow_statuses\": {\n+                status: len([w for w in self.telemetry_data if w.status == status])\n+                for status in [\"running\", \"completed\", \"failed\", \"cancelled\"]\n+            },\n+            \"linkml_validation_enabled\": LINKML_AVAILABLE and self.schema_view is not None,\n+            \"telemetry_data\": [w.to_dict() for w in self.telemetry_data]\n+        }\n+\n+# Example BPMN files for testing\n+def create_sample_bpmn_files():\n+    \"\"\"Create sample BPMN files for testing the orchestration examples\"\"\"\n+    \n+    # Simple process with automatic tasks\n+    simple_process = '''<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n+<bpmn:definitions xmlns:bpmn=\"http://www.omg.org/spec/BPMN/20100524/MODEL\"\n+                  xmlns:bpmndi=\"http://www.omg.org/spec/BPMN/20100524/DI\"\n+                  xmlns:dc=\"http://www.omg.org/spec/DD/20100524/DC\"\n+                  xmlns:di=\"http://www.omg.org/spec/DD/20100524/DI\"\n+                  id=\"Definitions_1\"\n+                  targetNamespace=\"http://bpmn.io/schema/bpmn\">\n+  <bpmn:process id=\"SimpleProcess\" isExecutable=\"true\">\n+    <bpmn:startEvent id=\"StartEvent_1\" name=\"Start\">\n+      <bpmn:outgoing>Flow_1</bpmn:outgoing>\n+    </bpmn:startEvent>\n+    <bpmn:task id=\"Task_1\" name=\"Automatic Task\">\n+      <bpmn:incoming>Flow_1</bpmn:incoming>\n+      <bpmn:outgoing>Flow_2</bpmn:outgoing>\n+    </bpmn:task>\n+    <bpmn:endEvent id=\"EndEvent_1\" name=\"End\">\n+      <bpmn:incoming>Flow_2</bpmn:incoming>\n+    </bpmn:endEvent>\n+    <bpmn:sequenceFlow id=\"Flow_1\" sourceRef=\"StartEvent_1\" targetRef=\"Task_1\" />\n+    <bpmn:sequenceFlow id=\"Flow_2\" sourceRef=\"Task_1\" targetRef=\"EndEvent_1\" />\n+  </bpmn:process>\n+  <bpmndi:BPMNDiagram id=\"BPMNDiagram_1\">\n+    <bpmndi:BPMNPlane id=\"BPMNPlane_1\" bpmnElement=\"SimpleProcess\">\n+      <bpmndi:BPMNShape id=\"StartEvent_1_di\" bpmnElement=\"StartEvent_1\">\n+        <dc:Bounds x=\"152\" y=\"102\" width=\"36\" height=\"36\" />\n+      </bpmndi:BPMNShape>\n+      <bpmndi:BPMNShape id=\"Task_1_di\" bpmnElement=\"Task_1\">\n+        <dc:Bounds x=\"240\" y=\"80\" width=\"100\" height=\"80\" />\n+      </bpmndi:BPMNShape>\n+      <bpmndi:BPMNShape id=\"EndEvent_1_di\" bpmnElement=\"EndEvent_1\">\n+        <dc:Bounds x=\"392\" y=\"102\" width=\"36\" height=\"36\" />\n+      </bpmndi:BPMNShape>\n+      <bpmndi:BPMNEdge id=\"Flow_1_di\" bpmnElement=\"Flow_1\">\n+        <di:waypoint x=\"188\" y=\"120\" />\n+        <di:waypoint x=\"240\" y=\"120\" />\n+      </bpmndi:BPMNEdge>\n+      <bpmndi:BPMNEdge id=\"Flow_2_di\" bpmnElement=\"Flow_2\">\n+        <di:waypoint x=\"340\" y=\"120\" />\n+        <di:waypoint x=\"392\" y=\"120\" />\n+      </bpmndi:BPMNEdge>\n+    </bpmndi:BPMNPlane>\n+  </bpmndi:BPMNDiagram>\n+</bpmn:definitions>'''\n+    \n+    # Process with human task\n+    human_task_process = '''<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n+<bpmn:definitions xmlns:bpmn=\"http://www.omg.org/spec/BPMN/20100524/MODEL\"\n+                  xmlns:bpmndi=\"http://www.omg.org/spec/BPMN/20100524/DI\"\n+                  xmlns:dc=\"http://www.omg.org/spec/DD/20100524/DC\"\n+                  xmlns:di=\"http://www.omg.org/spec/DD/20100524/DI\"\n+                  id=\"Definitions_2\"\n+                  targetNamespace=\"http://bpmn.io/schema/bpmn\">\n+  <bpmn:process id=\"HumanTaskProcess\" isExecutable=\"true\">\n+    <bpmn:startEvent id=\"StartEvent_1\" name=\"Start\">\n+      <bpmn:outgoing>Flow_1</bpmn:outgoing>\n+    </bpmn:startEvent>\n+    <bpmn:userTask id=\"UserTask_1\" name=\"Human Approval\">\n+      <bpmn:incoming>Flow_1</bpmn:incoming>\n+      <bpmn:outgoing>Flow_2</bpmn:outgoing>\n+    </bpmn:userTask>\n+    <bpmn:endEvent id=\"EndEvent_1\" name=\"End\">\n+      <bpmn:incoming>Flow_2</bpmn:incoming>\n+    </bpmn:endEvent>\n+    <bpmn:sequenceFlow id=\"Flow_1\" sourceRef=\"StartEvent_1\" targetRef=\"UserTask_1\" />\n+    <bpmn:sequenceFlow id=\"Flow_2\" sourceRef=\"UserTask_1\" targetRef=\"EndEvent_1\" />\n+  </bpmn:process>\n+  <bpmndi:BPMNDiagram id=\"BPMNDiagram_2\">\n+    <bpmndi:BPMNPlane id=\"BPMNPlane_2\" bpmnElement=\"HumanTaskProcess\">\n+      <bpmndi:BPMNShape id=\"StartEvent_1_di\" bpmnElement=\"StartEvent_1\">\n+        <dc:Bounds x=\"152\" y=\"102\" width=\"36\" height=\"36\" />\n+      </bpmndi:BPMNShape>\n+      <bpmndi:BPMNShape id=\"UserTask_1_di\" bpmnElement=\"UserTask_1\">\n+        <dc:Bounds x=\"240\" y=\"80\" width=\"100\" height=\"80\" />\n+      </bpmndi:BPMNShape>\n+      <bpmndi:BPMNShape id=\"EndEvent_1_di\" bpmnElement=\"EndEvent_1\">\n+        <dc:Bounds x=\"392\" y=\"102\" width=\"36\" height=\"36\" />\n+      </bpmndi:BPMNShape>\n+      <bpmndi:BPMNEdge id=\"Flow_1_di\" bpmnElement=\"Flow_1\">\n+        <di:waypoint x=\"188\" y=\"120\" />\n+        <di:waypoint x=\"240\" y=\"120\" />\n+      </bpmndi:BPMNEdge>\n+      <bpmndi:BPMNEdge id=\"Flow_2_di\" bpmnElement=\"Flow_2\">\n+        <di:waypoint x=\"340\" y=\"120\" />\n+        <di:waypoint x=\"392\" y=\"120\" />\n+      </bpmndi:BPMNEdge>\n+    </bpmndi:BPMNPlane>\n+  </bpmndi:BPMNDiagram>\n+</bpmn:definitions>'''\n+    \n+    # Create BPMN directory and files\n+    bpmn_path = Path(\"bpmn\")\n+    bpmn_path.mkdir(exist_ok=True)\n+    \n+    with open(bpmn_path / \"simple_process.bpmn\", \"w\") as f:\n+        f.write(simple_process)\n+    \n+    with open(bpmn_path / \"human_task_process.bpmn\", \"w\") as f:\n+        f.write(human_task_process)\n+    \n+    print(\"✅ Created sample BPMN files for testing\")\n+\n+def demonstrate_orchestration_patterns():\n+    \"\"\"Demonstrate the new orchestration patterns with LinkML validation\"\"\"\n+    print(\"🚀 SpiffWorkflow Orchestration Examples - LinkML + OpenTelemetry Validation\")\n+    print(\"=\" * 80)\n+    \n+    # Create sample BPMN files\n+    create_sample_bpmn_files()\n+    \n+    # Initialize orchestrator\n+    orchestrator = SpiffOrchestrator()\n+    \n+    print(f\"\\n📊 Orchestrator initialized with {len(orchestrator.process_definitions)} process definitions\")\n+    print(f\"🔍 LinkML validation: {'✅ Enabled' if LINKML_AVAILABLE else '❌ Disabled'}\")\n+    \n+    # Example 1: Simple automatic workflow\n+    print(f\"\\n🧪 Example 1: Simple Automatic Workflow\")\n+    print(\"-\" * 50)\n+    \n+    instance_id = orchestrator.start_workflow(\"SimpleProcess\", {\"input\": \"test_data\"})\n+    print(f\"✅ Started workflow: {instance_id}\")\n+    \n+    ready_tasks = orchestrator.run_until_user_input_required(instance_id)\n+    print(f\"📋 Ready tasks after execution: {len(ready_tasks)}\")\n+    \n+    status = orchestrator.get_workflow_status(instance_id)\n+    print(f\"📊 Workflow status: {status}\")\n+    \n+    # Example 2: Human task workflow\n+    print(f\"\\n🧪 Example 2: Human Task Workflow\")\n+    print(\"-\" * 50)\n+    \n+    instance_id_2 = orchestrator.start_workflow(\"HumanTaskProcess\", {\"approval_required\": True})\n+    print(f\"✅ Started workflow: {instance_id_2}\")\n+    \n+    ready_tasks = orchestrator.run_until_user_input_required(instance_id_2)\n+    print(f\"📋 Ready human tasks: {len(ready_tasks)}\")\n+    \n+    if ready_tasks:\n+        task = ready_tasks[0]\n+        print(f\"👤 Human task: {task['task_name']} (ID: {task['task_id']})\")\n+        \n+        # Complete the human task\n+        orchestrator.complete_task(instance_id_2, task['task_id'], {\"approved\": True})\n+        print(f\"✅ Completed human task\")\n+        \n+        # Continue workflow\n+        ready_tasks = orchestrator.run_until_user_input_required(instance_id_2)\n+        print(f\"📋 Remaining tasks: {len(ready_tasks)}\")\n+    \n+    status = orchestrator.get_workflow_status(instance_id_2)\n+    print(f\"📊 Workflow status: {status}\")\n+    \n+    # Example 3: Telemetry validation summary\n+    print(f\"\\n🧪 Example 3: Telemetry Validation Summary\")\n+    print(\"-\" * 50)\n+    \n+    summary = orchestrator.get_telemetry_summary()\n+    print(f\"📈 Total workflows: {summary['total_workflows']}\")\n+    print(f\"📊 Status distribution: {summary['workflow_statuses']}\")\n+    print(f\"🔍 LinkML validation: {'✅ Enabled' if summary['linkml_validation_enabled'] else '❌ Disabled'}\")\n+    \n+    # Example 4: Demonstrate the new paradigm\n+    print(f\"\\n🧪 Example 4: New Paradigm - Code Agent Validation\")\n+    print(\"-\" * 50)\n+    print(\"\"\"\n+🎯 NEW PARADIGM: Code agents validate their work by generating \n+   structured telemetry that conforms to LinkML schemas.\n+\n+✅ BENEFITS:\n+   - Structured, validated observability\n+   - Schema-driven telemetry validation\n+   - Consistent data formats across systems\n+   - Automated validation of agent outputs\n+   - Weaver-like integration but with formal schemas\n+\n+🔧 IMPLEMENTATION:\n+   - LinkML schemas define telemetry structure\n+   - OpenTelemetry provides the transport\n+   - Code agents generate validated telemetry\n+   - Validation ensures data quality and consistency\n+    \"\"\")\n+    \n+    return orchestrator\n+\n+if __name__ == \"__main__\":\n+    orchestrator = demonstrate_orchestration_patterns()\n+    \n+    print(f\"\\n🎉 Orchestration examples completed successfully!\")\n+    print(f\"📁 Check the 'bpmn/' directory for sample BPMN files\")\n+    print(f\"📊 Review the console output for OpenTelemetry traces\")\n+    print(f\"🔍 LinkML validation ensures telemetry data quality\") \n\\ No newline at end of file\n"
                },
                {
                    "date": 1752192426333,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -841,851 +841,5 @@\n     \n     print(f\"\\n🎉 Orchestration examples completed successfully!\")\n     print(f\"📁 Check the 'bpmn/' directory for sample BPMN files\")\n     print(f\"📊 Review the console output for OpenTelemetry traces\")\n-    print(f\"🔍 LinkML validation ensures telemetry data quality\") \n-#!/usr/bin/env python3\n-\"\"\"\n-SpiffWorkflow Orchestration Examples - LinkML + OpenTelemetry Validation Paradigm\n-Demonstrates how code agents validate their work by generating structured telemetry\n-\"\"\"\n-\n-import json\n-import uuid\n-import logging\n-from datetime import datetime, timedelta\n-from typing import Dict, List, Any, Optional, Union\n-from dataclasses import dataclass, field\n-from pathlib import Path\n-import yaml\n-\n-# SpiffWorkflow imports\n-from SpiffWorkflow.bpmn import BpmnWorkflow\n-from SpiffWorkflow.bpmn.parser.BpmnParser import BpmnParser\n-from SpiffWorkflow.bpmn.specs.bpmn_task_spec import BpmnTaskSpec\n-from SpiffWorkflow.bpmn.specs.bpmn_process_spec import BpmnProcessSpec\n-from SpiffWorkflow.util.task import TaskState\n-from SpiffWorkflow.bpmn.serializer import BpmnWorkflowSerializer\n-from SpiffWorkflow.bpmn.serializer.config import DEFAULT_CONFIG\n-\n-# OpenTelemetry for structured telemetry\n-from opentelemetry import trace, metrics\n-from opentelemetry.trace import Status, StatusCode\n-from opentelemetry.metrics import Counter, Histogram\n-from opentelemetry.sdk.trace import TracerProvider\n-from opentelemetry.sdk.trace.export import ConsoleSpanExporter, BatchSpanProcessor\n-from opentelemetry.sdk.metrics import MeterProvider\n-\n-# LinkML for schema validation\n-try:\n-    from linkml_runtime.utils.schemaview import SchemaView\n-    from linkml_runtime.loaders import yaml_loader, json_loader\n-    from linkml_runtime.dumpers import yaml_dumper, json_dumper\n-    LINKML_AVAILABLE = True\n-except ImportError:\n-    LINKML_AVAILABLE = False\n-    print(\"⚠️ LinkML not available - telemetry validation disabled\")\n-\n-# Configure logging\n-logging.basicConfig(level=logging.INFO)\n-logger = logging.getLogger(__name__)\n-\n-# Configure OpenTelemetry\n-trace.set_tracer_provider(TracerProvider())\n-trace.get_tracer_provider().add_span_processor(\n-    BatchSpanProcessor(ConsoleSpanExporter())\n-)\n-\n-metrics.set_meter_provider(MeterProvider())\n-meter = metrics.get_meter(__name__)\n-\n-# Telemetry instruments\n-workflow_counter = meter.create_counter(\n-    name=\"spiff_workflows_total\",\n-    description=\"Total number of SpiffWorkflow instances\"\n-)\n-\n-task_duration_histogram = meter.create_histogram(\n-    name=\"spiff_task_duration_seconds\",\n-    description=\"Duration of SpiffWorkflow tasks\"\n-)\n-\n-# LinkML Schema for Workflow Telemetry\n-WORKFLOW_TELEMETRY_SCHEMA = \"\"\"\n-id: https://example.org/spiff-workflow-telemetry\n-name: spiff-workflow-telemetry\n-title: SpiffWorkflow Telemetry Schema\n-version: 1.0.0\n-\n-prefixes:\n-  linkml: https://w3id.org/linkml/\n-  spiff: https://example.org/spiff-workflow-telemetry/\n-  \n-default_prefix: spiff\n-default_range: string\n-\n-imports:\n-  - linkml:types\n-\n-classes:\n-  WorkflowInstance:\n-    description: A SpiffWorkflow instance with telemetry data\n-    attributes:\n-      instance_id:\n-        range: string\n-        required: true\n-        description: Unique identifier for the workflow instance\n-      process_id:\n-        range: string\n-        required: true\n-        description: BPMN process definition ID\n-      status:\n-        range: WorkflowStatus\n-        required: true\n-        description: Current status of the workflow\n-      start_time:\n-        range: datetime\n-        required: true\n-        description: When the workflow started\n-      end_time:\n-        range: datetime\n-        description: When the workflow completed\n-      total_tasks:\n-        range: integer\n-        description: Total number of tasks in the workflow\n-      completed_tasks:\n-        range: integer\n-        description: Number of completed tasks\n-      variables:\n-        range: string\n-        description: JSON string of workflow variables\n-      error_message:\n-        range: string\n-        description: Error message if workflow failed\n-        \n-  TaskExecution:\n-    description: Execution details for a single task\n-    attributes:\n-      task_id:\n-        range: string\n-        required: true\n-        description: Unique task identifier\n-      task_name:\n-        range: string\n-        required: true\n-        description: Human-readable task name\n-      task_type:\n-        range: string\n-        required: true\n-        description: Type of task (e.g., UserTask, ServiceTask)\n-      status:\n-        range: TaskStatus\n-        required: true\n-        description: Current task status\n-      start_time:\n-        range: datetime\n-        required: true\n-        description: When task execution started\n-      end_time:\n-        range: datetime\n-        description: When task execution completed\n-      duration_seconds:\n-        range: float\n-        description: Task execution duration\n-      input_data:\n-        range: string\n-        description: JSON string of input data\n-      output_data:\n-        range: string\n-        description: JSON string of output data\n-      error_message:\n-        range: string\n-        description: Error message if task failed\n-      workflow_instance_id:\n-        range: string\n-        required: true\n-        description: Reference to parent workflow instance\n-\n-enums:\n-  WorkflowStatus:\n-    permissible_values:\n-      pending:\n-        description: Workflow is pending execution\n-      running:\n-        description: Workflow is currently running\n-      completed:\n-        description: Workflow completed successfully\n-      failed:\n-        description: Workflow failed with error\n-      suspended:\n-        description: Workflow is suspended\n-      cancelled:\n-        description: Workflow was cancelled\n-        \n-  TaskStatus:\n-    permissible_values:\n-      pending:\n-        description: Task is pending execution\n-      ready:\n-        description: Task is ready to execute\n-      running:\n-        description: Task is currently running\n-      completed:\n-        description: Task completed successfully\n-      failed:\n-        description: Task failed with error\n-      cancelled:\n-        description: Task was cancelled\n-\"\"\"\n-\n-@dataclass\n-class WorkflowTelemetry:\n-    \"\"\"Structured telemetry data for SpiffWorkflow operations\"\"\"\n-    instance_id: str\n-    process_id: str\n-    status: str\n-    start_time: datetime\n-    end_time: Optional[datetime] = None\n-    total_tasks: int = 0\n-    completed_tasks: int = 0\n-    variables: str = \"\"\n-    error_message: Optional[str] = None\n-    \n-    def to_dict(self) -> Dict[str, Any]:\n-        \"\"\"Convert to dictionary for LinkML validation\"\"\"\n-        return {\n-            \"instance_id\": self.instance_id,\n-            \"process_id\": self.process_id,\n-            \"status\": self.status,\n-            \"start_time\": self.start_time.isoformat(),\n-            \"end_time\": self.end_time.isoformat() if self.end_time else None,\n-            \"total_tasks\": self.total_tasks,\n-            \"completed_tasks\": self.completed_tasks,\n-            \"variables\": self.variables,\n-            \"error_message\": self.error_message\n-        }\n-\n-@dataclass\n-class TaskTelemetry:\n-    \"\"\"Structured telemetry data for task execution\"\"\"\n-    task_id: str\n-    task_name: str\n-    task_type: str\n-    status: str\n-    start_time: datetime\n-    end_time: Optional[datetime] = None\n-    duration_seconds: Optional[float] = None\n-    input_data: str = \"\"\n-    output_data: str = \"\"\n-    error_message: Optional[str] = None\n-    workflow_instance_id: str = \"\"\n-    \n-    def to_dict(self) -> Dict[str, Any]:\n-        \"\"\"Convert to dictionary for LinkML validation\"\"\"\n-        return {\n-            \"task_id\": self.task_id,\n-            \"task_name\": self.task_name,\n-            \"task_type\": self.task_type,\n-            \"status\": self.status,\n-            \"start_time\": self.start_time.isoformat(),\n-            \"end_time\": self.end_time.isoformat() if self.end_time else None,\n-            \"duration_seconds\": self.duration_seconds,\n-            \"input_data\": self.input_data,\n-            \"output_data\": self.output_data,\n-            \"error_message\": self.error_message,\n-            \"workflow_instance_id\": self.workflow_instance_id\n-        }\n-\n-class SpiffOrchestrator:\n-    \"\"\"\n-    SpiffWorkflow Orchestrator with LinkML-validated OpenTelemetry\n-    \n-    This demonstrates the new paradigm where code agents validate their work\n-    by generating structured telemetry that conforms to LinkML schemas.\n-    \"\"\"\n-    \n-    def __init__(self, bpmn_files_path: str = \"bpmn\"):\n-        \"\"\"Initialize the orchestrator with LinkML schema validation\"\"\"\n-        self.tracer = trace.get_tracer(__name__)\n-        self.bpmn_files_path = Path(bpmn_files_path)\n-        \n-        # Initialize SpiffWorkflow parser\n-        self.parser = BpmnParser()\n-        self.parser.add_bpmn_files_by_glob(str(self.bpmn_files_path / \"*.bpmn\"))\n-        \n-        # Load LinkML schema for validation\n-        self.schema_view = None\n-        if LINKML_AVAILABLE:\n-            try:\n-                # Create temporary schema file\n-                schema_path = Path(\"workflow_telemetry_schema.yaml\")\n-                with open(schema_path, 'w') as f:\n-                    f.write(WORKFLOW_TELEMETRY_SCHEMA)\n-                \n-                self.schema_view = SchemaView(str(schema_path))\n-                logger.info(\"✅ LinkML schema loaded for telemetry validation\")\n-            except Exception as e:\n-                logger.warning(f\"⚠️ Failed to load LinkML schema: {e}\")\n-        \n-        # Process definitions cache\n-        self.process_definitions: Dict[str, BpmnProcessSpec] = {}\n-        self.active_workflows: Dict[str, BpmnWorkflow] = {}\n-        self.telemetry_data: List[WorkflowTelemetry] = []\n-        \n-        # Load process definitions\n-        self._load_process_definitions()\n-        \n-        logger.info(f\"SpiffOrchestrator initialized with {len(self.process_definitions)} process definitions\")\n-    \n-    def _load_process_definitions(self) -> None:\n-        \"\"\"Load BPMN process definitions with telemetry\"\"\"\n-        with self.tracer.start_as_current_span(\"load_process_definitions\") as span:\n-            try:\n-                for process_id, process_spec in self.parser.find_all_specs().items():\n-                    self.process_definitions[process_id] = process_spec\n-                    span.add_event(f\"Loaded process definition: {process_id}\")\n-                \n-                logger.info(f\"Loaded {len(self.process_definitions)} process definitions\")\n-                \n-            except Exception as e:\n-                span.set_status(Status(StatusCode.ERROR, str(e)))\n-                logger.error(f\"Failed to load process definitions: {e}\")\n-                raise\n-    \n-    def _validate_telemetry_with_linkml(self, telemetry_data: Dict[str, Any], class_name: str) -> bool:\n-        \"\"\"Validate telemetry data against LinkML schema\"\"\"\n-        if not LINKML_AVAILABLE or not self.schema_view:\n-            return True  # Skip validation if LinkML not available\n-        \n-        try:\n-            # Validate using LinkML loader\n-            if class_name == \"WorkflowInstance\":\n-                validated_obj = yaml_loader.loads(\n-                    yaml.dump(telemetry_data), \n-                    target_class=\"WorkflowInstance\", \n-                    schemaview=self.schema_view\n-                )\n-            elif class_name == \"TaskExecution\":\n-                validated_obj = yaml_loader.loads(\n-                    yaml.dump(telemetry_data), \n-                    target_class=\"TaskExecution\", \n-                    schemaview=self.schema_view\n-                )\n-            else:\n-                logger.warning(f\"Unknown class name for validation: {class_name}\")\n-                return False\n-            \n-            logger.debug(f\"✅ LinkML validation passed for {class_name}\")\n-            return True\n-            \n-        except Exception as e:\n-            logger.error(f\"❌ LinkML validation failed for {class_name}: {e}\")\n-            return False\n-    \n-    def start_workflow(self, process_id: str, variables: Optional[Dict[str, Any]] = None) -> str:\n-        \"\"\"\n-        Start a new workflow instance with validated telemetry\n-        \n-        This demonstrates the new paradigm: the code agent validates its work\n-        by generating structured telemetry that conforms to LinkML schemas.\n-        \"\"\"\n-        with self.tracer.start_as_current_span(\"start_workflow\") as span:\n-            span.set_attributes({\n-                \"process.id\": process_id,\n-                \"process.variables\": json.dumps(variables or {})\n-            })\n-            \n-            try:\n-                # Validate process exists\n-                if process_id not in self.process_definitions:\n-                    raise ValueError(f\"Process definition '{process_id}' not found\")\n-                \n-                # Generate instance ID\n-                instance_id = f\"{process_id}_{uuid.uuid4().hex[:8]}\"\n-                \n-                # Create workflow instance\n-                workflow = BpmnWorkflow(self.process_definitions[process_id])\n-                \n-                # Set initial variables\n-                if variables:\n-                    workflow.data.update(variables)\n-                \n-                # Store workflow\n-                self.active_workflows[instance_id] = workflow\n-                \n-                # Generate structured telemetry\n-                workflow_telemetry = WorkflowTelemetry(\n-                    instance_id=instance_id,\n-                    process_id=process_id,\n-                    status=\"running\",\n-                    start_time=datetime.utcnow(),\n-                    total_tasks=len(workflow.get_tasks()),\n-                    completed_tasks=0,\n-                    variables=json.dumps(variables or {})\n-                )\n-                \n-                # Validate telemetry with LinkML\n-                telemetry_dict = workflow_telemetry.to_dict()\n-                validation_passed = self._validate_telemetry_with_linkml(telemetry_dict, \"WorkflowInstance\")\n-                \n-                if validation_passed:\n-                    # Record validated telemetry\n-                    self.telemetry_data.append(workflow_telemetry)\n-                    \n-                    # Update OpenTelemetry metrics\n-                    workflow_counter.add(1, {\n-                        \"process.id\": process_id,\n-                        \"action\": \"start\",\n-                        \"linkml_validated\": \"true\"\n-                    })\n-                    \n-                    span.set_attributes({\n-                        \"instance.id\": instance_id,\n-                        \"linkml.validation\": \"passed\"\n-                    })\n-                    \n-                    logger.info(f\"✅ Started workflow {instance_id} with validated telemetry\")\n-                else:\n-                    span.set_attributes({\"linkml.validation\": \"failed\"})\n-                    logger.error(f\"❌ Telemetry validation failed for workflow {instance_id}\")\n-                \n-                return instance_id\n-                \n-            except Exception as e:\n-                span.set_status(Status(StatusCode.ERROR, str(e)))\n-                logger.error(f\"Failed to start workflow {process_id}: {e}\")\n-                raise\n-    \n-    def run_until_user_input_required(self, instance_id: str) -> List[Dict[str, Any]]:\n-        \"\"\"\n-        Run workflow until user input is required (Arena pattern)\n-        \n-        This demonstrates advanced orchestration with validated telemetry.\n-        \"\"\"\n-        with self.tracer.start_as_current_span(\"run_until_user_input\") as span:\n-            span.set_attributes({\"instance.id\": instance_id})\n-            \n-            try:\n-                if instance_id not in self.active_workflows:\n-                    raise ValueError(f\"Workflow instance '{instance_id}' not found\")\n-                \n-                workflow = self.active_workflows[instance_id]\n-                ready_tasks = []\n-                \n-                # Run automatic tasks (manual=False)\n-                task = workflow.get_next_task(state=TaskState.READY, manual=False)\n-                while task is not None:\n-                    # Execute task with telemetry\n-                    self._execute_task_with_telemetry(task, instance_id)\n-                    \n-                    # Refresh waiting tasks and handle events\n-                    self._run_ready_events(workflow, instance_id)\n-                    \n-                    # Get next automatic task\n-                    task = workflow.get_next_task(state=TaskState.READY, manual=False)\n-                \n-                # Get ready human tasks\n-                human_tasks = workflow.get_tasks(state=TaskState.READY, manual=True)\n-                for task in human_tasks:\n-                    ready_tasks.append({\n-                        \"task_id\": str(task.id),\n-                        \"task_name\": task.task_spec.name,\n-                        \"task_type\": task.task_spec.__class__.__name__,\n-                        \"data\": dict(task.data) if hasattr(task, 'data') else {}\n-                    })\n-                \n-                # Update workflow telemetry\n-                self._update_workflow_telemetry(instance_id, workflow)\n-                \n-                span.set_attributes({\n-                    \"ready_tasks_count\": len(ready_tasks),\n-                    \"workflow_completed\": workflow.is_completed()\n-                })\n-                \n-                return ready_tasks\n-                \n-            except Exception as e:\n-                span.set_status(Status(StatusCode.ERROR, str(e)))\n-                logger.error(f\"Failed to run workflow {instance_id}: {e}\")\n-                raise\n-    \n-    def _execute_task_with_telemetry(self, task: BpmnTaskSpec, instance_id: str) -> None:\n-        \"\"\"Execute a task with LinkML-validated telemetry\"\"\"\n-        with self.tracer.start_as_current_span(\"execute_task\") as span:\n-            task_start_time = datetime.utcnow()\n-            \n-            span.set_attributes({\n-                \"task.id\": str(task.id),\n-                \"task.name\": task.task_spec.name,\n-                \"task.type\": task.task_spec.__class__.__name__\n-            })\n-            \n-            try:\n-                # Create task telemetry\n-                task_telemetry = TaskTelemetry(\n-                    task_id=str(task.id),\n-                    task_name=task.task_spec.name,\n-                    task_type=task.task_spec.__class__.__name__,\n-                    status=\"running\",\n-                    start_time=task_start_time,\n-                    input_data=json.dumps(dict(task.data) if hasattr(task, 'data') else {}),\n-                    workflow_instance_id=instance_id\n-                )\n-                \n-                # Execute the task\n-                task.complete()\n-                \n-                # Update task telemetry\n-                task_telemetry.end_time = datetime.utcnow()\n-                task_telemetry.duration_seconds = (\n-                    task_telemetry.end_time - task_telemetry.start_time\n-                ).total_seconds()\n-                task_telemetry.status = \"completed\"\n-                task_telemetry.output_data = json.dumps(dict(task.data) if hasattr(task, 'data') else {})\n-                \n-                # Validate task telemetry with LinkML\n-                telemetry_dict = task_telemetry.to_dict()\n-                validation_passed = self._validate_telemetry_with_linkml(telemetry_dict, \"TaskExecution\")\n-                \n-                if validation_passed:\n-                    # Record task duration metric\n-                    task_duration_histogram.record(\n-                        task_telemetry.duration_seconds,\n-                        {\n-                            \"task.id\": str(task.id),\n-                            \"task.type\": task_telemetry.task_type,\n-                            \"linkml_validated\": \"true\"\n-                        }\n-                    )\n-                    \n-                    span.set_attributes({\n-                        \"task.duration\": task_telemetry.duration_seconds,\n-                        \"linkml.validation\": \"passed\"\n-                    })\n-                    \n-                    logger.debug(f\"✅ Executed task {task.id} with validated telemetry\")\n-                else:\n-                    span.set_attributes({\"linkml.validation\": \"failed\"})\n-                    logger.error(f\"❌ Task telemetry validation failed for {task.id}\")\n-                \n-            except Exception as e:\n-                # Record failed task telemetry\n-                task_telemetry.end_time = datetime.utcnow()\n-                task_telemetry.duration_seconds = (\n-                    task_telemetry.end_time - task_telemetry.start_time\n-                ).total_seconds()\n-                task_telemetry.status = \"failed\"\n-                task_telemetry.error_message = str(e)\n-                \n-                # Validate failed telemetry\n-                telemetry_dict = task_telemetry.to_dict()\n-                self._validate_telemetry_with_linkml(telemetry_dict, \"TaskExecution\")\n-                \n-                span.set_status(Status(StatusCode.ERROR, str(e)))\n-                logger.error(f\"Failed to execute task {task.id}: {e}\")\n-                raise\n-    \n-    def _run_ready_events(self, workflow: BpmnWorkflow, instance_id: str) -> None:\n-        \"\"\"Run ready events (Arena pattern)\"\"\"\n-        workflow.refresh_waiting_tasks()\n-        \n-        # Handle catching events\n-        from SpiffWorkflow.bpmn.specs.event_definitions.base import CatchingEvent\n-        \n-        task = workflow.get_next_task(state=TaskState.READY, spec_class=CatchingEvent)\n-        while task is not None:\n-            self._execute_task_with_telemetry(task, instance_id)\n-            task = workflow.get_next_task(state=TaskState.READY, spec_class=CatchingEvent)\n-    \n-    def _update_workflow_telemetry(self, instance_id: str, workflow: BpmnWorkflow) -> None:\n-        \"\"\"Update workflow telemetry with current state\"\"\"\n-        # Find existing telemetry\n-        for telemetry in self.telemetry_data:\n-            if telemetry.instance_id == instance_id:\n-                telemetry.completed_tasks = len(workflow.get_tasks(state=TaskState.COMPLETED))\n-                \n-                if workflow.is_completed():\n-                    telemetry.status = \"completed\"\n-                    telemetry.end_time = datetime.utcnow()\n-                elif workflow.is_cancelled():\n-                    telemetry.status = \"cancelled\"\n-                    telemetry.end_time = datetime.utcnow()\n-                \n-                # Re-validate updated telemetry\n-                telemetry_dict = telemetry.to_dict()\n-                self._validate_telemetry_with_linkml(telemetry_dict, \"WorkflowInstance\")\n-                break\n-    \n-    def complete_task(self, instance_id: str, task_id: str, data: Optional[Dict[str, Any]] = None) -> None:\n-        \"\"\"Complete a human task with validated telemetry\"\"\"\n-        with self.tracer.start_as_current_span(\"complete_task\") as span:\n-            span.set_attributes({\n-                \"instance.id\": instance_id,\n-                \"task.id\": task_id\n-            })\n-            \n-            try:\n-                if instance_id not in self.active_workflows:\n-                    raise ValueError(f\"Workflow instance '{instance_id}' not found\")\n-                \n-                workflow = self.active_workflows[instance_id]\n-                \n-                # Find the task\n-                tasks = workflow.get_tasks()\n-                target_task = None\n-                \n-                for task in tasks:\n-                    if str(task.id) == task_id:\n-                        target_task = task\n-                        break\n-                \n-                if not target_task:\n-                    raise ValueError(f\"Task '{task_id}' not found\")\n-                \n-                # Set task data if provided\n-                if data and hasattr(target_task, 'data'):\n-                    target_task.data.update(data)\n-                \n-                # Execute task with telemetry\n-                self._execute_task_with_telemetry(target_task, instance_id)\n-                \n-                # Update workflow telemetry\n-                self._update_workflow_telemetry(instance_id, workflow)\n-                \n-                span.set_attributes({\"task.status\": \"completed\"})\n-                logger.info(f\"✅ Completed task {task_id} with validated telemetry\")\n-                \n-            except Exception as e:\n-                span.set_status(Status(StatusCode.ERROR, str(e)))\n-                logger.error(f\"Failed to complete task {task_id}: {e}\")\n-                raise\n-    \n-    def get_workflow_status(self, instance_id: str) -> Optional[Dict[str, Any]]:\n-        \"\"\"Get workflow status with telemetry validation\"\"\"\n-        if instance_id not in self.active_workflows:\n-            return None\n-        \n-        workflow = self.active_workflows[instance_id]\n-        \n-        # Find telemetry data\n-        for telemetry in self.telemetry_data:\n-            if telemetry.instance_id == instance_id:\n-                return {\n-                    \"instance_id\": instance_id,\n-                    \"status\": telemetry.status,\n-                    \"completed_tasks\": telemetry.completed_tasks,\n-                    \"total_tasks\": telemetry.total_tasks,\n-                    \"is_completed\": workflow.is_completed(),\n-                    \"linkml_validated\": True\n-                }\n-        \n-        return None\n-    \n-    def get_telemetry_summary(self) -> Dict[str, Any]:\n-        \"\"\"Get summary of all validated telemetry data\"\"\"\n-        return {\n-            \"total_workflows\": len(self.telemetry_data),\n-            \"workflow_statuses\": {\n-                status: len([w for w in self.telemetry_data if w.status == status])\n-                for status in [\"running\", \"completed\", \"failed\", \"cancelled\"]\n-            },\n-            \"linkml_validation_enabled\": LINKML_AVAILABLE and self.schema_view is not None,\n-            \"telemetry_data\": [w.to_dict() for w in self.telemetry_data]\n-        }\n-\n-# Example BPMN files for testing\n-def create_sample_bpmn_files():\n-    \"\"\"Create sample BPMN files for testing the orchestration examples\"\"\"\n-    \n-    # Simple process with automatic tasks\n-    simple_process = '''<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n-<bpmn:definitions xmlns:bpmn=\"http://www.omg.org/spec/BPMN/20100524/MODEL\"\n-                  xmlns:bpmndi=\"http://www.omg.org/spec/BPMN/20100524/DI\"\n-                  xmlns:dc=\"http://www.omg.org/spec/DD/20100524/DC\"\n-                  xmlns:di=\"http://www.omg.org/spec/DD/20100524/DI\"\n-                  id=\"Definitions_1\"\n-                  targetNamespace=\"http://bpmn.io/schema/bpmn\">\n-  <bpmn:process id=\"SimpleProcess\" isExecutable=\"true\">\n-    <bpmn:startEvent id=\"StartEvent_1\" name=\"Start\">\n-      <bpmn:outgoing>Flow_1</bpmn:outgoing>\n-    </bpmn:startEvent>\n-    <bpmn:task id=\"Task_1\" name=\"Automatic Task\">\n-      <bpmn:incoming>Flow_1</bpmn:incoming>\n-      <bpmn:outgoing>Flow_2</bpmn:outgoing>\n-    </bpmn:task>\n-    <bpmn:endEvent id=\"EndEvent_1\" name=\"End\">\n-      <bpmn:incoming>Flow_2</bpmn:incoming>\n-    </bpmn:endEvent>\n-    <bpmn:sequenceFlow id=\"Flow_1\" sourceRef=\"StartEvent_1\" targetRef=\"Task_1\" />\n-    <bpmn:sequenceFlow id=\"Flow_2\" sourceRef=\"Task_1\" targetRef=\"EndEvent_1\" />\n-  </bpmn:process>\n-  <bpmndi:BPMNDiagram id=\"BPMNDiagram_1\">\n-    <bpmndi:BPMNPlane id=\"BPMNPlane_1\" bpmnElement=\"SimpleProcess\">\n-      <bpmndi:BPMNShape id=\"StartEvent_1_di\" bpmnElement=\"StartEvent_1\">\n-        <dc:Bounds x=\"152\" y=\"102\" width=\"36\" height=\"36\" />\n-      </bpmndi:BPMNShape>\n-      <bpmndi:BPMNShape id=\"Task_1_di\" bpmnElement=\"Task_1\">\n-        <dc:Bounds x=\"240\" y=\"80\" width=\"100\" height=\"80\" />\n-      </bpmndi:BPMNShape>\n-      <bpmndi:BPMNShape id=\"EndEvent_1_di\" bpmnElement=\"EndEvent_1\">\n-        <dc:Bounds x=\"392\" y=\"102\" width=\"36\" height=\"36\" />\n-      </bpmndi:BPMNShape>\n-      <bpmndi:BPMNEdge id=\"Flow_1_di\" bpmnElement=\"Flow_1\">\n-        <di:waypoint x=\"188\" y=\"120\" />\n-        <di:waypoint x=\"240\" y=\"120\" />\n-      </bpmndi:BPMNEdge>\n-      <bpmndi:BPMNEdge id=\"Flow_2_di\" bpmnElement=\"Flow_2\">\n-        <di:waypoint x=\"340\" y=\"120\" />\n-        <di:waypoint x=\"392\" y=\"120\" />\n-      </bpmndi:BPMNEdge>\n-    </bpmndi:BPMNPlane>\n-  </bpmndi:BPMNDiagram>\n-</bpmn:definitions>'''\n-    \n-    # Process with human task\n-    human_task_process = '''<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n-<bpmn:definitions xmlns:bpmn=\"http://www.omg.org/spec/BPMN/20100524/MODEL\"\n-                  xmlns:bpmndi=\"http://www.omg.org/spec/BPMN/20100524/DI\"\n-                  xmlns:dc=\"http://www.omg.org/spec/DD/20100524/DC\"\n-                  xmlns:di=\"http://www.omg.org/spec/DD/20100524/DI\"\n-                  id=\"Definitions_2\"\n-                  targetNamespace=\"http://bpmn.io/schema/bpmn\">\n-  <bpmn:process id=\"HumanTaskProcess\" isExecutable=\"true\">\n-    <bpmn:startEvent id=\"StartEvent_1\" name=\"Start\">\n-      <bpmn:outgoing>Flow_1</bpmn:outgoing>\n-    </bpmn:startEvent>\n-    <bpmn:userTask id=\"UserTask_1\" name=\"Human Approval\">\n-      <bpmn:incoming>Flow_1</bpmn:incoming>\n-      <bpmn:outgoing>Flow_2</bpmn:outgoing>\n-    </bpmn:userTask>\n-    <bpmn:endEvent id=\"EndEvent_1\" name=\"End\">\n-      <bpmn:incoming>Flow_2</bpmn:incoming>\n-    </bpmn:endEvent>\n-    <bpmn:sequenceFlow id=\"Flow_1\" sourceRef=\"StartEvent_1\" targetRef=\"UserTask_1\" />\n-    <bpmn:sequenceFlow id=\"Flow_2\" sourceRef=\"UserTask_1\" targetRef=\"EndEvent_1\" />\n-  </bpmn:process>\n-  <bpmndi:BPMNDiagram id=\"BPMNDiagram_2\">\n-    <bpmndi:BPMNPlane id=\"BPMNPlane_2\" bpmnElement=\"HumanTaskProcess\">\n-      <bpmndi:BPMNShape id=\"StartEvent_1_di\" bpmnElement=\"StartEvent_1\">\n-        <dc:Bounds x=\"152\" y=\"102\" width=\"36\" height=\"36\" />\n-      </bpmndi:BPMNShape>\n-      <bpmndi:BPMNShape id=\"UserTask_1_di\" bpmnElement=\"UserTask_1\">\n-        <dc:Bounds x=\"240\" y=\"80\" width=\"100\" height=\"80\" />\n-      </bpmndi:BPMNShape>\n-      <bpmndi:BPMNShape id=\"EndEvent_1_di\" bpmnElement=\"EndEvent_1\">\n-        <dc:Bounds x=\"392\" y=\"102\" width=\"36\" height=\"36\" />\n-      </bpmndi:BPMNShape>\n-      <bpmndi:BPMNEdge id=\"Flow_1_di\" bpmnElement=\"Flow_1\">\n-        <di:waypoint x=\"188\" y=\"120\" />\n-        <di:waypoint x=\"240\" y=\"120\" />\n-      </bpmndi:BPMNEdge>\n-      <bpmndi:BPMNEdge id=\"Flow_2_di\" bpmnElement=\"Flow_2\">\n-        <di:waypoint x=\"340\" y=\"120\" />\n-        <di:waypoint x=\"392\" y=\"120\" />\n-      </bpmndi:BPMNEdge>\n-    </bpmndi:BPMNPlane>\n-  </bpmndi:BPMNDiagram>\n-</bpmn:definitions>'''\n-    \n-    # Create BPMN directory and files\n-    bpmn_path = Path(\"bpmn\")\n-    bpmn_path.mkdir(exist_ok=True)\n-    \n-    with open(bpmn_path / \"simple_process.bpmn\", \"w\") as f:\n-        f.write(simple_process)\n-    \n-    with open(bpmn_path / \"human_task_process.bpmn\", \"w\") as f:\n-        f.write(human_task_process)\n-    \n-    print(\"✅ Created sample BPMN files for testing\")\n-\n-def demonstrate_orchestration_patterns():\n-    \"\"\"Demonstrate the new orchestration patterns with LinkML validation\"\"\"\n-    print(\"🚀 SpiffWorkflow Orchestration Examples - LinkML + OpenTelemetry Validation\")\n-    print(\"=\" * 80)\n-    \n-    # Create sample BPMN files\n-    create_sample_bpmn_files()\n-    \n-    # Initialize orchestrator\n-    orchestrator = SpiffOrchestrator()\n-    \n-    print(f\"\\n📊 Orchestrator initialized with {len(orchestrator.process_definitions)} process definitions\")\n-    print(f\"🔍 LinkML validation: {'✅ Enabled' if LINKML_AVAILABLE else '❌ Disabled'}\")\n-    \n-    # Example 1: Simple automatic workflow\n-    print(f\"\\n🧪 Example 1: Simple Automatic Workflow\")\n-    print(\"-\" * 50)\n-    \n-    instance_id = orchestrator.start_workflow(\"SimpleProcess\", {\"input\": \"test_data\"})\n-    print(f\"✅ Started workflow: {instance_id}\")\n-    \n-    ready_tasks = orchestrator.run_until_user_input_required(instance_id)\n-    print(f\"📋 Ready tasks after execution: {len(ready_tasks)}\")\n-    \n-    status = orchestrator.get_workflow_status(instance_id)\n-    print(f\"📊 Workflow status: {status}\")\n-    \n-    # Example 2: Human task workflow\n-    print(f\"\\n🧪 Example 2: Human Task Workflow\")\n-    print(\"-\" * 50)\n-    \n-    instance_id_2 = orchestrator.start_workflow(\"HumanTaskProcess\", {\"approval_required\": True})\n-    print(f\"✅ Started workflow: {instance_id_2}\")\n-    \n-    ready_tasks = orchestrator.run_until_user_input_required(instance_id_2)\n-    print(f\"📋 Ready human tasks: {len(ready_tasks)}\")\n-    \n-    if ready_tasks:\n-        task = ready_tasks[0]\n-        print(f\"👤 Human task: {task['task_name']} (ID: {task['task_id']})\")\n-        \n-        # Complete the human task\n-        orchestrator.complete_task(instance_id_2, task['task_id'], {\"approved\": True})\n-        print(f\"✅ Completed human task\")\n-        \n-        # Continue workflow\n-        ready_tasks = orchestrator.run_until_user_input_required(instance_id_2)\n-        print(f\"📋 Remaining tasks: {len(ready_tasks)}\")\n-    \n-    status = orchestrator.get_workflow_status(instance_id_2)\n-    print(f\"📊 Workflow status: {status}\")\n-    \n-    # Example 3: Telemetry validation summary\n-    print(f\"\\n🧪 Example 3: Telemetry Validation Summary\")\n-    print(\"-\" * 50)\n-    \n-    summary = orchestrator.get_telemetry_summary()\n-    print(f\"📈 Total workflows: {summary['total_workflows']}\")\n-    print(f\"📊 Status distribution: {summary['workflow_statuses']}\")\n-    print(f\"🔍 LinkML validation: {'✅ Enabled' if summary['linkml_validation_enabled'] else '❌ Disabled'}\")\n-    \n-    # Example 4: Demonstrate the new paradigm\n-    print(f\"\\n🧪 Example 4: New Paradigm - Code Agent Validation\")\n-    print(\"-\" * 50)\n-    print(\"\"\"\n-🎯 NEW PARADIGM: Code agents validate their work by generating \n-   structured telemetry that conforms to LinkML schemas.\n-\n-✅ BENEFITS:\n-   - Structured, validated observability\n-   - Schema-driven telemetry validation\n-   - Consistent data formats across systems\n-   - Automated validation of agent outputs\n-   - Weaver-like integration but with formal schemas\n-\n-🔧 IMPLEMENTATION:\n-   - LinkML schemas define telemetry structure\n-   - OpenTelemetry provides the transport\n-   - Code agents generate validated telemetry\n-   - Validation ensures data quality and consistency\n-    \"\"\")\n-    \n-    return orchestrator\n-\n-if __name__ == \"__main__\":\n-    orchestrator = demonstrate_orchestration_patterns()\n-    \n-    print(f\"\\n🎉 Orchestration examples completed successfully!\")\n-    print(f\"📁 Check the 'bpmn/' directory for sample BPMN files\")\n-    print(f\"📊 Review the console output for OpenTelemetry traces\")\n     print(f\"🔍 LinkML validation ensures telemetry data quality\") \n\\ No newline at end of file\n"
                },
                {
                    "date": 1752192668078,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -559,9 +559,9 @@\n                 \n                 if workflow.is_completed():\n                     telemetry.status = \"completed\"\n                     telemetry.end_time = datetime.utcnow()\n-                elif workflow.is_cancelled():\n+                elif hasattr(workflow, 'task_tree') and getattr(workflow.task_tree, 'state', None) == TaskState.CANCELLED:\n                     telemetry.status = \"cancelled\"\n                     telemetry.end_time = datetime.utcnow()\n                 \n                 # Re-validate updated telemetry\n"
                }
            ],
            "date": 1752192373198,
            "name": "Commit-0",
            "content": "#!/usr/bin/env python3\n\"\"\"\nSpiffWorkflow Orchestration Examples - LinkML + OpenTelemetry Validation Paradigm\nDemonstrates how code agents validate their work by generating structured telemetry\n\"\"\"\n\nimport json\nimport uuid\nimport logging\nfrom datetime import datetime, timedelta\nfrom typing import Dict, List, Any, Optional, Union\nfrom dataclasses import dataclass, field\nfrom pathlib import Path\nimport yaml\n\n# SpiffWorkflow imports\nfrom SpiffWorkflow.bpmn import BpmnWorkflow\nfrom SpiffWorkflow.bpmn.parser.BpmnParser import BpmnParser\nfrom SpiffWorkflow.bpmn.specs.bpmn_task_spec import BpmnTaskSpec\nfrom SpiffWorkflow.bpmn.specs.bpmn_process_spec import BpmnProcessSpec\nfrom SpiffWorkflow.util.task import TaskState\nfrom SpiffWorkflow.bpmn.serializer import BpmnWorkflowSerializer\nfrom SpiffWorkflow.bpmn.serializer.config import DEFAULT_CONFIG\n\n# OpenTelemetry for structured telemetry\nfrom opentelemetry import trace, metrics\nfrom opentelemetry.trace import Status, StatusCode\nfrom opentelemetry.metrics import Counter, Histogram\nfrom opentelemetry.sdk.trace import TracerProvider\nfrom opentelemetry.sdk.trace.export import ConsoleSpanExporter, BatchSpanProcessor\nfrom opentelemetry.sdk.metrics import MeterProvider\n\n# LinkML for schema validation\ntry:\n    from linkml_runtime.utils.schemaview import SchemaView\n    from linkml_runtime.loaders import yaml_loader, json_loader\n    from linkml_runtime.dumpers import yaml_dumper, json_dumper\n    LINKML_AVAILABLE = True\nexcept ImportError:\n    LINKML_AVAILABLE = False\n    print(\"⚠️ LinkML not available - telemetry validation disabled\")\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n# Configure OpenTelemetry\ntrace.set_tracer_provider(TracerProvider())\ntrace.get_tracer_provider().add_span_processor(\n    BatchSpanProcessor(ConsoleSpanExporter())\n)\n\nmetrics.set_meter_provider(MeterProvider())\nmeter = metrics.get_meter(__name__)\n\n# Telemetry instruments\nworkflow_counter = meter.create_counter(\n    name=\"spiff_workflows_total\",\n    description=\"Total number of SpiffWorkflow instances\"\n)\n\ntask_duration_histogram = meter.create_histogram(\n    name=\"spiff_task_duration_seconds\",\n    description=\"Duration of SpiffWorkflow tasks\"\n)\n\n# LinkML Schema for Workflow Telemetry\nWORKFLOW_TELEMETRY_SCHEMA = \"\"\"\nid: https://example.org/spiff-workflow-telemetry\nname: spiff-workflow-telemetry\ntitle: SpiffWorkflow Telemetry Schema\nversion: 1.0.0\n\nprefixes:\n  linkml: https://w3id.org/linkml/\n  spiff: https://example.org/spiff-workflow-telemetry/\n  \ndefault_prefix: spiff\ndefault_range: string\n\nimports:\n  - linkml:types\n\nclasses:\n  WorkflowInstance:\n    description: A SpiffWorkflow instance with telemetry data\n    attributes:\n      instance_id:\n        range: string\n        required: true\n        description: Unique identifier for the workflow instance\n      process_id:\n        range: string\n        required: true\n        description: BPMN process definition ID\n      status:\n        range: WorkflowStatus\n        required: true\n        description: Current status of the workflow\n      start_time:\n        range: datetime\n        required: true\n        description: When the workflow started\n      end_time:\n        range: datetime\n        description: When the workflow completed\n      total_tasks:\n        range: integer\n        description: Total number of tasks in the workflow\n      completed_tasks:\n        range: integer\n        description: Number of completed tasks\n      variables:\n        range: string\n        description: JSON string of workflow variables\n      error_message:\n        range: string\n        description: Error message if workflow failed\n        \n  TaskExecution:\n    description: Execution details for a single task\n    attributes:\n      task_id:\n        range: string\n        required: true\n        description: Unique task identifier\n      task_name:\n        range: string\n        required: true\n        description: Human-readable task name\n      task_type:\n        range: string\n        required: true\n        description: Type of task (e.g., UserTask, ServiceTask)\n      status:\n        range: TaskStatus\n        required: true\n        description: Current task status\n      start_time:\n        range: datetime\n        required: true\n        description: When task execution started\n      end_time:\n        range: datetime\n        description: When task execution completed\n      duration_seconds:\n        range: float\n        description: Task execution duration\n      input_data:\n        range: string\n        description: JSON string of input data\n      output_data:\n        range: string\n        description: JSON string of output data\n      error_message:\n        range: string\n        description: Error message if task failed\n      workflow_instance_id:\n        range: string\n        required: true\n        description: Reference to parent workflow instance\n\nenums:\n  WorkflowStatus:\n    permissible_values:\n      pending:\n        description: Workflow is pending execution\n      running:\n        description: Workflow is currently running\n      completed:\n        description: Workflow completed successfully\n      failed:\n        description: Workflow failed with error\n      suspended:\n        description: Workflow is suspended\n      cancelled:\n        description: Workflow was cancelled\n        \n  TaskStatus:\n    permissible_values:\n      pending:\n        description: Task is pending execution\n      ready:\n        description: Task is ready to execute\n      running:\n        description: Task is currently running\n      completed:\n        description: Task completed successfully\n      failed:\n        description: Task failed with error\n      cancelled:\n        description: Task was cancelled\n\"\"\"\n\n@dataclass\nclass WorkflowTelemetry:\n    \"\"\"Structured telemetry data for SpiffWorkflow operations\"\"\"\n    instance_id: str\n    process_id: str\n    status: str\n    start_time: datetime\n    end_time: Optional[datetime] = None\n    total_tasks: int = 0\n    completed_tasks: int = 0\n    variables: str = \"\"\n    error_message: Optional[str] = None\n    \n    def to_dict(self) -> Dict[str, Any]:\n        \"\"\"Convert to dictionary for LinkML validation\"\"\"\n        return {\n            \"instance_id\": self.instance_id,\n            \"process_id\": self.process_id,\n            \"status\": self.status,\n            \"start_time\": self.start_time.isoformat(),\n            \"end_time\": self.end_time.isoformat() if self.end_time else None,\n            \"total_tasks\": self.total_tasks,\n            \"completed_tasks\": self.completed_tasks,\n            \"variables\": self.variables,\n            \"error_message\": self.error_message\n        }\n\n@dataclass\nclass TaskTelemetry:\n    \"\"\"Structured telemetry data for task execution\"\"\"\n    task_id: str\n    task_name: str\n    task_type: str\n    status: str\n    start_time: datetime\n    end_time: Optional[datetime] = None\n    duration_seconds: Optional[float] = None\n    input_data: str = \"\"\n    output_data: str = \"\"\n    error_message: Optional[str] = None\n    workflow_instance_id: str = \"\"\n    \n    def to_dict(self) -> Dict[str, Any]:\n        \"\"\"Convert to dictionary for LinkML validation\"\"\"\n        return {\n            \"task_id\": self.task_id,\n            \"task_name\": self.task_name,\n            \"task_type\": self.task_type,\n            \"status\": self.status,\n            \"start_time\": self.start_time.isoformat(),\n            \"end_time\": self.end_time.isoformat() if self.end_time else None,\n            \"duration_seconds\": self.duration_seconds,\n            \"input_data\": self.input_data,\n            \"output_data\": self.output_data,\n            \"error_message\": self.error_message,\n            \"workflow_instance_id\": self.workflow_instance_id\n        }\n\nclass SpiffOrchestrator:\n    \"\"\"\n    SpiffWorkflow Orchestrator with LinkML-validated OpenTelemetry\n    \n    This demonstrates the new paradigm where code agents validate their work\n    by generating structured telemetry that conforms to LinkML schemas.\n    \"\"\"\n    \n    def __init__(self, bpmn_files_path: str = \"bpmn\"):\n        \"\"\"Initialize the orchestrator with LinkML schema validation\"\"\"\n        self.tracer = trace.get_tracer(__name__)\n        self.bpmn_files_path = Path(bpmn_files_path)\n        \n        # Initialize SpiffWorkflow parser\n        self.parser = BpmnParser()\n        self.parser.add_bpmn_files_by_glob(str(self.bpmn_files_path / \"*.bpmn\"))\n        \n        # Load LinkML schema for validation\n        self.schema_view = None\n        if LINKML_AVAILABLE:\n            try:\n                # Create temporary schema file\n                schema_path = Path(\"workflow_telemetry_schema.yaml\")\n                with open(schema_path, 'w') as f:\n                    f.write(WORKFLOW_TELEMETRY_SCHEMA)\n                \n                self.schema_view = SchemaView(str(schema_path))\n                logger.info(\"✅ LinkML schema loaded for telemetry validation\")\n            except Exception as e:\n                logger.warning(f\"⚠️ Failed to load LinkML schema: {e}\")\n        \n        # Process definitions cache\n        self.process_definitions: Dict[str, BpmnProcessSpec] = {}\n        self.active_workflows: Dict[str, BpmnWorkflow] = {}\n        self.telemetry_data: List[WorkflowTelemetry] = []\n        \n        # Load process definitions\n        self._load_process_definitions()\n        \n        logger.info(f\"SpiffOrchestrator initialized with {len(self.process_definitions)} process definitions\")\n    \n    def _load_process_definitions(self) -> None:\n        \"\"\"Load BPMN process definitions with telemetry\"\"\"\n        with self.tracer.start_as_current_span(\"load_process_definitions\") as span:\n            try:\n                for process_id, process_spec in self.parser.find_all_specs().items():\n                    self.process_definitions[process_id] = process_spec\n                    span.add_event(f\"Loaded process definition: {process_id}\")\n                \n                logger.info(f\"Loaded {len(self.process_definitions)} process definitions\")\n                \n            except Exception as e:\n                span.set_status(Status(StatusCode.ERROR, str(e)))\n                logger.error(f\"Failed to load process definitions: {e}\")\n                raise\n    \n    def _validate_telemetry_with_linkml(self, telemetry_data: Dict[str, Any], class_name: str) -> bool:\n        \"\"\"Validate telemetry data against LinkML schema\"\"\"\n        if not LINKML_AVAILABLE or not self.schema_view:\n            return True  # Skip validation if LinkML not available\n        \n        try:\n            # Validate using LinkML loader\n            if class_name == \"WorkflowInstance\":\n                validated_obj = yaml_loader.loads(\n                    yaml.dump(telemetry_data), \n                    target_class=\"WorkflowInstance\", \n                    schemaview=self.schema_view\n                )\n            elif class_name == \"TaskExecution\":\n                validated_obj = yaml_loader.loads(\n                    yaml.dump(telemetry_data), \n                    target_class=\"TaskExecution\", \n                    schemaview=self.schema_view\n                )\n            else:\n                logger.warning(f\"Unknown class name for validation: {class_name}\")\n                return False\n            \n            logger.debug(f\"✅ LinkML validation passed for {class_name}\")\n            return True\n            \n        except Exception as e:\n            logger.error(f\"❌ LinkML validation failed for {class_name}: {e}\")\n            return False\n    \n    def start_workflow(self, process_id: str, variables: Optional[Dict[str, Any]] = None) -> str:\n        \"\"\"\n        Start a new workflow instance with validated telemetry\n        \n        This demonstrates the new paradigm: the code agent validates its work\n        by generating structured telemetry that conforms to LinkML schemas.\n        \"\"\"\n        with self.tracer.start_as_current_span(\"start_workflow\") as span:\n            span.set_attributes({\n                \"process.id\": process_id,\n                \"process.variables\": json.dumps(variables or {})\n            })\n            \n            try:\n                # Validate process exists\n                if process_id not in self.process_definitions:\n                    raise ValueError(f\"Process definition '{process_id}' not found\")\n                \n                # Generate instance ID\n                instance_id = f\"{process_id}_{uuid.uuid4().hex[:8]}\"\n                \n                # Create workflow instance\n                workflow = BpmnWorkflow(self.process_definitions[process_id])\n                \n                # Set initial variables\n                if variables:\n                    workflow.data.update(variables)\n                \n                # Store workflow\n                self.active_workflows[instance_id] = workflow\n                \n                # Generate structured telemetry\n                workflow_telemetry = WorkflowTelemetry(\n                    instance_id=instance_id,\n                    process_id=process_id,\n                    status=\"running\",\n                    start_time=datetime.utcnow(),\n                    total_tasks=len(workflow.get_tasks()),\n                    completed_tasks=0,\n                    variables=json.dumps(variables or {})\n                )\n                \n                # Validate telemetry with LinkML\n                telemetry_dict = workflow_telemetry.to_dict()\n                validation_passed = self._validate_telemetry_with_linkml(telemetry_dict, \"WorkflowInstance\")\n                \n                if validation_passed:\n                    # Record validated telemetry\n                    self.telemetry_data.append(workflow_telemetry)\n                    \n                    # Update OpenTelemetry metrics\n                    workflow_counter.add(1, {\n                        \"process.id\": process_id,\n                        \"action\": \"start\",\n                        \"linkml_validated\": \"true\"\n                    })\n                    \n                    span.set_attributes({\n                        \"instance.id\": instance_id,\n                        \"linkml.validation\": \"passed\"\n                    })\n                    \n                    logger.info(f\"✅ Started workflow {instance_id} with validated telemetry\")\n                else:\n                    span.set_attributes({\"linkml.validation\": \"failed\"})\n                    logger.error(f\"❌ Telemetry validation failed for workflow {instance_id}\")\n                \n                return instance_id\n                \n            except Exception as e:\n                span.set_status(Status(StatusCode.ERROR, str(e)))\n                logger.error(f\"Failed to start workflow {process_id}: {e}\")\n                raise\n    \n    def run_until_user_input_required(self, instance_id: str) -> List[Dict[str, Any]]:\n        \"\"\"\n        Run workflow until user input is required (Arena pattern)\n        \n        This demonstrates advanced orchestration with validated telemetry.\n        \"\"\"\n        with self.tracer.start_as_current_span(\"run_until_user_input\") as span:\n            span.set_attributes({\"instance.id\": instance_id})\n            \n            try:\n                if instance_id not in self.active_workflows:\n                    raise ValueError(f\"Workflow instance '{instance_id}' not found\")\n                \n                workflow = self.active_workflows[instance_id]\n                ready_tasks = []\n                \n                # Run automatic tasks (manual=False)\n                task = workflow.get_next_task(state=TaskState.READY, manual=False)\n                while task is not None:\n                    # Execute task with telemetry\n                    self._execute_task_with_telemetry(task, instance_id)\n                    \n                    # Refresh waiting tasks and handle events\n                    self._run_ready_events(workflow, instance_id)\n                    \n                    # Get next automatic task\n                    task = workflow.get_next_task(state=TaskState.READY, manual=False)\n                \n                # Get ready human tasks\n                human_tasks = workflow.get_tasks(state=TaskState.READY, manual=True)\n                for task in human_tasks:\n                    ready_tasks.append({\n                        \"task_id\": str(task.id),\n                        \"task_name\": task.task_spec.name,\n                        \"task_type\": task.task_spec.__class__.__name__,\n                        \"data\": dict(task.data) if hasattr(task, 'data') else {}\n                    })\n                \n                # Update workflow telemetry\n                self._update_workflow_telemetry(instance_id, workflow)\n                \n                span.set_attributes({\n                    \"ready_tasks_count\": len(ready_tasks),\n                    \"workflow_completed\": workflow.is_completed()\n                })\n                \n                return ready_tasks\n                \n            except Exception as e:\n                span.set_status(Status(StatusCode.ERROR, str(e)))\n                logger.error(f\"Failed to run workflow {instance_id}: {e}\")\n                raise\n    \n    def _execute_task_with_telemetry(self, task: BpmnTaskSpec, instance_id: str) -> None:\n        \"\"\"Execute a task with LinkML-validated telemetry\"\"\"\n        with self.tracer.start_as_current_span(\"execute_task\") as span:\n            task_start_time = datetime.utcnow()\n            \n            span.set_attributes({\n                \"task.id\": str(task.id),\n                \"task.name\": task.task_spec.name,\n                \"task.type\": task.task_spec.__class__.__name__\n            })\n            \n            try:\n                # Create task telemetry\n                task_telemetry = TaskTelemetry(\n                    task_id=str(task.id),\n                    task_name=task.task_spec.name,\n                    task_type=task.task_spec.__class__.__name__,\n                    status=\"running\",\n                    start_time=task_start_time,\n                    input_data=json.dumps(dict(task.data) if hasattr(task, 'data') else {}),\n                    workflow_instance_id=instance_id\n                )\n                \n                # Execute the task\n                task.complete()\n                \n                # Update task telemetry\n                task_telemetry.end_time = datetime.utcnow()\n                task_telemetry.duration_seconds = (\n                    task_telemetry.end_time - task_telemetry.start_time\n                ).total_seconds()\n                task_telemetry.status = \"completed\"\n                task_telemetry.output_data = json.dumps(dict(task.data) if hasattr(task, 'data') else {})\n                \n                # Validate task telemetry with LinkML\n                telemetry_dict = task_telemetry.to_dict()\n                validation_passed = self._validate_telemetry_with_linkml(telemetry_dict, \"TaskExecution\")\n                \n                if validation_passed:\n                    # Record task duration metric\n                    task_duration_histogram.record(\n                        task_telemetry.duration_seconds,\n                        {\n                            \"task.id\": str(task.id),\n                            \"task.type\": task_telemetry.task_type,\n                            \"linkml_validated\": \"true\"\n                        }\n                    )\n                    \n                    span.set_attributes({\n                        \"task.duration\": task_telemetry.duration_seconds,\n                        \"linkml.validation\": \"passed\"\n                    })\n                    \n                    logger.debug(f\"✅ Executed task {task.id} with validated telemetry\")\n                else:\n                    span.set_attributes({\"linkml.validation\": \"failed\"})\n                    logger.error(f\"❌ Task telemetry validation failed for {task.id}\")\n                \n            except Exception as e:\n                # Record failed task telemetry\n                task_telemetry.end_time = datetime.utcnow()\n                task_telemetry.duration_seconds = (\n                    task_telemetry.end_time - task_telemetry.start_time\n                ).total_seconds()\n                task_telemetry.status = \"failed\"\n                task_telemetry.error_message = str(e)\n                \n                # Validate failed telemetry\n                telemetry_dict = task_telemetry.to_dict()\n                self._validate_telemetry_with_linkml(telemetry_dict, \"TaskExecution\")\n                \n                span.set_status(Status(StatusCode.ERROR, str(e)))\n                logger.error(f\"Failed to execute task {task.id}: {e}\")\n                raise\n    \n    def _run_ready_events(self, workflow: BpmnWorkflow, instance_id: str) -> None:\n        \"\"\"Run ready events (Arena pattern)\"\"\"\n        workflow.refresh_waiting_tasks()\n        \n        # Handle catching events\n        from SpiffWorkflow.bpmn.specs.event_definitions.base import CatchingEvent\n        \n        task = workflow.get_next_task(state=TaskState.READY, spec_class=CatchingEvent)\n        while task is not None:\n            self._execute_task_with_telemetry(task, instance_id)\n            task = workflow.get_next_task(state=TaskState.READY, spec_class=CatchingEvent)\n    \n    def _update_workflow_telemetry(self, instance_id: str, workflow: BpmnWorkflow) -> None:\n        \"\"\"Update workflow telemetry with current state\"\"\"\n        # Find existing telemetry\n        for telemetry in self.telemetry_data:\n            if telemetry.instance_id == instance_id:\n                telemetry.completed_tasks = len(workflow.get_tasks(state=TaskState.COMPLETED))\n                \n                if workflow.is_completed():\n                    telemetry.status = \"completed\"\n                    telemetry.end_time = datetime.utcnow()\n                elif workflow.is_cancelled():\n                    telemetry.status = \"cancelled\"\n                    telemetry.end_time = datetime.utcnow()\n                \n                # Re-validate updated telemetry\n                telemetry_dict = telemetry.to_dict()\n                self._validate_telemetry_with_linkml(telemetry_dict, \"WorkflowInstance\")\n                break\n    \n    def complete_task(self, instance_id: str, task_id: str, data: Optional[Dict[str, Any]] = None) -> None:\n        \"\"\"Complete a human task with validated telemetry\"\"\"\n        with self.tracer.start_as_current_span(\"complete_task\") as span:\n            span.set_attributes({\n                \"instance.id\": instance_id,\n                \"task.id\": task_id\n            })\n            \n            try:\n                if instance_id not in self.active_workflows:\n                    raise ValueError(f\"Workflow instance '{instance_id}' not found\")\n                \n                workflow = self.active_workflows[instance_id]\n                \n                # Find the task\n                tasks = workflow.get_tasks()\n                target_task = None\n                \n                for task in tasks:\n                    if str(task.id) == task_id:\n                        target_task = task\n                        break\n                \n                if not target_task:\n                    raise ValueError(f\"Task '{task_id}' not found\")\n                \n                # Set task data if provided\n                if data and hasattr(target_task, 'data'):\n                    target_task.data.update(data)\n                \n                # Execute task with telemetry\n                self._execute_task_with_telemetry(target_task, instance_id)\n                \n                # Update workflow telemetry\n                self._update_workflow_telemetry(instance_id, workflow)\n                \n                span.set_attributes({\"task.status\": \"completed\"})\n                logger.info(f\"✅ Completed task {task_id} with validated telemetry\")\n                \n            except Exception as e:\n                span.set_status(Status(StatusCode.ERROR, str(e)))\n                logger.error(f\"Failed to complete task {task_id}: {e}\")\n                raise\n    \n    def get_workflow_status(self, instance_id: str) -> Optional[Dict[str, Any]]:\n        \"\"\"Get workflow status with telemetry validation\"\"\"\n        if instance_id not in self.active_workflows:\n            return None\n        \n        workflow = self.active_workflows[instance_id]\n        \n        # Find telemetry data\n        for telemetry in self.telemetry_data:\n            if telemetry.instance_id == instance_id:\n                return {\n                    \"instance_id\": instance_id,\n                    \"status\": telemetry.status,\n                    \"completed_tasks\": telemetry.completed_tasks,\n                    \"total_tasks\": telemetry.total_tasks,\n                    \"is_completed\": workflow.is_completed(),\n                    \"linkml_validated\": True\n                }\n        \n        return None\n    \n    def get_telemetry_summary(self) -> Dict[str, Any]:\n        \"\"\"Get summary of all validated telemetry data\"\"\"\n        return {\n            \"total_workflows\": len(self.telemetry_data),\n            \"workflow_statuses\": {\n                status: len([w for w in self.telemetry_data if w.status == status])\n                for status in [\"running\", \"completed\", \"failed\", \"cancelled\"]\n            },\n            \"linkml_validation_enabled\": LINKML_AVAILABLE and self.schema_view is not None,\n            \"telemetry_data\": [w.to_dict() for w in self.telemetry_data]\n        }\n\n# Example BPMN files for testing\ndef create_sample_bpmn_files():\n    \"\"\"Create sample BPMN files for testing the orchestration examples\"\"\"\n    \n    # Simple process with automatic tasks\n    simple_process = '''<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<bpmn:definitions xmlns:bpmn=\"http://www.omg.org/spec/BPMN/20100524/MODEL\"\n                  xmlns:bpmndi=\"http://www.omg.org/spec/BPMN/20100524/DI\"\n                  xmlns:dc=\"http://www.omg.org/spec/DD/20100524/DC\"\n                  xmlns:di=\"http://www.omg.org/spec/DD/20100524/DI\"\n                  id=\"Definitions_1\"\n                  targetNamespace=\"http://bpmn.io/schema/bpmn\">\n  <bpmn:process id=\"SimpleProcess\" isExecutable=\"true\">\n    <bpmn:startEvent id=\"StartEvent_1\" name=\"Start\">\n      <bpmn:outgoing>Flow_1</bpmn:outgoing>\n    </bpmn:startEvent>\n    <bpmn:task id=\"Task_1\" name=\"Automatic Task\">\n      <bpmn:incoming>Flow_1</bpmn:incoming>\n      <bpmn:outgoing>Flow_2</bpmn:outgoing>\n    </bpmn:task>\n    <bpmn:endEvent id=\"EndEvent_1\" name=\"End\">\n      <bpmn:incoming>Flow_2</bpmn:incoming>\n    </bpmn:endEvent>\n    <bpmn:sequenceFlow id=\"Flow_1\" sourceRef=\"StartEvent_1\" targetRef=\"Task_1\" />\n    <bpmn:sequenceFlow id=\"Flow_2\" sourceRef=\"Task_1\" targetRef=\"EndEvent_1\" />\n  </bpmn:process>\n  <bpmndi:BPMNDiagram id=\"BPMNDiagram_1\">\n    <bpmndi:BPMNPlane id=\"BPMNPlane_1\" bpmnElement=\"SimpleProcess\">\n      <bpmndi:BPMNShape id=\"StartEvent_1_di\" bpmnElement=\"StartEvent_1\">\n        <dc:Bounds x=\"152\" y=\"102\" width=\"36\" height=\"36\" />\n      </bpmndi:BPMNShape>\n      <bpmndi:BPMNShape id=\"Task_1_di\" bpmnElement=\"Task_1\">\n        <dc:Bounds x=\"240\" y=\"80\" width=\"100\" height=\"80\" />\n      </bpmndi:BPMNShape>\n      <bpmndi:BPMNShape id=\"EndEvent_1_di\" bpmnElement=\"EndEvent_1\">\n        <dc:Bounds x=\"392\" y=\"102\" width=\"36\" height=\"36\" />\n      </bpmndi:BPMNShape>\n      <bpmndi:BPMNEdge id=\"Flow_1_di\" bpmnElement=\"Flow_1\">\n        <di:waypoint x=\"188\" y=\"120\" />\n        <di:waypoint x=\"240\" y=\"120\" />\n      </bpmndi:BPMNEdge>\n      <bpmndi:BPMNEdge id=\"Flow_2_di\" bpmnElement=\"Flow_2\">\n        <di:waypoint x=\"340\" y=\"120\" />\n        <di:waypoint x=\"392\" y=\"120\" />\n      </bpmndi:BPMNEdge>\n    </bpmndi:BPMNPlane>\n  </bpmndi:BPMNDiagram>\n</bpmn:definitions>'''\n    \n    # Process with human task\n    human_task_process = '''<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<bpmn:definitions xmlns:bpmn=\"http://www.omg.org/spec/BPMN/20100524/MODEL\"\n                  xmlns:bpmndi=\"http://www.omg.org/spec/BPMN/20100524/DI\"\n                  xmlns:dc=\"http://www.omg.org/spec/DD/20100524/DC\"\n                  xmlns:di=\"http://www.omg.org/spec/DD/20100524/DI\"\n                  id=\"Definitions_2\"\n                  targetNamespace=\"http://bpmn.io/schema/bpmn\">\n  <bpmn:process id=\"HumanTaskProcess\" isExecutable=\"true\">\n    <bpmn:startEvent id=\"StartEvent_1\" name=\"Start\">\n      <bpmn:outgoing>Flow_1</bpmn:outgoing>\n    </bpmn:startEvent>\n    <bpmn:userTask id=\"UserTask_1\" name=\"Human Approval\">\n      <bpmn:incoming>Flow_1</bpmn:incoming>\n      <bpmn:outgoing>Flow_2</bpmn:outgoing>\n    </bpmn:userTask>\n    <bpmn:endEvent id=\"EndEvent_1\" name=\"End\">\n      <bpmn:incoming>Flow_2</bpmn:incoming>\n    </bpmn:endEvent>\n    <bpmn:sequenceFlow id=\"Flow_1\" sourceRef=\"StartEvent_1\" targetRef=\"UserTask_1\" />\n    <bpmn:sequenceFlow id=\"Flow_2\" sourceRef=\"UserTask_1\" targetRef=\"EndEvent_1\" />\n  </bpmn:process>\n  <bpmndi:BPMNDiagram id=\"BPMNDiagram_2\">\n    <bpmndi:BPMNPlane id=\"BPMNPlane_2\" bpmnElement=\"HumanTaskProcess\">\n      <bpmndi:BPMNShape id=\"StartEvent_1_di\" bpmnElement=\"StartEvent_1\">\n        <dc:Bounds x=\"152\" y=\"102\" width=\"36\" height=\"36\" />\n      </bpmndi:BPMNShape>\n      <bpmndi:BPMNShape id=\"UserTask_1_di\" bpmnElement=\"UserTask_1\">\n        <dc:Bounds x=\"240\" y=\"80\" width=\"100\" height=\"80\" />\n      </bpmndi:BPMNShape>\n      <bpmndi:BPMNShape id=\"EndEvent_1_di\" bpmnElement=\"EndEvent_1\">\n        <dc:Bounds x=\"392\" y=\"102\" width=\"36\" height=\"36\" />\n      </bpmndi:BPMNShape>\n      <bpmndi:BPMNEdge id=\"Flow_1_di\" bpmnElement=\"Flow_1\">\n        <di:waypoint x=\"188\" y=\"120\" />\n        <di:waypoint x=\"240\" y=\"120\" />\n      </bpmndi:BPMNEdge>\n      <bpmndi:BPMNEdge id=\"Flow_2_di\" bpmnElement=\"Flow_2\">\n        <di:waypoint x=\"340\" y=\"120\" />\n        <di:waypoint x=\"392\" y=\"120\" />\n      </bpmndi:BPMNEdge>\n    </bpmndi:BPMNPlane>\n  </bpmndi:BPMNDiagram>\n</bpmn:definitions>'''\n    \n    # Create BPMN directory and files\n    bpmn_path = Path(\"bpmn\")\n    bpmn_path.mkdir(exist_ok=True)\n    \n    with open(bpmn_path / \"simple_process.bpmn\", \"w\") as f:\n        f.write(simple_process)\n    \n    with open(bpmn_path / \"human_task_process.bpmn\", \"w\") as f:\n        f.write(human_task_process)\n    \n    print(\"✅ Created sample BPMN files for testing\")\n\ndef demonstrate_orchestration_patterns():\n    \"\"\"Demonstrate the new orchestration patterns with LinkML validation\"\"\"\n    print(\"🚀 SpiffWorkflow Orchestration Examples - LinkML + OpenTelemetry Validation\")\n    print(\"=\" * 80)\n    \n    # Create sample BPMN files\n    create_sample_bpmn_files()\n    \n    # Initialize orchestrator\n    orchestrator = SpiffOrchestrator()\n    \n    print(f\"\\n📊 Orchestrator initialized with {len(orchestrator.process_definitions)} process definitions\")\n    print(f\"🔍 LinkML validation: {'✅ Enabled' if LINKML_AVAILABLE else '❌ Disabled'}\")\n    \n    # Example 1: Simple automatic workflow\n    print(f\"\\n🧪 Example 1: Simple Automatic Workflow\")\n    print(\"-\" * 50)\n    \n    instance_id = orchestrator.start_workflow(\"SimpleProcess\", {\"input\": \"test_data\"})\n    print(f\"✅ Started workflow: {instance_id}\")\n    \n    ready_tasks = orchestrator.run_until_user_input_required(instance_id)\n    print(f\"📋 Ready tasks after execution: {len(ready_tasks)}\")\n    \n    status = orchestrator.get_workflow_status(instance_id)\n    print(f\"📊 Workflow status: {status}\")\n    \n    # Example 2: Human task workflow\n    print(f\"\\n🧪 Example 2: Human Task Workflow\")\n    print(\"-\" * 50)\n    \n    instance_id_2 = orchestrator.start_workflow(\"HumanTaskProcess\", {\"approval_required\": True})\n    print(f\"✅ Started workflow: {instance_id_2}\")\n    \n    ready_tasks = orchestrator.run_until_user_input_required(instance_id_2)\n    print(f\"📋 Ready human tasks: {len(ready_tasks)}\")\n    \n    if ready_tasks:\n        task = ready_tasks[0]\n        print(f\"👤 Human task: {task['task_name']} (ID: {task['task_id']})\")\n        \n        # Complete the human task\n        orchestrator.complete_task(instance_id_2, task['task_id'], {\"approved\": True})\n        print(f\"✅ Completed human task\")\n        \n        # Continue workflow\n        ready_tasks = orchestrator.run_until_user_input_required(instance_id_2)\n        print(f\"📋 Remaining tasks: {len(ready_tasks)}\")\n    \n    status = orchestrator.get_workflow_status(instance_id_2)\n    print(f\"📊 Workflow status: {status}\")\n    \n    # Example 3: Telemetry validation summary\n    print(f\"\\n🧪 Example 3: Telemetry Validation Summary\")\n    print(\"-\" * 50)\n    \n    summary = orchestrator.get_telemetry_summary()\n    print(f\"📈 Total workflows: {summary['total_workflows']}\")\n    print(f\"📊 Status distribution: {summary['workflow_statuses']}\")\n    print(f\"🔍 LinkML validation: {'✅ Enabled' if summary['linkml_validation_enabled'] else '❌ Disabled'}\")\n    \n    # Example 4: Demonstrate the new paradigm\n    print(f\"\\n🧪 Example 4: New Paradigm - Code Agent Validation\")\n    print(\"-\" * 50)\n    print(\"\"\"\n🎯 NEW PARADIGM: Code agents validate their work by generating \n   structured telemetry that conforms to LinkML schemas.\n\n✅ BENEFITS:\n   - Structured, validated observability\n   - Schema-driven telemetry validation\n   - Consistent data formats across systems\n   - Automated validation of agent outputs\n   - Weaver-like integration but with formal schemas\n\n🔧 IMPLEMENTATION:\n   - LinkML schemas define telemetry structure\n   - OpenTelemetry provides the transport\n   - Code agents generate validated telemetry\n   - Validation ensures data quality and consistency\n    \"\"\")\n    \n    return orchestrator\n\nif __name__ == \"__main__\":\n    orchestrator = demonstrate_orchestration_patterns()\n    \n    print(f\"\\n🎉 Orchestration examples completed successfully!\")\n    print(f\"📁 Check the 'bpmn/' directory for sample BPMN files\")\n    print(f\"📊 Review the console output for OpenTelemetry traces\")\n    print(f\"🔍 LinkML validation ensures telemetry data quality\") "
        }
    ]
}